[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "High-Dimensional Propensity Scores",
    "section": "",
    "text": "Background\nThe use of retrospective health care claims datasets is frequently criticized for lacking complete information on potential confounders. Ultimately, the treatment effects estimated utilizing such data sources may be subject to residual confounding. Digital electronic administrative records routinely collect a large volume of health-related information; and many of whom are usually not considered in conventional pharmacoepidemiological studies.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "index.html#proposal-to-reduce-residual-confounding-bias",
    "href": "index.html#proposal-to-reduce-residual-confounding-bias",
    "title": "High-Dimensional Propensity Scores",
    "section": "Proposal to reduce residual confounding bias",
    "text": "Proposal to reduce residual confounding bias\nIn 2009, a high-dimensional propensity score (hdPS) algorithm was proposed that utilizes such information as surrogates or proxies for mismeasured and unobserved confounders in an effort to reduce residual confounding bias. Since then, many machine learning and semi-parametric extensions of this algorithm have been proposed to exploit the wealth of high-dimensional proxy information properly.\n\n\nSchneeweiss et al. (2009)",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "index.html#purpose-of-the-workshop",
    "href": "index.html#purpose-of-the-workshop",
    "title": "High-Dimensional Propensity Scores",
    "section": "Purpose of the workshop",
    "text": "Purpose of the workshop\nThis workshop will\n\ndemonstrate logic, steps and implementation guidelines of hdPS utilizing an open data source as an example (using reproducible R codes),\n\n\n\n\n\n\n\n\n\n\n\nfamiliarize participants with the difference between propensity score vs. hdPS,\nexplain the rationale for using the machine learning extensions of hdPS, and their statistical properties, and\ndiscuss advantages, controversies, and hdPS reporting guidelines while writing a manuscript.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "index.html#workshop-prerequisite",
    "href": "index.html#workshop-prerequisite",
    "title": "High-Dimensional Propensity Scores",
    "section": "Workshop prerequisite",
    "text": "Workshop prerequisite\nAttendees should have prerequisite knowledge of multiple regression analysis and working knowledge in R (e.g., basic data manipulation and regression fitting).\n\nR Codes\nR Codes can be downloaded here:\n\nRMD file\nHTML file\nPDF file\nData file\n\n\n\nVersion history\nDifferent versions and updates of the materials were presented in the following sessions\n\n2026 Biostatistics section of Statistical Society of Canada through Instats, Feb 17.\n2025 Canadian Society for Epidemiology and Biostatistics, Montreal, Quebec, August 11.\n2025 Society of Epidemiologic Research Workshops, July 11.\n2025 Statistical Society of Canada, Biostatistics Workshop, May 25 (together with Md Belal Hossain)\n2024 Society of Epidemiologic Research Workshops, May 10th.\n2023 R/Medicine Conference, Virtual, June 5.\n2023 Society of Epidemiologic Research Workshops, Virtual, May 4.\n\nAdditional relevant talks (selected):\n\nStatistical issues in administrative data, Banff International Research Station, Banff, Feb 2019.\nStatistics Conference in Genomics, Pharmaceutical Science, and Health Data Science, August 15-17, 2022 University of Victoria, Victoria, BC\nWork in Progress Seminar, CHEOS, St. Paul’s Hospital (Hurlburt Auditorium), Dec 14th, 2022.\nStatistics and Biostatistics seminar series, at the Department of Statistics and Actuarial Science, University of Waterloo, April 26, 2023.\nConference on Statistics and Data Science with Applications in Biology, Genetics, Public Health, and Finance, Thompson Rivers University, Kamloops, August 21-24, 2023.\n\n\n\nCitation\n\n\n\n\n\n\nHow to cite\n\n\n\nKarim, M. E. (2025). High-dimensional propensity score and its machine learning extensions in residual confounding control. The American Statistician, 79(1), 72-90. DOI: 10.1080/00031305.2024.2368794.\n\n\n\n\n\n\n\n\n\n\n\n\n\nComments\nFor any comments regarding this document, reach out to me.\n\n\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn, Helen Mogun, and M Alan Brookhart. 2009. “High-Dimensional Propensity Score Adjustment in Studies of Treatment Effects Using Health Care Claims Data.” Epidemiology (Cambridge, Mass.) 20 (4): 512.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "motivating.html",
    "href": "motivating.html",
    "title": "Motivating example",
    "section": "",
    "text": "Literature\nType 2 diabetes is a metabolic disorder that is characterized by high blood sugar levels and insulin resistance. There is a growing body of evidence that, for type 2 diabetes, obesity is a well-established risk factor. Possible mechanism includes excess body fat leading to insulin resistance, while impairing the body’s ability to regulate blood sugar levels.",
    "crumbs": [
      "Motivating example"
    ]
  },
  {
    "objectID": "motivating.html#literature",
    "href": "motivating.html#literature",
    "title": "Motivating example",
    "section": "",
    "text": "(Klein et al. 2022)",
    "crumbs": [
      "Motivating example"
    ]
  },
  {
    "objectID": "motivating.html#research-question",
    "href": "motivating.html#research-question",
    "title": "Motivating example",
    "section": "Research question",
    "text": "Research question\n“Does obesity increase the risk of developing diabetes?”\n\n\n\n\n\n\nTip\n\n\n\nObesity is often considered a challenging exposure variable to define precisely in research studies (Hernán and Taubman 2008). In this case, we are using it as an illustrative example to explain the methods and not attempting to make any clinical statements about this topic.\n\n\n\n\n\n\n\n\nflowchart LR\n  A[Obesity] --&gt; Y(Diabetes)\n\n\n\n\n\n\n\n\n\nExposure: Being obese\n\nOutcome: Developing diabetes\n\n\n\n\n\n\n\nTip\n\n\n\nThe primary goal of the research is not to answer a clinical question or to draw conclusions about the relationship between obesity and diabetes in the general population, but rather to use the relationship as a motivating example for conducting simulations that compares different statistical methods.\n\n\n\n\n\n\nHernán, Miguel A, and Sarah L Taubman. 2008. “Does Obesity Shorten Life? The Importance of Well-Defined Interventions to Answer Causal Questions.” International Journal of Obesity 32 (3): S8–14.\n\n\nKlein, Samuel, Amalia Gastaldelli, Hannele Yki-Järvinen, and Philipp E Scherer. 2022. “Why Does Obesity Cause Diabetes?” Cell Metabolism 34 (1): 11–20.",
    "crumbs": [
      "Motivating example"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "1  Data to Analyze",
    "section": "",
    "text": "1.1 Choose a U.S. data source\nTo answer the research question “Does obesity increase the risk of developing diabetes?” in the U.S. context, we do the following:",
    "crumbs": [
      "Motivating example",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data to Analyze</span>"
    ]
  },
  {
    "objectID": "data.html#choose-a-u.s.-data-source",
    "href": "data.html#choose-a-u.s.-data-source",
    "title": "1  Data to Analyze",
    "section": "",
    "text": "Data source: National Health and Nutrition Examination Survey (NHANES) (Disease Control and Prevention 2021)\n\n2013-2014,\n2015-2016,\n2017-2018\n\nAvailability: NHANES is a publicly available dataset that can be downloaded for free from the CDC website.\nDesign: Observational cross-sectional data. Hence, inferring causality is not a possibility or our objective here.",
    "crumbs": [
      "Motivating example",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data to Analyze</span>"
    ]
  },
  {
    "objectID": "data.html#confounder-identification",
    "href": "data.html#confounder-identification",
    "title": "1  Data to Analyze",
    "section": "1.2 Confounder identification",
    "text": "1.2 Confounder identification\nDirected acyclic graph (DAG)\n\n\n(Greenland, Pearl, and Robins 1999)\n\n\n\n\n\n\nflowchart TB\n  A[Obesity A] --&gt; Y(Diabetes Y)\n  L[Confounders C] --&gt; Y\n  L --&gt; A\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesized Directed acyclic graph drawn based on analyst’s best understanding of the literature\n\n\n\n\n\n\nExposure: Being obese\n\nOutcome: Developing diabetes\n\nConfounders: Demographic and lab variables",
    "crumbs": [
      "Motivating example",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data to Analyze</span>"
    ]
  },
  {
    "objectID": "data.html#structure-of-the-data",
    "href": "data.html#structure-of-the-data",
    "title": "1  Data to Analyze",
    "section": "1.3 Structure of the data",
    "text": "1.3 Structure of the data\n\n\n\n\n\n\nflowchart LR\n  D[NHANES 2013-14] --&gt; demo[Demographic Variables and Sample Weights]\n  demo --&gt; Age\n  demo --&gt; Sex\n  demo --&gt; Education\n  demo --&gt; r[Race or ethnicity]\n  demo --&gt; m[Marital status]\n  demo --&gt; Income\n  demo --&gt; b[Birth place]\n  demo --&gt; sf[Survey features: sampling weights, strata, cluster]\n  D --&gt; bmi[Body Measures]\n  bmi --&gt; Obesity\n  D --&gt; diq[Diabetes]\n  diq --&gt; Diabetes\n  diq --&gt; f[Family history of diabetes]\n  D --&gt; smq[Smoking - Cigarette Use]\n  smq --&gt; Smoking\n  D --&gt; dbq[Diet Behavior & Nutrition]\n  dbq --&gt; Diet\n  D --&gt; paq[Physical Activity]\n  paq --&gt; p[Physical activities]\n  D --&gt; huq[Hospital Utilization & Access to Care]\n  huq --&gt; mm[Medical access]\n  D --&gt; bpx[Blood Pressure]\n  bpx --&gt; sbp[Systolic Blood Pressure]\n  bpx --&gt; dbp[Diastolic Blood Pressure]\n  D --&gt; bpq[Blood Pressure & Cholesterol]\n  bpq --&gt; hc[High cholesterol]\n  D --&gt; slq[Sleep Disorders]\n  slq --&gt; Sleep\n  D --&gt; biopro[Standard Biochemistry Profile]\n  biopro --&gt; u[Uric acid]\n  biopro --&gt; Protein\n  biopro --&gt; Bilirubin\n  biopro --&gt; Phosphorus\n  biopro --&gt; Sodium\n  biopro --&gt; Potassium\n  biopro --&gt; Globulin\n  biopro --&gt; Calcium\n  D --&gt; rxq[Prescription Medications - ICD-10-CM codes]\n  style D fill:#FFA500;\n  style rxq fill:#00FF00;\n  style biopro fill:#00FF00;\n  style slq fill:#00FF00;\n  style bpq fill:#00FF00;\n  style bpx fill:#00FF00;\n  style huq fill:#00FF00;\n  style paq fill:#00FF00;\n  style dbq fill:#00FF00;\n  style smq fill:#00FF00;\n  style diq fill:#00FF00;\n  style bmi fill:#00FF00;\n  style demo fill:#00FF00;\n\n\n\n\n\n\n\n\n\nWe do the same for the following cycles:\n\nNHANES 2015-16\nNHANES 2017-18",
    "crumbs": [
      "Motivating example",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data to Analyze</span>"
    ]
  },
  {
    "objectID": "data.html#identify-measured-and-unmeasured-variables-in-the-data",
    "href": "data.html#identify-measured-and-unmeasured-variables-in-the-data",
    "title": "1  Data to Analyze",
    "section": "1.4 Identify measured and unmeasured variables in the data",
    "text": "1.4 Identify measured and unmeasured variables in the data\nFind variables capturing the following concepts in the data based on a hypothesized DAG.\n\n\n\n\nRole\nData Component\nVariables considered based on DAG\n\n\n\n\nOutcome\nDIQ\nHave diabetes1\n\n\nExposure\nBMX\nObese; BMI &gt;= 30\n\n\nConfounder\n(demographic) DEMO\nAge, Sex, Education, Race/ethnicity, Marital status, Annual household income, County of birth, Survey cycle year\n\n\n\n(behaviour) SMQ, PAQ, SLQ, DBQ\nSmoking2, Vigorous work activity, Sleep3, Diet4\n\n\n\n(health history / access) DIQ, HUQ\nDiabetes family history, Access to care5\n\n\n\n(lab) BPX, BPQ, BIOPRO\nBlood pressure (systolic, diastolic6), Cholesterol, Uric acid, Total Protein, Total Bilirubin, Phosphorus, Sodium, Potassium, Globulin, Total Calcium\n\n\n\n\n\n\n14 demographic, behavioral, health history related variables\n\nMostly categorical\n\n11 lab variables\n\nMostly continuous",
    "crumbs": [
      "Motivating example",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data to Analyze</span>"
    ]
  },
  {
    "objectID": "data.html#fitting-crude-model-to-obtain-or",
    "href": "data.html#fitting-crude-model-to-obtain-or",
    "title": "1  Data to Analyze",
    "section": "1.5 Fitting crude model to obtain OR",
    "text": "1.5 Fitting crude model to obtain OR\n\n\n\n\n\n\nCrude association\n\n\n\nHere we estimate the crude association between the exposure and the outcome.\n\n\n\nout.formula &lt;- as.formula(\"outcome ~ exposure\")\nfit &lt;- glm(out.formula,\n            data = hdps.data,\n            family= binomial(link = \"logit\"))\nfit.summary &lt;- summary(fit)$coef[\"exposure\",\n                                 c(\"Estimate\", \n                                   \"Std. Error\", \n                                   \"Pr(&gt;|z|)\")]\nfit.ci &lt;- confint(fit, level = 0.95)[\"exposure\", ]\nfit.summary_with_ci.crude &lt;- c(fit.summary, fit.ci)\nknitr::kable(t(round(fit.summary_with_ci.crude, 2)))\n\n\n\n\nEstimate\nStd. Error\nPr(&gt;|z|)\n2.5 %\n97.5 %\n\n\n\n\n0.66\n0.08\n0\n0.51\n0.81\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisease Control, Centers for, and Prevention. 2021. “National Health and Nutrition Examination Survey (NHANES).” National Center for Health Statistics.\n\n\nGreenland, Sander, Judea Pearl, and James M Robins. 1999. “Causal Diagrams for Epidemiologic Research.” Epidemiology, 37–48.",
    "crumbs": [
      "Motivating example",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data to Analyze</span>"
    ]
  },
  {
    "objectID": "data.html#footnotes",
    "href": "data.html#footnotes",
    "title": "1  Data to Analyze",
    "section": "",
    "text": "combination of (a) Doctor told you have diabetes, (b) Taking insulin now, (c) Take diabetic pills to lower blood sugar.↩︎\ncigarette use (at least 100 cigarettes in life)↩︎\nSleep hours/workdays↩︎\nHow healthy is the diet↩︎\nRoutine place to go for healthcare↩︎\naverage of 4 measurements↩︎",
    "crumbs": [
      "Motivating example",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data to Analyze</span>"
    ]
  },
  {
    "objectID": "psipw.html",
    "href": "psipw.html",
    "title": "2  Propensity score",
    "section": "",
    "text": "2.1 Propensity Score Analysis\nThere are four approaches to propensity score (PS) analysis:",
    "crumbs": [
      "Motivating example",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Propensity score</span>"
    ]
  },
  {
    "objectID": "psipw.html#propensity-score-analysis-1",
    "href": "psipw.html#propensity-score-analysis-1",
    "title": "2  Propensity score",
    "section": "",
    "text": "Weighting: Assign weights to individuals based on their propensity scores to create a pseudo-population where treatment groups are balanced.\nMatching: Match individuals in the treatment group with individuals in the control group based on their propensity scores.\nStratification: Divide the sample into strata based on the propensity score and compare outcomes within each stratum.\nCovariate Adjustment: Include the propensity score as a covariate in a outcome model to adjust for confounding.",
    "crumbs": [
      "Motivating example",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Propensity score</span>"
    ]
  },
  {
    "objectID": "psipw.html#propensity-score-weighting",
    "href": "psipw.html#propensity-score-weighting",
    "title": "2  Propensity score",
    "section": "2.2 Propensity Score Weighting",
    "text": "2.2 Propensity Score Weighting\nFor this demonstration, we will focus on the Weighting approach. The other approaches are not covered in this demonstration, but they can be implemented using similar steps as shown below.\n\n\n\n\n\n\n\n\n\nThere are four steps in propensity score weighting (after data preparation):\n\nData preparation: Prepare the data by creating the treatment/exposure, outcome, and covariates.\nSpecifying PS & fit model: Specify the propensity score model with investigator-specified measured covariates and fit the model\nWeighting: Convert PS to inverse probability weights (IPW).\nCovariate balance: Check the balance of covariates between treatment groups after weighting.\nEstimating treatment effect: Fit the outcome model on the pseudo population.\n\n\n2.2.1 Step 0: Data preparation\n\n2.2.1.1 Creating Analytic data\n3 cycles of NHANES datasets were - downloaded from the US CDC website - recoded for consistency, and - merged together to make an analytic data.\nDetails of data download process, and recoding and merging are discussed in Appendix.\n\n\n\n\n\nflowchart LR\n  A[NHANES] --&gt; C1(2013-2014 cycle) --&gt; ss1(10,175&lt;br&gt;participants)\n  A --&gt; C2(2015-2016 cycle) --&gt; ss2(9,971&lt;br&gt;participants)\n  A --&gt; C3(2017-2018 cycle) --&gt; ss3(9,254&lt;br&gt;participants)\n\n  ss(7,585&lt;br&gt;participants&lt;br&gt;after imposing&lt;br&gt;eligibility criteria)\n\n  ss1 --&gt; ss\n  ss2 --&gt; ss\n  ss3 --&gt; ss\n\n  %% 1. Define a reusable style class named 'customOrange'\n  classDef customOrange fill:#FFA500,color:#333,stroke:#A66C00\n\n  %% 2. Apply the class to the desired nodes\n  class A,C1,C2,C3,ss1,ss2,ss3,ss customOrange;\n\n\n\n\n\n\n\n\nOur study population was restricted to the U.S. population who were\n\n20 years or older and\nnot pregnant at the time of survey data collection, and\nwho had available International Classification of Diseases (ICD) codes to ensure we can extract sufficient proxy information for the analysis (discussed in step 1).\n\nTo simplify the analysis, we only considered complete case data.\n\n\n\n\n\n\nImportant\n\n\n\nThis table compares our treated and untreated groups before any statistical adjustment.\nIt answers the question: ‘Are these two groups comparable at the start?’\n\nIn observational studies (unlike randomized trials), these groups are often very different.\nWe use the Standardized Mean Difference (SMD) to measure imbalance.\n\nUnlike p-values, SMD is independent of sample size, making it a reliable metric for comparing group differences regardless of how large our dataset is.\nIf you see variables with an SMD &gt; 0.1, these are variables that differ between our groups. These imbalances act as confounders that could bias our results if we don’t adjust for them.\n\n\n\n\n\n# Table 1\nlibrary(tableone)\ntab1 &lt;- CreateTableOne(vars = investigator.specified.covariates, \n                       strata = \"exposure\",\n                       data = hdps.data, \n                       test = FALSE)\nprint(tab1, \n      showAllLevels = TRUE, \n      noSpaces = TRUE, \n      quote = FALSE, \n      smd = TRUE, \n      printToggle = FALSE) |&gt;\n  kbl(caption = \"Table 1: Baseline Characteristics by Exposure Group\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"), \n                full_width = FALSE)\n\n\nTable 1: Baseline Characteristics by Exposure Group\n\n\n\nlevel\n0\n1\nSMD\n\n\n\n\nn\n\n2223\n1616\n\n\n\nage.cat (%)\n20-49\n703 (31.6)\n528 (32.7)\n0.149\n\n\n\n50-64\n673 (30.3)\n579 (35.8)\n\n\n\n\n65+\n847 (38.1)\n509 (31.5)\n\n\n\nsex (%)\nMale\n1009 (45.4)\n648 (40.1)\n0.107\n\n\n\nFemale\n1214 (54.6)\n968 (59.9)\n\n\n\neducation (%)\nLess than high school\n322 (14.5)\n248 (15.3)\n0.242\n\n\n\nHigh school\n951 (42.8)\n860 (53.2)\n\n\n\n\nCollege graduate or above\n950 (42.7)\n508 (31.4)\n\n\n\nrace (%)\nWhite\n933 (42.0)\n677 (41.9)\n0.452\n\n\n\nBlack\n302 (13.6)\n367 (22.7)\n\n\n\n\nHispanic\n453 (20.4)\n424 (26.2)\n\n\n\n\nOthers\n535 (24.1)\n148 (9.2)\n\n\n\nmarital (%)\nNever married\n274 (12.3)\n196 (12.1)\n0.115\n\n\n\nMarried/with partner\n1432 (64.4)\n964 (59.7)\n\n\n\n\nOther\n517 (23.3)\n456 (28.2)\n\n\n\nincome (%)\nless than $20,000\n364 (16.4)\n300 (18.6)\n0.184\n\n\n\n$20,000 to $74,999\n984 (44.3)\n821 (50.8)\n\n\n\n\n$75,000 and Over\n875 (39.4)\n495 (30.6)\n\n\n\nborn (%)\nBorn in US\n1342 (60.4)\n1170 (72.4)\n0.257\n\n\n\nOther place\n881 (39.6)\n446 (27.6)\n\n\n\nyear (%)\nNHANES 2013-2014 public release\n1026 (46.2)\n703 (43.5)\n0.090\n\n\n\nNHANES 2015-2016 public release\n305 (13.7)\n195 (12.1)\n\n\n\n\nNHANES 2017-2018 public release\n892 (40.1)\n718 (44.4)\n\n\n\ndiabetes.family.history (%)\nNo\n1900 (85.5)\n1251 (77.4)\n0.208\n\n\n\nYes\n323 (14.5)\n365 (22.6)\n\n\n\nmedical.access (%)\nNo\n150 (6.7)\n71 (4.4)\n0.103\n\n\n\nYes\n2073 (93.3)\n1545 (95.6)\n\n\n\nsmoking (%)\nNever smoker\n1350 (60.7)\n943 (58.4)\n0.095\n\n\n\nPrevious smoker\n576 (25.9)\n484 (30.0)\n\n\n\n\nCurrent smoker\n297 (13.4)\n189 (11.7)\n\n\n\ndiet.healthy (%)\nPoor or fair\n436 (19.6)\n615 (38.1)\n0.487\n\n\n\nGood\n904 (40.7)\n650 (40.2)\n\n\n\n\nVery good or excellent\n883 (39.7)\n351 (21.7)\n\n\n\nphysical.activity (%)\nNo\n1901 (85.5)\n1317 (81.5)\n0.108\n\n\n\nYes\n322 (14.5)\n299 (18.5)\n\n\n\nsleep (mean (SD))\n\n7.40 (1.48)\n7.30 (1.60)\n0.067\n\n\nuric.acid (mean (SD))\n\n5.25 (1.38)\n5.81 (1.53)\n0.383\n\n\nprotein.total (mean (SD))\n\n7.08 (0.46)\n7.06 (0.45)\n0.049\n\n\nbilirubin.total (mean (SD))\n\n0.58 (0.29)\n0.52 (0.33)\n0.193\n\n\nphosphorus (mean (SD))\n\n3.74 (0.55)\n3.68 (0.58)\n0.109\n\n\nsodium (mean (SD))\n\n139.83 (2.62)\n139.74 (2.74)\n0.031\n\n\npotassium (mean (SD))\n\n4.05 (0.39)\n4.07 (0.39)\n0.046\n\n\nglobulin (mean (SD))\n\n2.87 (0.46)\n3.00 (0.46)\n0.289\n\n\ncalcium.total (mean (SD))\n\n9.41 (0.39)\n9.34 (0.40)\n0.166\n\n\nsystolicBP (mean (SD))\n\n126.07 (19.52)\n129.23 (17.72)\n0.169\n\n\ndiastolicBP (mean (SD))\n\n70.17 (11.59)\n72.35 (11.82)\n0.186\n\n\nhigh.cholesterol (%)\nNo\n1137 (51.1)\n756 (46.8)\n0.087\n\n\n\nYes\n1086 (48.9)\n860 (53.2)\n\n\n\n\n\n\n\n\n\n2.2.2 Step 1: Specifying PS & fit model\nWe build the propensity score model in this data using the investigator-specified covariates.\n\n\n\n\n\n\n\n\n\n\n\nC = investigator-specified covariates.\n\nIf you are somewhat unfamiliar with propensity score paradigm, look at tutorials dedicated towards that topic. There are additional tutorials also talking about propensity score weighting.\n\n\n2.2.2.1 PS model specification\nNow let us create the propensity score formula with the investigator-specified covariates:\n\ncovform &lt;- paste0(investigator.specified.covariates, collapse = \"+\")\nps.formula &lt;- as.formula(paste0(\"exposure\", \"~\", covform))\nps.formula\n#&gt; exposure ~ age.cat + sex + education + race + marital + income + \n#&gt;     born + year + diabetes.family.history + medical.access + \n#&gt;     smoking + diet.healthy + physical.activity + sleep + uric.acid + \n#&gt;     protein.total + bilirubin.total + phosphorus + sodium + potassium + \n#&gt;     globulin + calcium.total + systolicBP + diastolicBP + high.cholesterol\n\n\n\n\n\n\n\nVariable Selection Strategy\n\n\n\nIn standard Propensity Score analysis, we rely on Investigator-Specified Covariates. According to causal theory, we should include:\n\n✅ Confounders: Variables causing both Exposure and Outcome.\n✅ Risk Factors: Variables causing the Outcome (to improve precision)\n\nWe must avoid:\n\n❌ Instruments: Variables causing Exposure only (increases variance/bias).\n❌ Colliders: Effects of both Exposure and Outcome (induces bias).\n\nTransition to hdPS: Later, we will see how hdPS automates this selection when we have thousands of potential proxies!\n\n\n\n\n\nOnly use investigator specified covariates to build the formula.\nDuring the construction of the propensity score model, researchers should consider incorporating additional model specifications, such as interactions and polynomials, if they are deemed necessary.\n\n\n\n2.2.2.2 Fit the PS model\n\nrequire(WeightIt)\nW.out &lt;- weightit(ps.formula, \n                    data = hdps.data, \n                    estimand = \"ATE\",\n                    method = \"ps\")\n\n\n\n\nUse that formula to estimate propensity scores.\nIn this demonstration, we did not use stabilize = TRUE. However, stabilized propensity score weights often reduce the variance of treatment effect estimates.\n\n\n\n2.2.2.3 Obtain PS\n\nhdps.data$ps &lt;- W.out$ps\nggplot(hdps.data, aes(x = ps, fill = factor(exposure))) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = c(\"darkblue\", \"darkred\")) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nCheck propensity score overlap in both exposure groups.\n\n\n\n2.2.3 Step 2: Weighting\nAs mentioned, we only talk about inverse probability weighting in our current context.\nThe specific formula used to calculate the weights depends on the estimand of interest. Let \\(Z\\) be the binary treatment indicator (\\(Z=1\\) for treated, \\(Z=0\\) for control) and \\(e\\) be the estimated propensity score.\nAverage Treatment Effect (ATE): (this is what we are using here) The goal is to estimate the effect if the entire population were treated versus if the entire population were untreated. \\[w_{ATE} = \\frac{Z}{e} + \\frac{1-Z}{1-e}\\]\nAverage Treatment Effect on the Treated (ATT): The goal is to estimate the effect specifically for those who actually received the treatment. \\[w_{ATT} = Z + \\frac{e(1-Z)}{1-e}\\]\n\nhdps.data$w &lt;- W.out$weights\nsummary(hdps.data$w)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   1.016   1.277   1.559   2.008   2.144  45.795\n\n\nggplot(hdps.data, aes(x = \"\", y = w)) +\n  geom_boxplot(fill = \"lightblue\", \n               color = \"blue\", \n               size = 1) +\n  geom_text(aes(x = 1, y = max(w), \n                label = paste0(\"Max = \", round(max(w), 2))), \n            vjust = 1.5, \n            hjust = -0.3, \n            size = 4, \n            color = \"red\") +\n  geom_text(aes(x = 1, y = min(w), \n                label = paste0(\"Min = \", round(min(w), 2))), \n            vjust = -2.5, \n            hjust = -0.3, \n            size = 4, \n            color = \"red\") +\n  ggtitle(\"Boxplot of Inverse Probability Weights\") +\n  xlab(\"\") +\n  ylab(\"Weights\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nCheck the summary statistics of the weights to assess whether there are extreme weights. Less extreme weights now?\n\n\n\n2.2.4 Step 3: Covariate balance\n\nrequire(cobalt)\nlove.plot(x = W.out,\n          thresholds = c(m = .1), \n          var.order = \"unadjusted\",\n          stars = \"raw\")\n\n\n\n\n\n\n\n\n\n\n\nAssess balance against SMD 0.1. Still balanced?\nPredictive measures such as c-statistics are not helpful in this context (Westreich et al. 2011): “use of the c-statistic as a guide in constructing propensity scores may result in less overlap in propensity scores between treated and untreated subjects”!\n\n\n\n\n\n\n\nWarning\n\n\n\nYou balance the groups without looking at the outcome (blinded). You only proceed to the outcome analysis once you have achieved balance (SMD &lt; 0.1). This mimics a Randomized Controlled Trial (RCT).\n\n\n\n# Extract balance statistics including SMD\nbal_stats &lt;- bal.tab(W.out, \n                     stats = c(\"m\", \"v\"), # 'm' for mean diff (SMD), 'v' for variance ratios\n                     thresholds = c(m = 0.1)) # Highlight SMDs &gt; 0.1\n\n# Display the balance table as a nice kable\nbal_stats$Balance %&gt;%\n  select(Diff.Adj) %&gt;% # Select only the adjusted (weighted) SMD column\n  arrange(desc(abs(Diff.Adj))) %&gt;% # Sort by absolute SMD to see worst imbalance first\n  kbl(caption = \"Standardized Mean Differences (SMD) After Weighting\",\n      digits = 3) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"), \n                full_width = FALSE)\n\n\nStandardized Mean Differences (SMD) After Weighting\n\n\n\nDiff.Adj\n\n\n\n\nbilirubin.total\n0.085\n\n\nglobulin\n-0.058\n\n\ncalcium.total\n0.041\n\n\nsodium\n0.040\n\n\nprop.score\n-0.030\n\n\nprotein.total\n-0.026\n\n\nage.cat_50-64\n-0.022\n\n\nyear_NHANES 2013-2014 public release\n0.019\n\n\nage.cat_20-49\n0.017\n\n\npotassium\n0.017\n\n\nincome_less than $20,000\n-0.016\n\n\nrace_White\n0.016\n\n\ndiastolicBP\n-0.015\n\n\ndiet.healthy_Poor or fair\n-0.012\n\n\nrace_Others\n-0.011\n\n\nyear_NHANES 2015-2016 public release\n-0.010\n\n\nyear_NHANES 2017-2018 public release\n-0.010\n\n\nsleep\n0.009\n\n\nincome_$20,000 to $74,999\n0.008\n\n\nincome_$75,000 and Over\n0.008\n\n\nsmoking_Current smoker\n-0.008\n\n\nmarital_Married/with partner\n-0.007\n\n\nsmoking_Never smoker\n0.007\n\n\neducation_High school\n0.006\n\n\ndiet.healthy_Very good or excellent\n0.006\n\n\nuric.acid\n-0.006\n\n\nmedical.access_Yes\n-0.006\n\n\ndiet.healthy_Good\n0.006\n\n\nsex_Female\n-0.005\n\n\ndiabetes.family.history_Yes\n0.005\n\n\nhigh.cholesterol_Yes\n0.005\n\n\nage.cat_65+\n0.005\n\n\nmarital_Never married\n0.004\n\n\neducation_College graduate or above\n-0.004\n\n\nphosphorus\n0.004\n\n\nsystolicBP\n-0.003\n\n\nrace_Black\n-0.003\n\n\nmarital_Other\n0.003\n\n\nborn_Other place\n-0.003\n\n\neducation_Less than high school\n-0.003\n\n\nrace_Hispanic\n-0.002\n\n\nphysical.activity_Yes\n-0.002\n\n\nsmoking_Previous smoker\n0.001\n\n\n\n\n\n\n\n2.2.5 Step 4: Estimating treatment effect\n\n2.2.5.1 Set outcome formula\n\nout.formula &lt;- as.formula(paste0(\"outcome\", \"~\", \"exposure\"))\nout.formula\n#&gt; outcome ~ exposure\n\n\n\nWe are again using a crude weighted outcome model here.\n\n\n2.2.5.2 Obtain OR\n\nfit &lt;- glm(out.formula,\n            data = hdps.data,\n            weights = W.out$weights,\n            family= binomial(link = \"logit\"))\nfit.summary &lt;- summary(fit)$coef[\"exposure\",\n                                 c(\"Estimate\", \n                                   \"Std. Error\", \n                                   \"Pr(&gt;|z|)\")]\nfit.summary[2] &lt;- sqrt(sandwich::sandwich(fit)[2,2])\nrequire(lmtest)\nconf.int &lt;- confint(fit, \"exposure\", level = 0.95, method = \"hc1\")\n\nfit.summary_with_ci.ps &lt;- c(fit.summary, conf.int)\nknitr::kable(t(round(fit.summary_with_ci.ps,2))) \n\n\n\n\nEstimate\nStd. Error\nPr(&gt;|z|)\n2.5 %\n97.5 %\n\n\n\n\n0.64\n0.1\n0\n0.53\n0.75\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.5.3 Obtain RD\n\nfit &lt;- glm(out.formula,\n            data = hdps.data,\n            weights = W.out$weights,\n            family= gaussian(link = \"identity\"))\nfit.summary &lt;- summary(fit)$coef[\"exposure\",\n                                 c(\"Estimate\", \n                                   \"Std. Error\", \n                                   \"Pr(&gt;|t|)\")]\nfit.summary[2] &lt;- sqrt(sandwich::sandwich(fit)[2,2])\nrequire(lmtest)\nconf.int &lt;- confint(fit, \"exposure\", level = 0.95, method = \"hc1\")\nfit.summary_with_ci.ps.rd &lt;- c(fit.summary, conf.int)\nknitr::kable(t(round(fit.summary_with_ci.ps.rd,2))) \n\n\n\n\nEstimate\nStd. Error\nPr(&gt;|t|)\n2.5 %\n97.5 %\n\n\n\n\n0.11\n0.02\n0\n0.09\n0.14",
    "crumbs": [
      "Motivating example",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Propensity score</span>"
    ]
  },
  {
    "objectID": "psipw.html#summary-of-causal-inference-methods",
    "href": "psipw.html#summary-of-causal-inference-methods",
    "title": "2  Propensity score",
    "section": "2.3 Summary of Causal Inference Methods",
    "text": "2.3 Summary of Causal Inference Methods\n\n\n\n\n\n\n\n\n\nMethod Category\nSpecific Methods\nAdvantages\nLimitations\n\n\n\n\nRegression & Outcome Modeling\nRegression, G-Computation, Decision Trees, Random Forest\n• Efficiency: High statistical efficiency (small SEs) if correctly specified.• Flexibility (ML): ML versions handle non-linearities/interactions automatically.\n• Model Dependence: Biased if outcome model is misspecified.• No Design Phase: Risk of “p-hacking” as analysis is not blinded to outcome.• Inference: Standard SEs/CIs often invalid for pure ML methods.\n\n\nPropensity Score (Exposure Modeling)\nPS Matching, PS Weighting\n• Design Separation: Balances groups blind to outcome (mimics RCT).• Dimension Reduction: Summarizes confounders into one score.• Diagnostics: Forces check of “common support” (overlap).\n• Overfitting: High-dimensional data can lead to perfect prediction and extreme weights.• Instability: Sensitive to lack of overlap (positivity violations).• Precision: Less efficient than correct regression.\n\n\nDouble Robust Methods\nAugmented IPW, TMLE, Double ML, Causal Forest\n• Double Robustness: Unbiased if either outcome or exposure model is correct.• Valid Inference with ML: Allows use of ML for variable selection with valid CIs/SEs.\n• Complexity: Computationally intensive and harder to implement/interpret.• Positivity Sensitivity: Can still be unstable with severe overlap issues.\n\n\nInstrumental Variable Methods\n2SLS\n• Unmeasured Confounding: Can account for unmeasured confounders (unlike all others above).\n• Assumption Heavy: Valid instruments are extremely rare in observational health data.• Efficiency: Large standard errors compared to other methods.\n\n\n\n\n\n\n\nWestreich, Daniel, Stephen R Cole, Michele Jonsson Funk, M Alan Brookhart, and Til Stürmer. 2011. “The Role of the c-Statistic in Variable Selection for Propensity Score Models.” Pharmacoepidemiology and Drug Safety 20 (3): 317–20.",
    "crumbs": [
      "Motivating example",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Propensity score</span>"
    ]
  },
  {
    "objectID": "proxy.html",
    "href": "proxy.html",
    "title": "3  Reducing residual confounding",
    "section": "",
    "text": "3.1 Measuring comorbidity burden\nIn health research, the overall health status/ Disease burden could be a potential confounding factor. In the original DAG, we had comorbidity as a known confounder.",
    "crumbs": [
      "Motivating example",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reducing residual confounding</span>"
    ]
  },
  {
    "objectID": "proxy.html#measuring-comorbidity-burden",
    "href": "proxy.html#measuring-comorbidity-burden",
    "title": "3  Reducing residual confounding",
    "section": "",
    "text": "flowchart TB\n  A[Obesity] --&gt; Y(Diabete)\n  L[Comorbidity measure unobserved] --&gt; Y\n  L --&gt; A\n  style A fill:#90EE90;\n  style Y fill:#ADD8E6;\n  style L fill:#FF0000;\n\n\n\n\n\n\n\n\nCharlson Comorbidity Index (CCI) is a measure that quantifies the burden of comorbidities or pre-existing medical conditions in patients (takes into account 17 comorbidities), which can impact their health outcomes and overall survival.\nElixhauser Comorbidity Index (ECI) is a measure of the burden of comorbidities, based on 30 different comorbid conditions.\nChronic Disease Score (CDS) is a weighted score of the number and severity of chronic diseases, calculated using self-reported data on diagnosed conditions (considers the presence of 21 chronic conditions).\n\n\n\n\n(Charlson et al. 1987; Elixhauser et al. 1998; Von Korff, Wagner, and Saunders 1992)\nNHANES does not include information on all of the comorbidities included in theses scores / indices.\n\n\n\n\n\n\n\nResidual confounding\n\n\n\nComorbidity scores are widely used as a measure of comorbidity burden, and their calculation often relies on data that may not be available in certain contexts, such as in NHANES or Canadian health administrative databases. In such cases, when comorbidity burden is a known confounder, researchers may use proxy information to approximate and mimic the information. Not being able to adjust for such variable can introduce bias and residual confounding in the treatment effect estimation.\n\n\n\n\n\n(Schneeweiss and Maclure 2000; L. Lix et al. 2011; L. M. Lix et al. 2013)",
    "crumbs": [
      "Motivating example",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reducing residual confounding</span>"
    ]
  },
  {
    "objectID": "proxy.html#proxy-adjustment-empirical-criterion",
    "href": "proxy.html#proxy-adjustment-empirical-criterion",
    "title": "3  Reducing residual confounding",
    "section": "3.2 Proxy Adjustment Empirical criterion",
    "text": "3.2 Proxy Adjustment Empirical criterion\nEmpirical criterion: Modified disjunctive cause criterion\nVanderWeele et al. 2019 European Journal of Epidemiology: CC BY license\n\n\n\n\n\nHypothesized Directed acyclic graph with comorbidity measure being unmeasured, and approximated by the simple count measures based on the ICD codes\n\n\n\n\n\n\nAdjust for variables that are (a) causes of exposure or outcome or both, (b) discard: known instrument, (c) including good proxies for unmeasured common causes (VanderWeele 2019)",
    "crumbs": [
      "Motivating example",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reducing residual confounding</span>"
    ]
  },
  {
    "objectID": "proxy.html#additional-information-icd-10-cm",
    "href": "proxy.html#additional-information-icd-10-cm",
    "title": "3  Reducing residual confounding",
    "section": "3.3 Additional information: ICD-10-CM",
    "text": "3.3 Additional information: ICD-10-CM\n\n\nThe International Classification of Diseases 10th Revision (ICD-10) is a standardized system of codes for the classification of diseases, disorders, and injuries.\n\n\n\nRole\nData Source\nVariables considered\n\n\n\n\nRole unclear as they may not directly relate to the research question\nRXQ_RX\nPrescription medication ICD-10-CM code\n\n\n\n\n\nRXQ_RX questionnaire (a) collects information on prescription medications taken in the past 30 days, (b) conducted by trained interviewers, and (c) with some quality control efforts.\n\n\n\n\n\nExamples of ICD-10-CM codes (3-7 characters, 1st character being alpha, 2-end are numberic, often with a dot) assigned to reasons for using medication (see Appendix in NHANES RXQ_RX component)\n\n\n\n\n\n\nWe have a lot of information through these ICD-10-CM codes, but for most of these information, it is unclear what role they play within the context of our research questions.\nCount of prescriptions is often used to measure comorbidity burden. This is not a perfect measure. But could serve as a proxy for our purpose.\n\n\n\n\nPrescription medication (ICD-10-CM codes from all 3 cycles) data was liked with the initial data.\n\n\nCharlson, Mary E, Peter Pompei, Kathy L Ales, and C Ronald MacKenzie. 1987. “A New Method of Classifying Prognostic Comorbidity in Longitudinal Studies: Development and Validation.” Journal of Chronic Diseases 40 (5): 373–83.\n\n\nElixhauser, Anne, Claudia Steiner, D Robert Harris, and Rosanna M Coffey. 1998. “Comorbidity Measures for Use with Administrative Data.” Medical Care, 8–27.\n\n\nLix, Lisa M, Jacqueline Quail, Opeyemi Fadahunsi, and Gary F Teare. 2013. “Predictive Performance of Comorbidity Measures in Administrative Databases for Diabetes Cohorts.” BMC Health Services Research 13: 1–12.\n\n\nLix, LM, J Quail, G Teare, and B Acan. 2011. “Performance of Comorbidity Measures for Predicting Outcomes in Population-Based Osteoporosis Cohorts.” Osteoporosis International 22: 2633–43.\n\n\nSchneeweiss, Sebastian, and Malcolm Maclure. 2000. “Use of Comorbidity Scores for Control of Confounding in Studies Using Administrative Databases.” International Journal of Epidemiology 29 (5): 891–98.\n\n\nVanderWeele, Tyler J. 2019. “Principles of Confounder Selection.” European Journal of Epidemiology 34: 211–19.\n\n\nVon Korff, Michael, Edward H Wagner, and Kathleen Saunders. 1992. “A Chronic Disease Score from Automated Pharmacy Data.” Journal of Clinical Epidemiology 45 (2): 197–203.",
    "crumbs": [
      "Motivating example",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reducing residual confounding</span>"
    ]
  },
  {
    "objectID": "hdps.html",
    "href": "hdps.html",
    "title": "High-dimensional Propensity score",
    "section": "",
    "text": "Origin",
    "crumbs": [
      "High-dimensional Propensity score"
    ]
  },
  {
    "objectID": "hdps.html#origin",
    "href": "hdps.html#origin",
    "title": "High-dimensional Propensity score",
    "section": "",
    "text": "(Schneeweiss et al. 2009)",
    "crumbs": [
      "High-dimensional Propensity score"
    ]
  },
  {
    "objectID": "hdps.html#key-idea",
    "href": "hdps.html#key-idea",
    "title": "High-dimensional Propensity score",
    "section": "Key idea",
    "text": "Key idea\nSchneeweiss et al. 2009 extended to a variety of classifications to code diagnoses (ICD), procedure (CPT), medications (eg, NDC, AHFS, ATCC), or others (PCP, LOINC).\n\n\n\n\n\n\n\n\n\n\n\nCPT-4 (Current Procedural Terminology, 4th edition), ICD-9 (International Classification of Diseases, 9th edition), PCP visits (Primary Care Physician visits), NDC (National Drug Code), and ATC (Anatomical Therapeutic Chemical classification) are all codes or measures commonly used in healthcare and medical research.\nSchneeweiss et al. 2018 Clinical Epidemiology: CC BY license\n\n\n(Schneeweiss 2018)\n\n\n\n\n\n\nAdjust useful proxies\n\n\n\nIn administrative data sources, the main idea of hdPS (high-dimensional propensity score) is to adjust for proxies that are empirically associated with the outcome of interest, which may not be directly measured in the data.\n\n\n\n\nWith hdPS, users do not need to know which unmeasured confounders are being adjusted for by proxy information.\n\nAdjusting for something that may not be interpretable directly with the context of the research question.\nLogic: measures from same subject should be correlated = has relevant proxy information\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSchneeweiss, Sebastian. 2018. “Automated Data-Adaptive Analytics for Electronic Healthcare Data to Study Causal Treatment Effects.” Clinical Epidemiology, 771–88.\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn, Helen Mogun, and M Alan Brookhart. 2009. “High-Dimensional Propensity Score Adjustment in Studies of Treatment Effects Using Health Care Claims Data.” Epidemiology (Cambridge, Mass.) 20 (4): 512.",
    "crumbs": [
      "High-dimensional Propensity score"
    ]
  },
  {
    "objectID": "step1.html",
    "href": "step1.html",
    "title": "4  Step 1: Proxy sources",
    "section": "",
    "text": "4.1 Data with investigator-specified variables\nanalytic &lt;- data.complete\nidx &lt;- analytic$id\noutcome &lt;- as.numeric(analytic$diabetes == \"Yes\") \nexposure &lt;- as.numeric(analytic$obese == \"Yes\")\ndomain &lt;- \"dx\"\nanalytic.dfx &lt;- as.data.frame(cbind(idx, exposure, outcome, domain))",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Step 1: Proxy sources</span>"
    ]
  },
  {
    "objectID": "step1.html#data-with-investigator-specified-variables",
    "href": "step1.html#data-with-investigator-specified-variables",
    "title": "4  Step 1: Proxy sources",
    "section": "",
    "text": "Data: part 1\n\n\n\nWe will work with the data.complete data for the investigator-specified information.\n\n\n\n\n\nWe prepare the minimal analytic data only with the following 4 information:\n\nidentifying information (idx)\nexposure (obese)\noutcome (diabetes)\ndomain of the codes (dx). In this example we only have prescription domain (1 domain dx)",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Step 1: Proxy sources</span>"
    ]
  },
  {
    "objectID": "step1.html#proxy-data",
    "href": "step1.html#proxy-data",
    "title": "4  Step 1: Proxy sources",
    "section": "4.2 Proxy data",
    "text": "4.2 Proxy data\n\n4.2.1 Identify the data dimensions (proxy sources)\nIn this example we only have prescription domain (1 domain dx of ICD-10-CM code). Hence \\(p = 1\\) in this exercise.\n\n\nNHANES Questionnaire collects information on: (a) dietary supplements, (b) nonprescription antacids, (c) prescription medications, and (d) preventive aspirin use.\n\n\n4.2.2 Define a covariate assessment period (CAP)\n\n\n\n\n\n\n\n\n\n\n\n(Connolly et al. 2019; Schneeweiss et al. 2009)\nWe only collect proxy information from a well-defined CAP. In our case, it was \\(30\\) days.\n\n\nNHANES asked “In the past 30 days, have you used or taken medication for which a prescription is needed? Do not include prescription vitamins or minerals you may have already told me about.”\n\n\n\n\n\n\nData: part 2\n\n\n\nWe will work with the merge proxy data (ICD-10 codes) from 3 cycles: dat.proxy.long.\n\n\n\n\n4.2.3 Omit duplicated information\n\n\nWe need to delete codes that could be close proxies of exposure and/or outcome, or other investigator specified covariates we have already selected in step0.\n\n\n\n\n\n\n\n\n\n\ndat.proxy.long &lt;- subset(dat.proxy.long, \n                         icd10 != \"E66\") # Overweight and obesity\ndat.proxy.long &lt;- subset(dat.proxy.long, \n                         icd10 != \"O24\") # Gestational diabetes mellitus\ndat.proxy.long &lt;- subset(dat.proxy.long, \n                         icd10 != \"E10\") # Type 1 diabetes mellitus\ndat.proxy.long &lt;- subset(dat.proxy.long, \n                         icd10 != \"E11\") # Type 2 diabetes mellitus\n\n\n\n\nWe delete codes associated with exposure and outcome.\nSame should be done for any other proxies that may have duplicating information compared to the investigator-specified covariates.\n\n\n\n4.2.4 Long format proxy data\n\n\nHere is an example of 3 digit codes for 1 patient with subject ID “100001”. We create the same for all patients.\n\n\n\n\n\nID\nICD 10 codes (3 digit)\nDescription\n\n\n\n\n100001\nF33\nMajor depressive disorder, recurrent\n\n\n100001\nI10\nHypertension\n\n\n100001\nM62\nMuscle spasm\n\n\n100001\nF32\nMajor depressive disorder, single episode\n\n\n100001\nM25\nJoint disorder/pain\n\n\n100001\nK21\nGastro-esophageal reflux disease\n\n\n100001\nM79\nmusculoskeletal pain conditions\n\n\n100001\nR12\nHeartburn",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Step 1: Proxy sources</span>"
    ]
  },
  {
    "objectID": "step1.html#merge-proxy-data-with-analytic-data",
    "href": "step1.html#merge-proxy-data-with-analytic-data",
    "title": "4  Step 1: Proxy sources",
    "section": "4.3 Merge Proxy data with Analytic data",
    "text": "4.3 Merge Proxy data with Analytic data\n\n\n\n\n\n\nMerged Data: parts 1 and 2\n\n\n\n\nWe will work with the merge proxy data with analytic data.\nThat will provide us with the IDs (idx) of the subject that have proxy (ICD-10) information associated with them.\n\n\n\n\nrequire(dplyr) \ndfx &lt;- merge(analytic.dfx, proxy.var.long, by = \"idx\")\nhead(dfx)\n\n\n  \n\n\nbasetable &lt;- dfx %&gt;% select(idx, exposure, outcome) %&gt;% distinct()\npatientIds &lt;- basetable$idx\nlength(patientIds)\n#&gt; [1] 3839\n\n\n\n\n\nConnolly, John G, Sebastian Schneeweiss, Robert J Glynn, and Joshua J Gagne. 2019. “Quantifying Bias Reduction with Fixed-Duration Versus All-Available Covariate Assessment Periods.” Pharmacoepidemiology and Drug Safety 28 (5): 665–70.\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn, Helen Mogun, and M Alan Brookhart. 2009. “High-Dimensional Propensity Score Adjustment in Studies of Treatment Effects Using Health Care Claims Data.” Epidemiology (Cambridge, Mass.) 20 (4): 512.",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Step 1: Proxy sources</span>"
    ]
  },
  {
    "objectID": "step2.html",
    "href": "step2.html",
    "title": "5  Step 2: Empirical",
    "section": "",
    "text": "5.1 Sort by prevalence\nBased on the merged dataset, we identify which patients were linked in both databases. Using those IDs, we want to sort the list of candidate empirical covariates.\nCheck out the frequency of each codes:\nlibrary(dplyr)\ndf &lt;- data.frame(\n  icd10 = names(sort(table(dfx$icd10), decreasing = TRUE)),\n  count = sort(table(dfx$icd10), decreasing = TRUE)\n)\nICD10 Code Frequencies\n\n\nICD10 Code\nCount\n\n\n\n\nI10\n2775\n\n\nE78\n1517\n\n\nF32\n536\n\n\nF41\n524\n\n\nK21\n441\n\n\nM79\n401\n\n\nE03\n397\n\n\nM54\n314\n\n\nG47\n307\n\n\nJ45\n301\nHowever, some may be associated with lower counts (e.g., less than 20).\nIf there were more dimensions, separate list of candidate empirical covariates would be identified.",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Step 2: Empirical</span>"
    ]
  },
  {
    "objectID": "step2.html#sort-by-prevalence",
    "href": "step2.html#sort-by-prevalence",
    "title": "5  Step 2: Empirical",
    "section": "",
    "text": "Only top 10 prevalent codes are shown.\n\n\n\n\n\n\n\nRestrictions\n\n\n\nCandidate empirical covariates list is constrained by\n\ntheir prevalence of codes. Only top n covariates with highest prevalence would be chosen.\nanalysts absolutely need to get rid of the codes that have zero variance (e.g., everyone has the code, or nobody has it).\ncodes associated with very low prevalence are also numerically problematic for further analyses.\n\n\n\n\n\nWe choose n = 200 [for (1)] as it was proposed in the original algorithm (Schneeweiss et al. 2009). In reality, this is not necessary to be so restrictive (Schuster, Pang, and Platt 2015). Parts (2) and (3) are more likely and addressed by the following restriction: At least min_num_patients number of patients need to have that code to be selected in the list.",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Step 2: Empirical</span>"
    ]
  },
  {
    "objectID": "step2.html#choose-granularity",
    "href": "step2.html#choose-granularity",
    "title": "5  Step 2: Empirical",
    "section": "5.2 Choose Granularity",
    "text": "5.2 Choose Granularity\nOne important point here is that we have chosen granularity to be 3 digits in the ICD-10 code.\n\n\nWe have already truncated the codes at 3 digit level while preparing the data.",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Step 2: Empirical</span>"
    ]
  },
  {
    "objectID": "step2.html#retain-top-n-empirical-covariates",
    "href": "step2.html#retain-top-n-empirical-covariates",
    "title": "5  Step 2: Empirical",
    "section": "5.3 Retain top n empirical covariates",
    "text": "5.3 Retain top n empirical covariates\n\nrequire(autoCovariateSelection)\nstep1 &lt;- get_candidate_covariates(df = dfx,  \n                                  domainVarname = \"domain\",\n                                  eventCodeVarname = \"icd10\", \n                                  patientIdVarname = \"idx\",\n                                  patientIdVector = patientIds,\n                                  n = 200, \n                                  min_num_patients = 20)\n\n\n\nYou can use autoCovariateSelection package to implement these restrictions (Robert 2020).\n\n5.3.1 Long format data\n\nout1 &lt;- step1$covars_data\nhead(out1)\n\n\n  \n\n\n\n\n\n5.3.2 Updated frequency data\n\ndf2 &lt;- data.frame(\n  icd10 = names(table(out1$icd10)),\n  count = as.numeric(table(out1$icd10))\n)\n\n\n\n\n\n\nICD10 Code\nCount\n\n\n\n\ndx_A49\n28\n\n\ndx_B00\n20\n\n\ndx_B35\n22\n\n\ndx_C50\n31\n\n\ndx_D75\n136\n\n\ndx_E03\n397\n\n\n\n\n\n\n\nOnly first few code frequencies are shown (alphabetic order), that were selected based on the restrictions n = 200 and min_num_patients = 20.\n\n\n\n\n\n\nICD10 Code\nCount\n\n\n\n\n77\ndx_R52\n40\n\n\n78\ndx_R60\n187\n\n\n79\ndx_R73\n202\n\n\n80\ndx_T14\n82\n\n\n81\ndx_T78\n96\n\n\n82\ndx_Z79\n277\n\n\n\n\n\n\n\nOnly last few code frequencies are shown (alphabetic order).\n\n\n5.3.3 Total number of codes retained\n\nnrow(df2)\n#&gt; [1] 82\n\n\n\n\n\nRobert, Dennis. 2020. autoCovariateSelection: Automatic Covariate Selection. https://CRAN.R-project.org/package=autoCovariateSelection.\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn, Helen Mogun, and M Alan Brookhart. 2009. “High-Dimensional Propensity Score Adjustment in Studies of Treatment Effects Using Health Care Claims Data.” Epidemiology (Cambridge, Mass.) 20 (4): 512.\n\n\nSchuster, Tibor, Menglan Pang, and Robert W Platt. 2015. “On the Role of Marginal Confounder Prevalence–Implications for the High-Dimensional Propensity Score Algorithm.” Pharmacoepidemiology and Drug Safety 24 (9): 1004–7.",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Step 2: Empirical</span>"
    ]
  },
  {
    "objectID": "step3.html",
    "href": "step3.html",
    "title": "6  Step 3: Recurrence",
    "section": "",
    "text": "6.1 Genrate recurrence covariates\nIn this step, we generate 3 binary recurrence covariates for each of the candidate empirical covariates identified in the previous step:\nstep2 &lt;- get_recurrence_covariates(df = out1, \n                                   patientIdVarname = \"idx\",\n                                   eventCodeVarname = \"icd10\", \n                                   patientIdVector = patientIds)",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Step 3: Recurrence</span>"
    ]
  },
  {
    "objectID": "step3.html#genrate-recurrence-covariates",
    "href": "step3.html#genrate-recurrence-covariates",
    "title": "6  Step 3: Recurrence",
    "section": "",
    "text": "(Schneeweiss et al. 2009)\n\n\noccurred at least once\noccurred sporadically (at least more than the median)\noccurred frequently (at least more than the 75th percentile)",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Step 3: Recurrence</span>"
    ]
  },
  {
    "objectID": "step3.html#example-of-recurrence-covariates",
    "href": "step3.html#example-of-recurrence-covariates",
    "title": "6  Step 3: Recurrence",
    "section": "6.2 Example of recurrence covariates",
    "text": "6.2 Example of recurrence covariates\n\n\n\n\n\n\n\n\n\n\nICD-10-CM code (dimension 1)\ncode appeared at least once\ncode appeared at least more than the median\ncode appeared at least more than the 75th percentile\n\n\n\n\nD64.9 Anemia\nrec_dx_D64_once\nrec_dx_D64_sporadic\nrec_dx_D64_frequent\n\n\nD75.9P Blood clots\nrec_dx_D75_once\nrec_dx_D75_sporadic\nrec_dx_D75_frequent\n\n\nD89.9 Immune disorder\nrec_dx_D89_once\nrec_dx_D89_sporadic\nrec_dx_D89_frequent\n\n\n\\(\\ldots\\)\n\\(\\ldots\\)\n\\(\\ldots\\)\n\\(\\ldots\\)\n\n\nE07.9 Disorder of thyroid\nrec_dx_E07_once\nrec_dx_E07_sporadic\nrec_dx_E07_frequent\n\n\n\n\n\nExample of 3 binary covariates (hypothetical) created based on the candidate empirical covariates.",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Step 3: Recurrence</span>"
    ]
  },
  {
    "objectID": "step3.html#recurrence-covariates-in-the-data",
    "href": "step3.html#recurrence-covariates-in-the-data",
    "title": "6  Step 3: Recurrence",
    "section": "6.3 Recurrence covariates in the data",
    "text": "6.3 Recurrence covariates in the data\n\nout2 &lt;- step2$recurrence_data\nncol(out2)-1\n#&gt; [1] 91\n\n\n\n\n  \n\n\n\n\n\nHere we show binary recurrence covariates for only 2 columns",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Step 3: Recurrence</span>"
    ]
  },
  {
    "objectID": "step3.html#refined-recurrence-covariates",
    "href": "step3.html#refined-recurrence-covariates",
    "title": "6  Step 3: Recurrence",
    "section": "6.4 Refined recurrence covariates",
    "text": "6.4 Refined recurrence covariates\nBelow you can click to see a list of all recurrence covariates obtained in our data.\n\nShow/Hide Table\n\n\n\nICD-10 Recurrence Data\n\n\n1\nrec_dx_A49_once\nrec_dx_B00_once\nrec_dx_B35_once\n\n\n2\nrec_dx_C50_once\nrec_dx_D75_once\nrec_dx_E03_once\n\n\n3\nrec_dx_E04_once\nrec_dx_E07_once\nrec_dx_E78_once\n\n\n4\nrec_dx_E87_once\nrec_dx_F31_once\nrec_dx_F31_frequent\n\n\n5\nrec_dx_F32_once\nrec_dx_F39_once\nrec_dx_F41_once\n\n\n6\nrec_dx_F43_once\nrec_dx_F90_once\nrec_dx_G25_once\n\n\n7\nrec_dx_G40_once\nrec_dx_G40_frequent\nrec_dx_G43_once\n\n\n8\nrec_dx_G47_once\nrec_dx_H04_once\nrec_dx_H40_once\n\n\n9\nrec_dx_H40_frequent\nrec_dx_I10_once\nrec_dx_I10_frequent\n\n\n10\nrec_dx_I20_once\nrec_dx_I21_once\nrec_dx_I48_once\n\n\n11\nrec_dx_I48_frequent\nrec_dx_I49_once\nrec_dx_I50_once\n\n\n12\nrec_dx_I50_frequent\nrec_dx_I51_once\nrec_dx_I63_once\n\n\n13\nrec_dx_J30_once\nrec_dx_J42_once\nrec_dx_J44_once\n\n\n14\nrec_dx_J44_frequent\nrec_dx_J45_once\nrec_dx_J45_frequent\n\n\n15\nrec_dx_K04_once\nrec_dx_K08_once\nrec_dx_K21_once\n\n\n16\nrec_dx_K25_once\nrec_dx_K27_once\nrec_dx_K30_once\n\n\n17\nrec_dx_K59_once\nrec_dx_K92_once\nrec_dx_L40_once\n\n\n18\nrec_dx_L70_once\nrec_dx_M06_once\nrec_dx_M06_frequent\n\n\n19\nrec_dx_M10_once\nrec_dx_M13_once\nrec_dx_M19_once\n\n\n20\nrec_dx_M1A_once\nrec_dx_M25_once\nrec_dx_M54_once\n\n\n21\nrec_dx_M62_once\nrec_dx_M79_once\nrec_dx_M81_once\n\n\n22\nrec_dx_N28_once\nrec_dx_N32_once\nrec_dx_N39_once\n\n\n23\nrec_dx_N40_once\nrec_dx_N92_once\nrec_dx_N94_once\n\n\n24\nrec_dx_N95_once\nrec_dx_R00_once\nrec_dx_R05_once\n\n\n25\nrec_dx_R06_once\nrec_dx_R07_once\nrec_dx_R09_once\n\n\n26\nrec_dx_R10_once\nrec_dx_R11_once\nrec_dx_R12_once\n\n\n27\nrec_dx_R25_once\nrec_dx_R32_once\nrec_dx_R35_once\n\n\n28\nrec_dx_R39_once\nrec_dx_R41_once\nrec_dx_R42_once\n\n\n29\nrec_dx_R51_once\nrec_dx_R52_once\nrec_dx_R60_once\n\n\n30\nrec_dx_R73_once\nrec_dx_T14_once\nrec_dx_T78_once\n\n\n31\nrec_dx_Z79_once\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nGiven that we had one dimension of proxy data, \\(p=1\\), at most \\(n=200\\) most prevalent codes (with the restriction that minimum number of patients in each code = 20), and \\(3\\) intensity, we could theoretically have at most \\(p \\times n \\times 3 = 1 \\times 200 \\times \\ 3 = 600\\) recurrence covariates.\n\n\n\n\nBased on all of the restrictions, we created 91 distinct recurrence covariates.\nThe merged data (analytic and proxies) size is now 7,585.\n\n\n\n\n\n\nIf 2 or all 3 recurrence covariates are identical, only one distinct recurrence covariate is returned. This is why you do not see any sporadic recurrence covariate here.\nRecurrence covariate creation is for each patient, and it is possible to have same code occur multiple time because we are working with a 3 digit granularity (possible to have medications from other codes within same ICD-10 3 digit granularity).\n\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn, Helen Mogun, and M Alan Brookhart. 2009. “High-Dimensional Propensity Score Adjustment in Studies of Treatment Effects Using Health Care Claims Data.” Epidemiology (Cambridge, Mass.) 20 (4): 512.",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Step 3: Recurrence</span>"
    ]
  },
  {
    "objectID": "step4.html",
    "href": "step4.html",
    "title": "7  Step 4: Prioritize",
    "section": "",
    "text": "7.1 Bross formula\nWe need to make an educated guess about 3 components (i.e., make an assumption), that are used in the calculation of bias contributed by not adjusting for a covariate based on Bross (1966) formula:\nThe above components can help us calculate \\(bias\\) amount (known as ‘Bias Multiplier’) using the Bross formula when we omit adjusting for \\(U\\):\n\\[\\text{Bias}_U = \\frac{P_{UA_1} (RR_{UY} - 1) + 1}{P_{UA_0} (RR_{UY} - 1) + 1}\\]",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Step 4: Prioritize</span>"
    ]
  },
  {
    "objectID": "step4.html#bross-formula",
    "href": "step4.html#bross-formula",
    "title": "7  Step 4: Prioritize",
    "section": "",
    "text": "Bross formula (Bross 1966; Schneeweiss 2006) for the Bias Multiplier considers both the imbalance in the prevalence of the unmeasured confounder between the exposure groups and the association between the confounder and the outcome to assess the potential bias.\n\nprevalence of a binary unmeasured confounder (\\(U\\)) among exposed (\\(P_{UA_1}\\))\nprevalence of that binary unmeasured confounder among unexposed (\\(P_{UA_0}\\))\nassociation between that binary unmeasured confounder and the outcome (\\(RR_{UY} = \\frac{P_{UY_1}}{P_{UY_1}}\\))\n\n\n\n\n\nThese are the ingredients of the Bross formula. This formula is helpful for understanding the impact of unmeasured confounding of a binary variable. We have to put assumed prevalence and risk ratio associated with an unmeasured confounder.",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Step 4: Prioritize</span>"
    ]
  },
  {
    "objectID": "step4.html#calculating-bias-from-a-recurrence-covariate",
    "href": "step4.html#calculating-bias-from-a-recurrence-covariate",
    "title": "7  Step 4: Prioritize",
    "section": "7.2 Calculating bias from a recurrence covariate",
    "text": "7.2 Calculating bias from a recurrence covariate\nFor recurrence covariates (\\(R\\)), we do not need to assume, we just plug-in \\(R\\) instead of \\(U\\) in the following calculations:\n\nprevalence of a binary recurrence variable among exposed (\\(P_{RA_1}\\))\nprevalence of that binary recurrence variable among unexposed (\\(P_{RA_0}\\))\nassociation between that binary recurrence variable and the outcome (\\(RR_{RY} = \\frac{P_{RY_1}}{P_{RY_1}}\\))\n\nThese components can help us empirically calculate \\(bias\\) amount:\n\\[\\text{Bias}_R = \\frac{P_{RA_1} (RR_{RY} - 1) + 1}{P_{RA_0} (RR_{RY} - 1) + 1}\\]\nHere, \\(RR_{RY}\\) is the crude risk ratio between the recurrence covariate and the outcome, \\(Y\\) is the outcome, \\(A\\) is the exposure, and \\(R\\) is a recurrence covariate.\n\n\nFor recurrence covariates, we do not need to assume, we can basically calculate these numbers (\\(log-absolute-bias\\)) for all of the recurrence covariates (Schneeweiss et al. 2009). For each data dimension, we can rank each of the recurrence covariates based on the amount of bias (confounding or imbalance) it could likely adjust.",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Step 4: Prioritize</span>"
    ]
  },
  {
    "objectID": "step4.html#calculating-bias-from-all-recurrence-covariates",
    "href": "step4.html#calculating-bias-from-all-recurrence-covariates",
    "title": "7  Step 4: Prioritize",
    "section": "7.3 Calculating bias from all recurrence covariates",
    "text": "7.3 Calculating bias from all recurrence covariates\nIn our example, we simply plug-in each recurrence covariates one-by-one to calculate \\(log-absolute-bias\\):\n\n\n\n\n\nR=rec_dx_D64_once\n\n\nR=rec_dx_D75_sporadic\n\n\n…\n\n\nR=rec_dx_E07_frequent",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Step 4: Prioritize</span>"
    ]
  },
  {
    "objectID": "step4.html#obtain-log-of-absolute-bias",
    "href": "step4.html#obtain-log-of-absolute-bias",
    "title": "7  Step 4: Prioritize",
    "section": "7.4 Obtain log of absolute-bias",
    "text": "7.4 Obtain log of absolute-bias\nWe calculate \\(log-absolute-bias\\) for all recurrence covariates.\n\n\nAbsolute log of the Bias Multiplier, \\(log-absolute-bias\\), is a symmetric measure of the potential bias introduced by the recurrence covariate, making it easier to compare and rank recurrence covariates.\n\nout3 &lt;- get_prioritised_covariates(df = out2,\n                                   patientIdVarname = \"idx\", \n                                   exposureVector = basetable$exposure,\n                                   outcomeVector = basetable$outcome,\n                                   patientIdVector = patientIds, \n                                   k = 50)\nsorted_values &lt;- sort(out3$multiplicative_bias, \n                      decreasing = TRUE)\n\nThis would return absolute log of the multiplicative bias for each recurrence covariate (by univariate Bross formula). We can use this information to prioritize recurrence covariates in the next step.",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Step 4: Prioritize</span>"
    ]
  },
  {
    "objectID": "step4.html#convert-to-absolute-log-of-multiplicative-bias",
    "href": "step4.html#convert-to-absolute-log-of-multiplicative-bias",
    "title": "7  Step 4: Prioritize",
    "section": "7.5 Convert to Absolute log of multiplicative bias",
    "text": "7.5 Convert to Absolute log of multiplicative bias\nHere are the few covariates and associated Absolute log of the multiplicative bias:\n\n\n\n\n\n\n\n\n\n\nrec_dx_I10_once : 0.124\n\n\nrec_dx_R73_once : 0.078\n\n\nrec_dx_I10_frequent : 0.065\n\n\nrec_dx_R60_once : 0.038\n\n\nrec_dx_E78_once : 0.036\n\n\nrec_dx_M79_once : 0.033\n\n\nrec_dx_I51_once : 0.019\n\n\nrec_dx_M10_once : 0.017\n\n\nrec_dx_I50_once : 0.016\n\n\n\n\n\nAnd here are translated table with description:\n\n\n\n\n\n\n\n\n\n\nHypertension : 0.115\n\n\nElevated blood glucose level : 0.088\n\n\nHypertension : 0.068\n\n\nEdema : 0.054\n\n\nPure hypercholesterolemia : 0.038\n\n\nmusculoskeletal pain : 0.017\n\n\nHypokalemia : 0.015\n\n\nHeart disease : 0.013\n\n\nHeart failure : 0.011\n\n\n\n\n\n\n\nSome of the empirical covariates with top Absolute log of the multiplicative bias are actually relevant to the outcome (diabetes): Hypertension, Elevated blood glucose level , etc. (Choi and Shi 2001)\n\n\n\n\n\n\nSMD vs Bias multiplier\n\n\n\nStandardized mean difference (SMD) is useful for assessing the balance in the propensity score literature. However, Bross formula incorporates outcome information. In the investigation of empirical covariates or recurrence covariates where interpretations of these covariates are unknown, it may seem more safe to use the multiplicative bias term from the Bross formula to identify proxy covariates that are helpful in predicting the outcome.\n\n\n\n\n\n\n(Stuart, Lee, and Leacy 2013)\n\n\nBross, Irwin DJ. 1966. “Spurious Effects from an Extraneous Variable.” Journal of Chronic Diseases 19 (6): 637–47.\n\n\nChoi, BCK, and F Shi. 2001. “Risk Factors for Diabetes Mellitus by Age and Sex: Results of the National Population Health Survey.” Diabetologia 44: 1221–31.\n\n\nSchneeweiss, Sebastian. 2006. “Sensitivity Analysis and External Adjustment for Unmeasured Confounders in Epidemiologic Database Studies of Therapeutics.” Pharmacoepidemiology and Drug Safety 15 (5): 291–303.\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn, Helen Mogun, and M Alan Brookhart. 2009. “High-Dimensional Propensity Score Adjustment in Studies of Treatment Effects Using Health Care Claims Data.” Epidemiology (Cambridge, Mass.) 20 (4): 512.\n\n\nStuart, Elizabeth A, Brian K Lee, and Finbarr P Leacy. 2013. “Prognostic Score–Based Balance Measures Can Be a Useful Diagnostic for Propensity Score Methods in Comparative Effectiveness Research.” Journal of Clinical Epidemiology 66 (8): S84–90.",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Step 4: Prioritize</span>"
    ]
  },
  {
    "objectID": "step5.html",
    "href": "step5.html",
    "title": "8  Step 5: Covariates",
    "section": "",
    "text": "8.1 Ideal number of prioritised covariates\nWe select 2 types of covariates for the next step (to analyze using propensity score or other alternative approaches):\nBased on calculated \\(log-absolute-bias\\), we select top k recurrence covariates to be used in the hdPS analyses later. Below is a plot of all of the absolute log of the Bias Multiplier:\nWe used \\(k = 50\\) covariates selected by the hdPS algorithm (we call them ‘hdPS covariates’). What should be the cutpoint?",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Step 5: Covariates</span>"
    ]
  },
  {
    "objectID": "step5.html#ideal-number-of-prioritised-covariates",
    "href": "step5.html#ideal-number-of-prioritised-covariates",
    "title": "8  Step 5: Covariates",
    "section": "",
    "text": "Absolute log of the Bias Multiplier has a null value of 0. Anything above 0 is an indication of confounding bias adjusted by the adjustment of the associated recurrent covariate.\nFor large proxy data sources, \\(k = 500\\) is suggested (Schneeweiss et al. 2009).\nSee Sensitivity Analysis section for an understanding of how to choose a value based on an ad-hoc process.",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Step 5: Covariates</span>"
    ]
  },
  {
    "objectID": "step5.html#selected-hdps-variables-proxies",
    "href": "step5.html#selected-hdps-variables-proxies",
    "title": "8  Step 5: Covariates",
    "section": "8.2 Selected hdPS variables (proxies)",
    "text": "8.2 Selected hdPS variables (proxies)\n\nhdps.dim &lt;- out3$autoselected_covariate_df\ndim(hdps.dim) # id + k\n#&gt; [1] 3839   51\nhead(hdps.dim)[,1:3]\n\n\n  \n\n\nhdps.dim$id &lt;- hdps.dim$idx\nhdps.dim$idx &lt;- NULL",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Step 5: Covariates</span>"
    ]
  },
  {
    "objectID": "step5.html#investigator-specified-covariates",
    "href": "step5.html#investigator-specified-covariates",
    "title": "8  Step 5: Covariates",
    "section": "8.3 Investigator-specified covariates",
    "text": "8.3 Investigator-specified covariates\n\\(25\\) investigator-specified covariates are selected based on variables in the DAG that are available in the data set.\n\n\nWe should also add necessary interactions of these investigator-specified covariates, or add other useful model-specifications (e.g., polynomials).\n\n\n\n\n\nHypothesized Directed acyclic graph drawn based on analyst’s best understanding of the literature\n\n\n\n\n\n\n\n14 demographic, behavioral, health history related variables/access\n\nMostly categorical\n\n11 lab variables\n\nMostly continuous\n\n\n\nexposure &lt;- \"obese\"\noutcome &lt;- \"diabetes\" \ninvestigator.specified.covariates &lt;- \n  c(# Demographic\n  \"age.cat\", \"sex\", \"education\", \"race\", \n  \"marital\", \"income\", \"born\", \"year\",\n  \n  # health history related variables/access\n  \"diabetes.family.history\", \"medical.access\",\n  \n  # behavioral\n  \"smoking\", \"diet.healthy\", \"physical.activity\", \"sleep\",\n  \n  # Laboratory \n  \"uric.acid\", \"protein.total\", \"bilirubin.total\", \"phosphorus\",\n  \"sodium\", \"potassium\", \"globulin\", \"calcium.total\", \n  \"systolicBP\", \"diastolicBP\", \"high.cholesterol\"\n)\nlength(investigator.specified.covariates)\n#&gt; [1] 25",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Step 5: Covariates</span>"
    ]
  },
  {
    "objectID": "step5.html#merged-data",
    "href": "step5.html#merged-data",
    "title": "8  Step 5: Covariates",
    "section": "8.4 Merged data",
    "text": "8.4 Merged data\n\nload(\"data/analytic3cycles.RData\")\nhdps.data &lt;- merge(data.complete[,c(\"id\",\n                                    outcome, \n                                    exposure, \n                                    investigator.specified.covariates)], \n                       hdps.dim, by = \"id\")\ndim(hdps.data)\n#&gt; [1] 3839   78\n\n\n\n\n\nVariable count (78)\n\n1 ID variable\n1 exposure\n1 outcome\n25 investigator-specified covariates\n50 hdPS variables\n\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn, Helen Mogun, and M Alan Brookhart. 2009. “High-Dimensional Propensity Score Adjustment in Studies of Treatment Effects Using Health Care Claims Data.” Epidemiology (Cambridge, Mass.) 20 (4): 512.",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Step 5: Covariates</span>"
    ]
  },
  {
    "objectID": "step6.html",
    "href": "step6.html",
    "title": "9  Step 6: Propensity",
    "section": "",
    "text": "9.1 hdPS model\nThen the hdPS can be used as matching, weighting, stratifying variables, or as covariates (usually in deciles) in outcome model.",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Step 6: Propensity</span>"
    ]
  },
  {
    "objectID": "step6.html#hdps-model",
    "href": "step6.html#hdps-model",
    "title": "9  Step 6: Propensity",
    "section": "",
    "text": "C = investigator-specified covariates and EC = hdPS covariates (Schneeweiss et al. 2009)\n\n\n\n(Wyss et al. 2022)\n\n9.1.1 Create propensity score formula\n\nhdps.data$exposure &lt;- as.numeric(I(hdps.data$obese=='Yes'))\nhdps.data$outcome &lt;- as.numeric(I(hdps.data$diabetes=='Yes'))\nproxy.list.sel &lt;- names(out3$autoselected_covariate_df[,-1])\nproxyform &lt;- paste0(proxy.list.sel, collapse = \"+\")\ncovform &lt;- paste0(investigator.specified.covariates, collapse = \"+\")\n\n\nrhsformula &lt;- paste0(c(covform, proxyform), collapse = \"+\")\nps.formula &lt;- as.formula(paste0(\"exposure\", \"~\", rhsformula))\nps.formula\n#&gt; exposure ~ age.cat + sex + education + race + marital + income + \n#&gt;     born + year + diabetes.family.history + medical.access + \n#&gt;     smoking + diet.healthy + physical.activity + sleep + uric.acid + \n#&gt;     protein.total + bilirubin.total + phosphorus + sodium + potassium + \n#&gt;     globulin + calcium.total + systolicBP + diastolicBP + high.cholesterol + \n#&gt;     rec_dx_I10_once + rec_dx_R73_once + rec_dx_I10_frequent + \n#&gt;     rec_dx_R60_once + rec_dx_E78_once + rec_dx_M79_once + rec_dx_I51_once + \n#&gt;     rec_dx_M10_once + rec_dx_I50_once + rec_dx_K21_once + rec_dx_D75_once + \n#&gt;     rec_dx_Z79_once + rec_dx_F41_once + rec_dx_M1A_once + rec_dx_E87_once + \n#&gt;     rec_dx_R12_once + rec_dx_R51_once + rec_dx_J45_once + rec_dx_I50_frequent + \n#&gt;     rec_dx_L70_once + rec_dx_M25_once + rec_dx_I63_once + rec_dx_R39_once + \n#&gt;     rec_dx_N28_once + rec_dx_K25_once + rec_dx_F90_once + rec_dx_B00_once + \n#&gt;     rec_dx_J42_once + rec_dx_R41_once + rec_dx_I20_once + rec_dx_M54_once + \n#&gt;     rec_dx_J44_once + rec_dx_K08_once + rec_dx_I21_once + rec_dx_F32_once + \n#&gt;     rec_dx_J30_once + rec_dx_F43_once + rec_dx_R06_once + rec_dx_I48_once + \n#&gt;     rec_dx_R32_once + rec_dx_R42_once + rec_dx_N92_once + rec_dx_N95_once + \n#&gt;     rec_dx_M19_once + rec_dx_E07_once + rec_dx_R25_once + rec_dx_G43_once + \n#&gt;     rec_dx_R52_once + rec_dx_M81_once + rec_dx_T78_once\n\n\n\nThis is an overly simplistic scenario where we are adding only the main effects in the non-transformed form.\n\n\n9.1.2 Fit PS model\n\nrequire(WeightIt)\nW.out &lt;- weightit(ps.formula, \n                    data = hdps.data, \n                    estimand = \"ATE\",\n                    method = \"ps\")\n\n\n\n9.1.3 Obtain PS\n\nhdps.data$ps &lt;- W.out$ps\n\n\n\n\n\n\n\n\n\n\n\n\nAlways a good idea to check propensity score overlap in both exposure groups\n\n\n9.1.4 Obtain Weights\n\nhdps.data$w &lt;- W.out$weights\n\n\n\n\n\n\n\n\n\n\n\n\nAlways a good idea to check the summary statistics of the weights to assess whether there are extreme weights\n\n\n9.1.5 Assessing balance\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlways a good idea to assess balance. Here we are measuring against SMD 0.1. Use love.plot function from the cobalt package. See more descriptions of balanced diagnostics elsewhere for a propensity score context.\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn, Helen Mogun, and M Alan Brookhart. 2009. “High-Dimensional Propensity Score Adjustment in Studies of Treatment Effects Using Health Care Claims Data.” Epidemiology (Cambridge, Mass.) 20 (4): 512.\n\n\nWyss, Richard, Chen Yanover, Tal El-Hay, Dimitri Bennett, Robert W Platt, Andrew R Zullo, Grammati Sari, et al. 2022. “Machine Learning for Improving High-Dimensional Proxy Confounder Adjustment in Healthcare Database Studies: An Overview of the Current Literature.” Pharmacoepidemiology and Drug Safety 31 (9): 932–43.",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Step 6: Propensity</span>"
    ]
  },
  {
    "objectID": "step7.html",
    "href": "step7.html",
    "title": "10  Step 7: Association",
    "section": "",
    "text": "10.0.1 Set outcome formula\n\nout.formula &lt;- as.formula(paste0(\"outcome\", \"~\", \"exposure\"))\nout.formula\n#&gt; outcome ~ exposure\n\n\n\n10.0.2 Obtain OR from unadjusted model\n\nfit &lt;- glm(out.formula,\n            data = hdps.data,\n            weights = W.out$weights,\n            family= binomial(link = \"logit\"))\nfit.summary &lt;- summary(fit)$coef[\"exposure\",\n                                 c(\"Estimate\", \n                                   \"Std. Error\", \n                                   \"Pr(&gt;|z|)\")]\nfit.summary[2] &lt;- sqrt(sandwich::sandwich(fit)[2,2])\nrequire(lmtest)\nconf.int &lt;- confint(fit, \"exposure\", level = 0.95, method = \"hc1\")\nfit.summary_with_ci &lt;- c(fit.summary, conf.int)\nknitr::kable(t(round(fit.summary_with_ci,2))) \n\n\n\n\nEstimate\nStd. Error\nPr(&gt;|z|)\n2.5 %\n97.5 %\n\n\n\n\n0.35\n0.12\n0\n0.25\n0.46\n\n\n\n\n\n\n\n\nWe are using a crude outcome model here.\nSomewhat controversial to adjust for all (investigator-specified and all 100 proxies) covariates.\n\n\n\n\n\n\n\n\n\n\n\n\n10.0.3 Obtain RD from unadjusted model\n\nfit &lt;- glm(out.formula,\n            data = hdps.data,\n            weights = W.out$weights,\n            family= gaussian(link = \"identity\"))\nfit.summary &lt;- summary(fit)$coef[\"exposure\",\n                                 c(\"Estimate\", \n                                   \"Std. Error\", \n                                   \"Pr(&gt;|t|)\")]\nfit.summary[2] &lt;- sqrt(sandwich::sandwich(fit)[2,2])\nrequire(lmtest)\nconf.int &lt;- confint(fit, \"exposure\", level = 0.95, method = \"hc1\")\nfit.summary &lt;- c(fit.summary, conf.int)\nknitr::kable(t(round(fit.summary, 2))) \n\n\n\n\nEstimate\nStd. Error\nPr(&gt;|t|)\n2.5 %\n97.5 %\n\n\n\n\n0.06\n0.02\n0\n0.04\n0.09\n\n\n\n\n\n\n\n\n\n(Naimi and Whitcomb 2020)\n\n\nNaimi, Ashley I, and Brian W Whitcomb. 2020. “Estimating Risk Ratios and Risk Differences Using Regression.” American Journal of Epidemiology 189 (6): 508–10.",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Step 7: Association</span>"
    ]
  },
  {
    "objectID": "sens.html",
    "href": "sens.html",
    "title": "11  Sensitivity",
    "section": "",
    "text": "11.1 Sensitivity analysis for k",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Sensitivity</span>"
    ]
  },
  {
    "objectID": "sens.html#sensitivity-analysis-for-k",
    "href": "sens.html#sensitivity-analysis-for-k",
    "title": "11  Sensitivity",
    "section": "",
    "text": "11.1.1 Create propensity score formula\n\n\n\n\n\n\n\n\n\n\n\nHence we iterate the process (change k parameter in get_prioritised_covariates function in step 4) and obtain odds ratio (exponentiation of log-OR) for each k. We varied k from 10 to 90.\n\n\n\n\n\n\nTip\n\n\n\nFind out where OR estimates stabilizes",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Sensitivity</span>"
    ]
  },
  {
    "objectID": "sens.html#sensitivity-analysis-for-n",
    "href": "sens.html#sensitivity-analysis-for-n",
    "title": "11  Sensitivity",
    "section": "11.2 Sensitivity analysis for n",
    "text": "11.2 Sensitivity analysis for n\n\n\n\n\n\n\nTip\n\n\n\nWe varied n from 10 to 90, remaining everything else constant\n\n\n\n\n\n\n\n\n\n\n\n\n\nHence we iterate the process (change n parameter in get_candidate_covariates function step 2) and obtain odds ratio (exponentiation of log-OR) for each n. We varied n from 10 to 90.\n\n\n\n\n\n\nTip\n\n\n\nFind out where OR estimates stabilize\n\n\n\n\n\n\nLiterature suggested that this restriction of n can be detrimental (Schuster, Pang, and Platt 2015). Hence in the original analysis we chose n such that that is larger than available empirical covariates.\n\n\nSchuster, Tibor, Menglan Pang, and Robert W Platt. 2015. “On the Role of Marginal Confounder Prevalence–Implications for the High-Dimensional Propensity Score Algorithm.” Pharmacoepidemiology and Drug Safety 24 (9): 1004–7.",
    "crumbs": [
      "High-dimensional Propensity score",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Sensitivity</span>"
    ]
  },
  {
    "objectID": "extension.html",
    "href": "extension.html",
    "title": "Challenges",
    "section": "",
    "text": "Issues with hdPS",
    "crumbs": [
      "Challenges"
    ]
  },
  {
    "objectID": "extension.html#issues-with-hdps",
    "href": "extension.html#issues-with-hdps",
    "title": "Challenges",
    "section": "",
    "text": "Univariate selection of many proxies\n\n\n\n\nRecurrent covariates selected separately / univariately\ncan be correlated (coming from same patient) and cause multicollinearity\nmay inflate variance\nGeneral overfitting problem. Too many adjustment variables?\n\n\n\n\n\n(Franklin et al. 2015; Schuster, Lowe, and Platt 2016; Karim, Pang, and Platt 2018)",
    "crumbs": [
      "Challenges"
    ]
  },
  {
    "objectID": "extension.html#potential-ways-to-improve",
    "href": "extension.html#potential-ways-to-improve",
    "title": "Challenges",
    "section": "Potential ways to improve",
    "text": "Potential ways to improve\n\nMultiple recurrent covariates could provide same information, may not be useful anymore in the presence of others. Multivariate structure could be good to consider in a single model.\nMachine learning variable selection methods could be useful to combat multicollinearity.\nSample splitting methods could be useful in combating overfitting in high dimensions.\n\n\n\nCross-validation is embedded within super (ensemble) learning.",
    "crumbs": [
      "Challenges"
    ]
  },
  {
    "objectID": "extension.html#controversy",
    "href": "extension.html#controversy",
    "title": "Challenges",
    "section": "Controversy",
    "text": "Controversy\nResearchers argue that the PS model, which does not allow for data-driven selection of variables, is a more principled approach to adjusting for confounding in observational studies, without introducing any bias in the analysis .\nOther researchers argue that the hdPS approach can improve the precision of effect estimates by including additional variables that are empirically associated with both the exposure and the outcome, which may reduce residual confounding.\n\n\nMachine learning alternatives have the same criticism as some of them depend on association with the outcome.\n\n\n\n\n\n\nTip\n\n\n\nhdPS can only control for observed confounding, and cannot guarantee the direction or magnitude of residual confounding that may still exist. This is why sensitivity analyses and model diagnostics are important in assessing the robustness of hdPS results.\n\n\n\n\n\n\n(VanderWeele 2019)\n\n\nFranklin, Jessica M, Wesley Eddings, Robert J Glynn, and Sebastian Schneeweiss. 2015. “Regularized Regression Versus the High-Dimensional Propensity Score for Confounding Adjustment in Secondary Database Analyses.” American Journal of Epidemiology 182 (7): 651–59.\n\n\nKarim, Mohammad Ehsanul, Menglan Pang, and Robert W Platt. 2018. “Can We Train Machine Learning Methods to Outperform the High-Dimensional Propensity Score Algorithm?” Epidemiology 29 (2): 191–98.\n\n\nSchuster, Tibor, Wilfrid Kouokam Lowe, and Robert W Platt. 2016. “Propensity Score Model Overfitting Led to Inflated Variance of Estimated Odds Ratios.” Journal of Clinical Epidemiology 80: 97–106.\n\n\nVanderWeele, Tyler J. 2019. “Principles of Confounder Selection.” European Journal of Epidemiology 34: 211–19.",
    "crumbs": [
      "Challenges"
    ]
  },
  {
    "objectID": "pubmed.html",
    "href": "pubmed.html",
    "title": "12  Literature",
    "section": "",
    "text": "12.1 PubMed\nCombination of plasmode, simulation, high-dimensional propensity provides 7 papers (searched in April 23, 2023):",
    "crumbs": [
      "Challenges",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Literature</span>"
    ]
  },
  {
    "objectID": "pubmed.html#pubmed",
    "href": "pubmed.html#pubmed",
    "title": "12  Literature",
    "section": "",
    "text": "flowchart LR\n  A[PubMed] --&gt; p4(Karim et al. 2018&lt;br&gt;Epidemiology)\n  p4 --&gt; ml1\n  p4 --&gt; ml0[Hybrid]\n\n  A[PubMed] --&gt; p2(Tian et al. 2018&lt;br&gt;Int J Epidemiol.)\n  p2 --&gt; ml1[Pure LASSO]\n\n  A[PubMed] --&gt; p5(Wyss et al. 2018&lt;br&gt;Epidemiology)\n  p5 --&gt; sl1[vary k,&lt;br&gt;k=25,100:500&lt;br&gt;Super&lt;br&gt;Learner]\n  p5 --&gt; ct1\n\n  A[PubMed] --&gt; p1(Benasseur et al. 2022&lt;br&gt;Pharmacoepidemiol Drug Saf.)\n  p1 --&gt; ml2[Low k,&lt;br&gt;k = 10]\n  p1 --&gt; ct1[cTMLE]\n\n  A[PubMed] --&gt; p7(Neugebauer et al. 2015&lt;br&gt;Stat Med.)\n  p7 --&gt; O2[time-varying&lt;br&gt;interventions]\n\n  A[PubMed] --&gt; p6(Franklin et al. 2015&lt;br&gt;Am J Epidemiol.)\n  p6 --&gt; ml1\n  p6 --&gt; ml0\n\n  A[PubMed] --&gt; p3(Schneeweiss et al. 2018&lt;br&gt;Clin Epidemiol.)\n  p3 --&gt; O1[Review]\n\n  %% Define style classes\n  classDef redNode fill:#f44,stroke-width:2px,stroke:#f00,color:#fff\n  classDef yellowNode fill:#ffff00,stroke-width:2px,stroke:#ffcc00,color:#000\n  classDef greenNode fill:#9f9,stroke-width:2px,stroke:#090,color:#000\n\n  %% Apply classes to nodes\n  class p1,p3,p7 redNode\n  class p5 yellowNode\n  class p2,p4,p6 greenNode\n\n\n\n\n\n\n\n\n\n(Benasseur et al. 2022; Tian, Schuemie, and Suchard 2018; Franklin et al. 2015; Neugebauer et al. 2015; Wyss et al. 2018; Karim, Pang, and Platt 2018; Schneeweiss 2018)",
    "crumbs": [
      "Challenges",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Literature</span>"
    ]
  },
  {
    "objectID": "pubmed.html#outside-of-pubmed",
    "href": "pubmed.html#outside-of-pubmed",
    "title": "12  Literature",
    "section": "12.2 Outside of PubMed",
    "text": "12.2 Outside of PubMed\n\n\n\n\n\n\nflowchart LR\n  S[Simulations] --&gt; p0(Pang et al. 2016&lt;br&gt;Int. J Biostat.)\n  p0 --&gt; t1[TMLE,&lt;br&gt;No&lt;br&gt;super&lt;br&gt;learner]\n\n  D[Data&lt;br&gt;analysis] --&gt; p00(Pang et al. 2016&lt;br&gt;Epidemiology)\n  p00 --&gt; t1\n\n  D --&gt; p1(Ju et al. 2019&lt;br&gt;J App Stat.)\n  p1 --&gt; sl1[Super&lt;br&gt;learner,&lt;br&gt;No TMLE,&lt;br&gt;bias not&lt;br&gt;used as a&lt;br&gt;performance&lt;br&gt;measure]\n\n  D --&gt; p3(Schneeweiss et al. 2017&lt;br&gt;Epidemiology)\n  p3 --&gt; ml1[LASSO]\n\n  S --&gt; p4(Weberpals et al. 2021&lt;br&gt;Epidemiology)\n  p4 --&gt; ml1[LASSO]\n  p4 --&gt; ml2[Autoencoder]\n\n  S --&gt; p5(Ju et al. 2019&lt;br&gt;Stat Meth Med Res.)\n  p5 --&gt; t1\n  p5 --&gt; t2[cTMLE,&lt;br&gt;more about&lt;br&gt;time&lt;br&gt;complexity]\n\n  S --&gt; p6(Low et al. 2015&lt;br&gt;J Comp Eff Res.)\n  p6 --&gt; ml1\n\n  %% Define style classes\n  classDef yellowNode fill:#ffff00,stroke-width:2px,stroke:#ffcc00,color:#000\n  classDef greenNode fill:#9f9,stroke-width:2px,stroke:#090,color:#000\n  classDef blueNode fill:#44f,stroke-width:2px,stroke:#00f,color:#fff\n\n  %% Apply classes to nodes\n  class p1 yellowNode\n  class p3,p4,p6 greenNode\n  class p0,p00,p5 blueNode\n\n\n\n\n\n\n\n\n\n\n\n(Pang, Schuster, Filion, Eberg, et al. 2016; Pang, Schuster, Filion, Schnitzer, et al. 2016; Ju, Gruber, et al. 2019; Ju, Combs, et al. 2019; Schneeweiss et al. 2017; Weberpals et al. 2021; Low, Gallego, and Shah 2016)\n\n\nBenasseur, Imane, Denis Talbot, Madeleine Durand, Anne Holbrook, Alexis Matteau, Brian J Potter, Christel Renoux, Mireille E Schnitzer, Jean-Éric Tarride, and Jason R Guertin. 2022. “A Comparison of Confounder Selection and Adjustment Methods for Estimating Causal Effects Using Large Healthcare Databases.” Pharmacoepidemiology and Drug Safety 31 (4): 424–33.\n\n\nFranklin, Jessica M, Wesley Eddings, Robert J Glynn, and Sebastian Schneeweiss. 2015. “Regularized Regression Versus the High-Dimensional Propensity Score for Confounding Adjustment in Secondary Database Analyses.” American Journal of Epidemiology 182 (7): 651–59.\n\n\nJu, Cheng, Mary Combs, Samuel D Lendle, Jessica M Franklin, Richard Wyss, Sebastian Schneeweiss, and Mark J van der Laan. 2019. “Propensity Score Prediction for Electronic Healthcare Databases Using Super Learner and High-Dimensional Propensity Score Methods.” Journal of Applied Statistics 46 (12): 2216–36.\n\n\nJu, Cheng, Susan Gruber, Samuel D Lendle, Antoine Chambaz, Jessica M Franklin, Richard Wyss, Sebastian Schneeweiss, and Mark J van Der Laan. 2019. “Scalable Collaborative Targeted Learning for High-Dimensional Data.” Statistical Methods in Medical Research 28 (2): 532–54.\n\n\nKarim, Mohammad Ehsanul, Menglan Pang, and Robert W Platt. 2018. “Can We Train Machine Learning Methods to Outperform the High-Dimensional Propensity Score Algorithm?” Epidemiology 29 (2): 191–98.\n\n\nLow, Yen Sia, Blanca Gallego, and Nigam Haresh Shah. 2016. “Comparing High-Dimensional Confounder Control Methods for Rapid Cohort Studies from Electronic Health Records.” Journal of Comparative Effectiveness Research 5 (2): 179–92.\n\n\nNeugebauer, Romain, Julie A Schmittdiel, Zheng Zhu, Jeremy A Rassen, John D Seeger, and Sebastian Schneeweiss. 2015. “High-Dimensional Propensity Score Algorithm in Comparative Effectiveness Research with Time-Varying Interventions.” Statistics in Medicine 34 (5): 753–81.\n\n\nPang, Menglan, Tibor Schuster, Kristian B Filion, Maria Eberg, and Robert W Platt. 2016. “Targeted Maximum Likelihood Estimation for Pharmacoepidemiologic Research.” Epidemiology (Cambridge, Mass.) 27 (4): 570.\n\n\nPang, Menglan, Tibor Schuster, Kristian B Filion, Mireille E Schnitzer, Maria Eberg, and Robert W Platt. 2016. “Effect Estimation in Point-Exposure Studies with Binary Outcomes and High-Dimensional Covariate Data–a Comparison of Targeted Maximum Likelihood Estimation and Inverse Probability of Treatment Weighting.” The International Journal of Biostatistics 12 (2).\n\n\nSchneeweiss, Sebastian. 2018. “Automated Data-Adaptive Analytics for Electronic Healthcare Data to Study Causal Treatment Effects.” Clinical Epidemiology, 771–88.\n\n\nSchneeweiss, Sebastian, Wesley Eddings, Robert J Glynn, Elisabetta Patorno, Jeremy Rassen, and Jessica M Franklin. 2017. “Variable Selection for Confounding Adjustment in High-Dimensional Covariate Spaces When Analyzing Healthcare Databases.” Epidemiology 28 (2): 237–48.\n\n\nTian, Yuxi, Martijn J Schuemie, and Marc A Suchard. 2018. “Evaluating Large-Scale Propensity Score Performance Through Real-World and Synthetic Data Experiments.” International Journal of Epidemiology 47 (6): 2005–14.\n\n\nWeberpals, Janick, Tim Becker, Jessica Davies, Fabian Schmich, Dominik Rüttinger, Fabian J Theis, and Anna Bauer-Mehren. 2021. “Deep Learning-Based Propensity Scores for Confounding Control in Comparative Effectiveness Research: A Large-Scale, Real-World Data Study.” Epidemiology 32 (3): 378–88.\n\n\nWyss, Richard, Sebastian Schneeweiss, Mark Van Der Laan, Samuel D Lendle, Cheng Ju, and Jessica M Franklin. 2018. “Using Super Learner Prediction Modeling to Improve High-Dimensional Propensity Score Estimation.” Epidemiology 29 (1): 96–106.",
    "crumbs": [
      "Challenges",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Literature</span>"
    ]
  },
  {
    "objectID": "mllogic.html",
    "href": "mllogic.html",
    "title": "Machine learning",
    "section": "",
    "text": "Understanding variable’s role",
    "crumbs": [
      "Machine learning"
    ]
  },
  {
    "objectID": "mllogic.html#understanding-variables-role",
    "href": "mllogic.html#understanding-variables-role",
    "title": "Machine learning",
    "section": "",
    "text": "(Rubin and Thomas 1996; Rubin 1997; Brookhart et al. 2006)\n\nConfounders\n\n\n\n\n\nflowchart LR\n  C --&gt; A\n  C --&gt; Y\n  A --&gt; Y\n  style A fill:#90EE90;\n  style Y fill:#ADD8E6;\n  style C fill:#FF0000;\n\n\n\n\n\n\n\n\nAdjusting Confounders help reduce bias\n\n\n(Near) instruments\n\n\n\n\n\nflowchart LR\n  C --&gt; A\n  A --&gt; Y\n  style A fill:#90EE90;\n  style Y fill:#ADD8E6;\n  style C fill:#FF0000;\n\n\n\n\n\n\n\n\nAdjusting for covariates strongly associated with the exposure: Adjusting for these variables can potentially amplify bias in the treatment effect estimate and increase standard error (SE).\n\n\nPrecision variables\n\n\n\n\n\nflowchart LR\n  C --&gt; Y\n  A --&gt; Y\n  style A fill:#90EE90;\n  style Y fill:#ADD8E6;\n  style C fill:#FF0000;\n\n\n\n\n\n\n\n\nAdjusting for covariates strongly associated with the outcome: Adjusting for these variables can lead to decrease of the SE of the treatment effect estimate.\n\n\nNoise variables\n\n\n\n\n\nflowchart LR\n  C\n  A --&gt; Y\n  style A fill:#90EE90;\n  style Y fill:#ADD8E6;\n  style C fill:#FF0000;\n\n\n\n\n\n\n\n\nAdjusting for covariates that are neither associated with the outcome or the exposure can increase the SE of the treatment effect estimate.",
    "crumbs": [
      "Machine learning"
    ]
  },
  {
    "objectID": "mllogic.html#overall-picture",
    "href": "mllogic.html#overall-picture",
    "title": "Machine learning",
    "section": "Overall picture",
    "text": "Overall picture\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoose variables associated with the outcome in general, as long as they are not mediator, collider or effect of the outcome. In hdPS, we chose proxies in the covariate assessment period (before exposure occurs), reducing the possibility of those proxies to be mediator, collider or effect of the outcome.\n\n\nBrookhart, M Alan, Sebastian Schneeweiss, Kenneth J Rothman, Robert J Glynn, Jerry Avorn, and Til Stürmer. 2006. “Variable Selection for Propensity Score Models.” American Journal of Epidemiology 163 (12): 1149–56.\n\n\nRubin, Donald B. 1997. “Estimating Causal Effects from Large Data Sets Using Propensity Scores.” Annals of Internal Medicine 127 (8_Part_2): 757–63.\n\n\nRubin, Donald B, and Neal Thomas. 1996. “Matching Using Estimated Propensity Scores: Relating Theory to Practice.” Biometrics, 249–64.",
    "crumbs": [
      "Machine learning"
    ]
  },
  {
    "objectID": "mllasso.html",
    "href": "mllasso.html",
    "title": "13  Pure ML",
    "section": "",
    "text": "14 Pure ML approach (LASSO)\nStart with all recurrence variables (EC in the following equation)\nSay, 100 proxies (associated with outcome) were selected by LASSO approach (ML-hdPS)",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Pure ML</span>"
    ]
  },
  {
    "objectID": "mllasso.html#choose-variables-associated-with-outcome",
    "href": "mllasso.html#choose-variables-associated-with-outcome",
    "title": "13  Pure ML",
    "section": "14.1 Choose variables associated with outcome",
    "text": "14.1 Choose variables associated with outcome\n\nproxy.dim &lt;- out2 # from step 3\ndim(proxy.dim) \n#&gt; [1] 3839   92\nproxy.dim$id &lt;- proxy.dim$idx\nproxy.dim$idx &lt;- NULL\nfullcovproxy.data &lt;- merge(data.complete[,c(\"id\",\n                                    outcome, \n                                    exposure, \n                                    investigator.specified.covariates)], \n                       proxy.dim, by = \"id\")\ndim(fullcovproxy.data)\n#&gt; [1] 3839  119\nfullcovproxy.data$outcome &lt;- as.numeric(I(fullcovproxy.data$diabetes=='Yes'))\nfullcovproxy.data$exposure &lt;- as.numeric(I(fullcovproxy.data$obese=='Yes'))\n\n\nproxy.list &lt;- names(out2[-1])\n# out3$autoselected_covariate_df[,-1] for hybrid \n# out2 is from step2$recurrence_data\ncovarsTfull &lt;- c(investigator.specified.covariates, proxy.list)\nY.form &lt;- as.formula(paste0(c(\"outcome~ exposure\", \n                              covarsTfull), collapse = \"+\") )\ncovar.mat &lt;- model.matrix(Y.form, data = fullcovproxy.data)[,-1]\nlasso.fit&lt;-glmnet::cv.glmnet(y = fullcovproxy.data$outcome, \n                             x = covar.mat, \n                             type.measure='mse',\n                             family=\"binomial\",\n                             alpha = 1, \n                             nfolds = 5)\ncoef.fit&lt;-coef(lasso.fit,s='lambda.min',exact=TRUE)\nsel.variables&lt;-row.names(coef.fit)[which(as.numeric(coef.fit)!=0)]\nproxy.list.sel.ml &lt;- proxy.list[proxy.list %in% sel.variables]\nlength(proxy.list.sel.ml)\n#&gt; [1] 42\n\n\n\n\nFrom all proxies, we try to identify proxies that are empirically associated with the outcome based on a multivariate LASSO (outcome with all proxies in one model).\nNote that LASSO model is choosing variables based on association with the outcome conditional on the ’exposure`.\nVariable selection is only happening for proxy variables.\nInvestigator specified variables are not being subject to variable selection.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Pure ML</span>"
    ]
  },
  {
    "objectID": "mllasso.html#build-model-formula-based-on-selected-variables",
    "href": "mllasso.html#build-model-formula-based-on-selected-variables",
    "title": "13  Pure ML",
    "section": "14.2 Build model formula based on selected variables",
    "text": "14.2 Build model formula based on selected variables\n\ncovform &lt;- paste0(investigator.specified.covariates, collapse = \"+\")\nproxyform &lt;- paste0(proxy.list.sel.ml, collapse = \"+\")\nrhsformula &lt;- paste0(c(covform, proxyform), collapse = \"+\")\nps.formula &lt;- as.formula(paste0(\"exposure\", \"~\", rhsformula))\n\n\n\nBuild propensity score model based on selected variables based on LASSO.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Pure ML</span>"
    ]
  },
  {
    "objectID": "mllasso.html#fit-the-ps-model",
    "href": "mllasso.html#fit-the-ps-model",
    "title": "13  Pure ML",
    "section": "14.3 Fit the PS model",
    "text": "14.3 Fit the PS model\n\nhdps.data &lt;- fullcovproxy.data\nrequire(WeightIt)\nW.out &lt;- weightit(ps.formula, \n                    data = hdps.data, \n                    estimand = \"ATE\",\n                    method = \"ps\")\n\n\n\nPropensity score model fit to be able to calculate the inverse probability weights.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Pure ML</span>"
    ]
  },
  {
    "objectID": "mllasso.html#obtain-log-or-from-unadjusted-outcome-model",
    "href": "mllasso.html#obtain-log-or-from-unadjusted-outcome-model",
    "title": "13  Pure ML",
    "section": "14.4 Obtain log-OR from unadjusted outcome model",
    "text": "14.4 Obtain log-OR from unadjusted outcome model\n\nout.formula &lt;- as.formula(paste0(\"outcome\", \"~\", \"exposure\"))\nfit &lt;- glm(out.formula,\n            data = hdps.data,\n            weights = W.out$weights,\n            family= binomial(link = \"logit\"))\nfit.summary &lt;- summary(fit)$coef[\"exposure\",\n                                 c(\"Estimate\", \n                                   \"Std. Error\", \n                                   \"Pr(&gt;|z|)\")]\nfit.summary[2] &lt;- sqrt(sandwich::sandwich(fit)[2,2])\nrequire(lmtest)\nconf.int &lt;- confint(fit, \"exposure\", level = 0.95, method = \"hc1\")\nfit.summary_with_ci &lt;- c(fit.summary, conf.int)\nknitr::kable(t(round(fit.summary_with_ci,2))) \n\n\n\n\nEstimate\nStd. Error\nPr(&gt;|z|)\n2.5 %\n97.5 %\n\n\n\n\n0.39\n0.1\n0\n0.28\n0.49\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of results (log-OR).\n\n\nFranklin, Jessica M, Wesley Eddings, Robert J Glynn, and Sebastian Schneeweiss. 2015. “Regularized Regression Versus the High-Dimensional Propensity Score for Confounding Adjustment in Secondary Database Analyses.” American Journal of Epidemiology 182 (7): 651–59.\n\n\nKarim, Mohammad Ehsanul, Menglan Pang, and Robert W Platt. 2018. “Can We Train Machine Learning Methods to Outperform the High-Dimensional Propensity Score Algorithm?” Epidemiology 29 (2): 191–98.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Pure ML</span>"
    ]
  },
  {
    "objectID": "mlhybrid.html",
    "href": "mlhybrid.html",
    "title": "14  Hybrid ML",
    "section": "",
    "text": "14.1 Build model formula based on selected variables\nInstead of all recurrence variables, you start with the hdPS variables chosen by the hdPS algorithm first.\nlength(proxy.list.sel)\n#&gt; [1] 50\nproxy.list &lt;- names(out3$autoselected_covariate_df[,-1]) # from step 4\ncovarsTfull &lt;- c(investigator.specified.covariates, proxy.list)\nY.form &lt;- as.formula(paste0(c(\"outcome~ exposure\", \n                              covarsTfull), collapse = \"+\") )\ncovar.mat &lt;- model.matrix(Y.form, data = hdps.data)[,-1]\nlasso.fit&lt;-glmnet::cv.glmnet(y = hdps.data$outcome, \n                             x = covar.mat, \n                             type.measure='mse',\n                             family=\"binomial\",\n                             alpha = 1, \n                             nfolds = 5)\ncoef.fit&lt;-coef(lasso.fit,s='lambda.min',exact=TRUE)\nsel.variables&lt;-row.names(coef.fit)[which(as.numeric(coef.fit)!=0)]\nproxy.list.sel.hybrid &lt;- proxy.list[proxy.list %in% sel.variables]\nlength(proxy.list.sel.hybrid)\n#&gt; [1] 37\nproxyform &lt;- paste0(proxy.list.sel.hybrid, collapse = \"+\")\nrhsformula &lt;- paste0(c(covform, proxyform), collapse = \"+\")\nps.formula &lt;- as.formula(paste0(\"exposure\", \"~\", rhsformula))",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Hybrid ML</span>"
    ]
  },
  {
    "objectID": "mlhybrid.html#build-model-formula-based-on-selected-variables",
    "href": "mlhybrid.html#build-model-formula-based-on-selected-variables",
    "title": "14  Hybrid ML",
    "section": "",
    "text": "From hdPS variables, we try to identify proxies that are empirically associated with the outcome based on a multivariate LASSO (outcome with all proxies in one model).\n\n\n\nBuild propensity score model based on selected variables based on LASSO.\n\n14.1.1 Fit the PS model\n\nW.out &lt;- weightit(ps.formula, \n                    data = hdps.data, \n                    estimand = \"ATE\",\n                    method = \"ps\")\n\n\n\nPropensity score model fit to be able to calculate the inverse probability weights.\n\n\n14.1.2 Obtain log-OR from unadjusted outcome model\n\nout.formula &lt;- as.formula(paste0(\"outcome\", \"~\", \"exposure\"))\nfit &lt;- glm(out.formula,\n            data = hdps.data,\n            weights = W.out$weights,\n            family= binomial(link = \"logit\"))\nfit.summary &lt;- summary(fit)$coef[\"exposure\",\n                                 c(\"Estimate\", \n                                   \"Std. Error\", \n                                   \"Pr(&gt;|z|)\")]\nfit.summary[2] &lt;- sqrt(sandwich::sandwich(fit)[2,2])\nrequire(lmtest)\nconf.int &lt;- confint(fit, \"exposure\", level = 0.95, method = \"hc1\")\nfit.summary_with_ci.h &lt;- c(fit.summary, conf.int)\nknitr::kable(t(round(fit.summary_with_ci.h,2))) \n\n\n\n\nEstimate\nStd. Error\nPr(&gt;|z|)\n2.5 %\n97.5 %\n\n\n\n\n0.39\n0.1\n0\n0.29\n0.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of results (log-OR).\n\n\n\n\n\n\nAlternative process\n\n\n\nIt is also possible to start with ML selection, and then applying Bross’s formula on top of it (Schneeweiss et al. 2017).\n\n\n\n\n\n\nFranklin, Jessica M, Wesley Eddings, Robert J Glynn, and Sebastian Schneeweiss. 2015. “Regularized Regression Versus the High-Dimensional Propensity Score for Confounding Adjustment in Secondary Database Analyses.” American Journal of Epidemiology 182 (7): 651–59.\n\n\nKarim, Mohammad Ehsanul, Menglan Pang, and Robert W Platt. 2018. “Can We Train Machine Learning Methods to Outperform the High-Dimensional Propensity Score Algorithm?” Epidemiology 29 (2): 191–98.\n\n\nSchneeweiss, Sebastian, Wesley Eddings, Robert J Glynn, Elisabetta Patorno, Jeremy Rassen, and Jessica M Franklin. 2017. “Variable Selection for Confounding Adjustment in High-Dimensional Covariate Spaces When Analyzing Healthcare Databases.” Epidemiology 28 (2): 237–48.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Hybrid ML</span>"
    ]
  },
  {
    "objectID": "sl.html",
    "href": "sl.html",
    "title": "15  Ensemble",
    "section": "",
    "text": "15.1 Build model formula based on all variables\nproxy.list &lt;- names(out2[-1])\n# out3$autoselected_covariate_df[,-1] for hybrid \n# out2 is from step2$recurrence_data\nlength(proxy.list)\n#&gt; [1] 91\ncovform &lt;- paste0(investigator.specified.covariates, collapse = \"+\")\nproxyform &lt;- paste0(proxy.list, collapse = \"+\")\nrhsformula &lt;- paste0(c(covform, proxyform), collapse = \"+\")\nps.formula &lt;- as.formula(paste0(\"exposure\", \"~\", rhsformula))",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Ensemble</span>"
    ]
  },
  {
    "objectID": "sl.html#build-model-formula-based-on-all-variables",
    "href": "sl.html#build-model-formula-based-on-all-variables",
    "title": "15  Ensemble",
    "section": "",
    "text": "We work with all proxies",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Ensemble</span>"
    ]
  },
  {
    "objectID": "sl.html#fit-the-ps-model-with-super-learner",
    "href": "sl.html#fit-the-ps-model-with-super-learner",
    "title": "15  Ensemble",
    "section": "15.2 Fit the PS model with super learner",
    "text": "15.2 Fit the PS model with super learner\n\nrequire(WeightIt)\nW.out &lt;- weightit(ps.formula, \n                  data = hdps.data, \n                  estimand = \"ATE\",\n                  method = \"super\",\n                  SL.library = c(\"SL.glm\", \n                                 \"SL.glmnet\",\n                                 \"SL.earth\"))\n#&gt; Loading required namespace: glmnet\n#&gt; Loading required namespace: earth\n\n\n\nPropensity score model fit based on super learning algorithm to be able to calculate the inverse probability weights.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Ensemble</span>"
    ]
  },
  {
    "objectID": "sl.html#obtain-log-or-from-unadjusted-outcome-model",
    "href": "sl.html#obtain-log-or-from-unadjusted-outcome-model",
    "title": "15  Ensemble",
    "section": "15.3 Obtain log-OR from unadjusted outcome model",
    "text": "15.3 Obtain log-OR from unadjusted outcome model\n\nsummary(W.out$ps)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.01795 0.22327 0.39066 0.42094 0.59780 0.99230\nout.formula &lt;- as.formula(paste0(\"outcome\", \"~\", \"exposure\"))\nfit &lt;- glm(out.formula,\n            data = hdps.data,\n            weights = W.out$weights,\n            family= binomial(link = \"logit\"))\nfit.summary &lt;- summary(fit)$coef[\"exposure\",\n                                 c(\"Estimate\", \n                                   \"Std. Error\", \n                                   \"Pr(&gt;|z|)\")]\nfit.summary[2] &lt;- sqrt(sandwich::sandwich(fit)[2,2])\nrequire(lmtest)\nconf.int &lt;- confint(fit, \"exposure\", level = 0.95, method = \"hc1\")\nfit.summary_with_ci.sl &lt;- c(fit.summary, conf.int)\nknitr::kable(t(round(fit.summary_with_ci.sl,2))) \n\n\n\n\nEstimate\nStd. Error\nPr(&gt;|z|)\n2.5 %\n97.5 %\n\n\n\n\n0.43\n0.09\n0\n0.32\n0.54\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of results (log-OR).",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Ensemble</span>"
    ]
  },
  {
    "objectID": "tmle.html",
    "href": "tmle.html",
    "title": "16  TMLE",
    "section": "",
    "text": "16.1 Obtain OR with superlearner\nsummary(W.out$ps)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.01795 0.22327 0.39066 0.42094 0.59780 0.99230\nSL.library = c(\"SL.glm\", \"SL.glmnet\",\"SL.earth\")\nproxy.list &lt;- names(out2[-1])\n# out3$autoselected_covariate_df[,-1] for hybrid \n# out2 is from step2$recurrence_data\nObsData.noYA &lt;- hdps.data[,c(investigator.specified.covariates, \n                             proxy.list)]\ntmle.fit &lt;- tmle::tmle(Y = hdps.data$outcome,\n                       A = hdps.data$exposure, \n                       W = ObsData.noYA, \n                       family = \"binomial\",\n                       V.Q = 3,\n                       V.g = 3,\n                       Q.SL.library = SL.library,\n                       g1W = W.out$ps)\nestOR.tmle &lt;- tmle.fit$estimates$OR\nestOR.tmle\n#&gt; $psi\n#&gt; [1] 1.43478\n#&gt; \n#&gt; $log.psi\n#&gt; [1] 0.3610113\n#&gt; \n#&gt; $CI\n#&gt; [1] 1.234739 1.667229\n#&gt; \n#&gt; $pvalue\n#&gt; [1] 2.448685e-06\n#&gt; \n#&gt; $var.log.psi\n#&gt; [1] 0.005869015\n#&gt; \n#&gt; $bs.var.log.psi\n#&gt; [1] NA\n#&gt; \n#&gt; $bs.CI.twosided\n#&gt; [1] NA NA\n#&gt; \n#&gt; $bs.CI.onesided.lower\n#&gt; [1] -Inf   NA\n#&gt; \n#&gt; $bs.CI.onesided.upper\n#&gt; [1]  NA Inf",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>TMLE</span>"
    ]
  },
  {
    "objectID": "tmle.html#obtain-or-with-superlearner",
    "href": "tmle.html#obtain-or-with-superlearner",
    "title": "16  TMLE",
    "section": "",
    "text": "We use the same propensity score model that was fitted based on super learning algorithm.\n\n\n\nIf you want to know more about TMLE, look at other tutorials.\n\n\n\nSummary of results (OR).",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>TMLE</span>"
    ]
  },
  {
    "objectID": "tmle.html#obtain-or-without-superlearner",
    "href": "tmle.html#obtain-or-without-superlearner",
    "title": "16  TMLE",
    "section": "16.2 Obtain OR without superlearner",
    "text": "16.2 Obtain OR without superlearner\n\nsummary(W.out0$ps)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.01333 0.21054 0.38992 0.42094 0.61259 0.99414\nSL.library = c(\"SL.glm\")\nproxy.list &lt;- names(out2[-1])\n# out3$autoselected_covariate_df[,-1] for hybrid \n# out2 is from step2$recurrence_data\nObsData.noYA &lt;- hdps.data[,c(investigator.specified.covariates, \n                             proxy.list)]\n\n\n\nWe use the same propensity score model that was fitted based on hdPS variables via logistic regression (no other learners).\n\ntmle.fit0 &lt;- tmle::tmle(Y = hdps.data$outcome,\n                       A = hdps.data$exposure, \n                       W = ObsData.noYA, \n                       family = \"binomial\",\n                       V.Q = 3,\n                       V.g = 3,\n                       Q.SL.library = SL.library,\n                       g1W = W.out$ps)\n\n\nestOR.tmle0 &lt;- tmle.fit0$estimates$OR\nestOR.tmle0\n#&gt; $psi\n#&gt; [1] 1.414274\n#&gt; \n#&gt; $log.psi\n#&gt; [1] 0.346616\n#&gt; \n#&gt; $CI\n#&gt; [1] 1.216181 1.644632\n#&gt; \n#&gt; $pvalue\n#&gt; [1] 6.731435e-06\n#&gt; \n#&gt; $var.log.psi\n#&gt; [1] 0.005927678\n#&gt; \n#&gt; $bs.var.log.psi\n#&gt; [1] NA\n#&gt; \n#&gt; $bs.CI.twosided\n#&gt; [1] NA NA\n#&gt; \n#&gt; $bs.CI.onesided.lower\n#&gt; [1] -Inf   NA\n#&gt; \n#&gt; $bs.CI.onesided.upper\n#&gt; [1]  NA Inf\n\n\n\nSummary of results (OR).\n\n#&gt; Warning: `geom_errorbarh()` was deprecated in ggplot2 4.0.0.\n#&gt; ℹ Please use the `orientation` argument of `geom_errorbar()` instead.\n#&gt; `height` was translated to `width`.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>TMLE</span>"
    ]
  },
  {
    "objectID": "stat.html",
    "href": "stat.html",
    "title": "17  Statistical Approaches",
    "section": "",
    "text": "17.1 Background\nRecent work compares multiple variable selection strategies for hdPS analysis (Karim and Lei 2025). The study aims to identify methods that best balance bias, precision, and computational cost in causal inference using observational data. It is based on NHANES 2013–2018 data evaluating the association between obesity and diabetes.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Approaches</span>"
    ]
  },
  {
    "objectID": "stat.html#background",
    "href": "stat.html#background",
    "title": "17  Statistical Approaches",
    "section": "",
    "text": "Tip\n\n\n\n(Karim and Lei 2025)",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Approaches</span>"
    ]
  },
  {
    "objectID": "stat.html#simulation-design",
    "href": "stat.html#simulation-design",
    "title": "17  Statistical Approaches",
    "section": "17.2 Simulation Design",
    "text": "17.2 Simulation Design\n\n\n\n\n\n\n\nElement\nDetails\n\n\n\n\nData Source\nNHANES 2013–2018\n\n\nSample Size\n3,000 participants per iteration\n\n\nIterations\n500\n\n\nPrevalence Scenarios\n1. Frequent exposure & frequent outcome  2. Rare exposure & frequent outcome  3. Frequent exposure & rare outcome\n\n\nTrue Effect\nOR = 1 (null); RD = 0\n\n\nOutcome Generation\nIncluded nonlinear transforms, interactions, and a comorbidity index from 94 proxies\n\n\nNoise Variables\n48 of 142 proxy covariates used as noise",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Approaches</span>"
    ]
  },
  {
    "objectID": "stat.html#methods-compared",
    "href": "stat.html#methods-compared",
    "title": "17  Statistical Approaches",
    "section": "17.3 Methods Compared",
    "text": "17.3 Methods Compared\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nKitchen Sink\nIncludes all investigator and proxy covariates (no selection)\n\n\nBross hdPS\nSelects top 100 proxies using the Bross formula\n\n\nHybrid (Bross + LASSO)\nFirst applies Bross, then refines with LASSO\n\n\nLASSO\nPenalized regression with cross-validation\n\n\nElastic Net\nCombines LASSO and Ridge penalties to handle collinearity\n\n\nRandom Forest\nRanks variables by importance using Gini impurity\n\n\nXGBoost\nBoosted trees optimizing impurity reduction\n\n\nForward Selection\nAdds variables sequentially based on adjusted R²\n\n\nBackward Elimination\nRemoves variables iteratively based on adjusted R²\n\n\nGenetic Algorithm\nEvolves variable subsets via stochastic search",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Approaches</span>"
    ]
  },
  {
    "objectID": "stat.html#simulation-results",
    "href": "stat.html#simulation-results",
    "title": "17  Statistical Approaches",
    "section": "17.4 Simulation Results",
    "text": "17.4 Simulation Results\n\n\n\n\n\nFigure 1. Bias across Methods in NHANES Plasmode Simulation\n\n\n\n\n\n\n\n\n\nFigure 2. Coverage across Methods in NHANES Plasmode Simulation\n\n\n\n\nSee interactive results: 👉 Shiny App",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Approaches</span>"
    ]
  },
  {
    "objectID": "stat.html#key-takeaways",
    "href": "stat.html#key-takeaways",
    "title": "17  Statistical Approaches",
    "section": "17.5 Key Takeaways",
    "text": "17.5 Key Takeaways\n\nSimpler methods (Forward/Backward selection) offer strong coverage with efficiency.\nBross-based and Hybrid hdPS methods remain reliable and interpretable.\nMethod choice should reflect the specific inferential goal: bias reduction vs variance minimization.\n\n\n\n\n\nKarim, ME, and Y Lei. 2025. “Is There a Competitive Advantage to Using Multivariate Statistical or Machine Learning Methods over the Bross Formula in the hdPS Framework for Bias and Variance Estimation?” PLoS One 20 (5): e0324639.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Approaches</span>"
    ]
  },
  {
    "objectID": "mlcompare.html",
    "href": "mlcompare.html",
    "title": "18  Compare results",
    "section": "",
    "text": "Summary of model results\n\n\n\nOR\nBeta-coef\ncoef-SE\nCI (2.5 %)\nCI (97.5 %)\np-value\n\n\n\n\nCrude (no adjustment)\n1.94\n0.66\n0.08\n0.51\n0.81\n&lt; 2e-16\n\n\nPS (no proxies)\n1.89\n0.64\n0.10\n0.53\n0.75\n&lt; 2e-16\n\n\nhdPS\n1.42\n0.35\n0.12\n0.25\n0.46\n6.7e-11\n\n\nPure LASSO\n1.47\n0.39\n0.10\n0.28\n0.49\n1.1e-12\n\n\nHybrid (hdPS, then LASSO)\n1.48\n0.39\n0.10\n0.29\n0.50\n3.9e-13\n\n\nSuper learner (GLM, LASSO, MARS)\n1.53\n0.43\n0.09\n0.32\n0.54\n1.4e-14\n\n\nTMLE (GLM, LASSO, MARS in SL)\n1.43\n0.36\n0.08\n0.21\n0.51\n2.4e-06\n\n\nTMLE (only GLM in SL)\n1.41\n0.35\n0.08\n0.20\n0.50\n6.7e-06\n\n\nKitchen Sink\n1.50\n0.41\n0.04\n0.32\n0.48\n&lt; 2e-16\n\n\nRandom Forest\n1.54\n0.43\n0.04\n0.35\n0.51\n&lt; 2e-16\n\n\nXGBoost\n1.51\n0.41\n0.04\n0.33\n0.49\n&lt; 2e-16\n\n\nForward Selection\n1.56\n0.44\n0.04\n0.36\n0.52\n&lt; 2e-16\n\n\nBackward Elimination\n1.53\n0.43\n0.04\n0.34\n0.50\n&lt; 2e-16\n\n\n\n\n\n\n\n\nPS is the result from the propensity score approach that did not include any proxies.\nResults from this approach is somewhat different than other approaches.\nMore detailed results from simulations are available elsewhere (Karim 2023).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAcross all methods evaluated—including hdPS, regularized regression (LASSO, Hybrid), ensemble learners (Super Learner, TMLE), and high-dimensional variable selection strategies (e.g., Kitchen Sink, Random Forest, XGBoost)—adjusted odds ratios ranged from 1.34 to 1.56, with most clustering between 1.50 and 1.56. In contrast, unadjusted and PS-only models produced substantially higher ORs (&gt;1.9).\n\n\nKarim, ME. 2023. “Rethinking Residual Confounding Bias Reduction: Why Vanilla hdPS Alone Is No Longer Enough.”",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Compare results</span>"
    ]
  },
  {
    "objectID": "dctmle.html",
    "href": "dctmle.html",
    "title": "19  DC-TMLE",
    "section": "",
    "text": "19.1 Background\nDouble Cross-Fit TMLE (DC-TMLE) (Mondol and Karim 2024; M. Karim and Mondol 2025) is an extension of TMLE designed to improve robustness and reduce overfitting when using flexible, high-dimensional or machine learning-based models. It works by splitting the data into multiple folds, training nuisance models (e.g., the propensity score and outcome regressions) on one subset, and then evaluating the targeted update and parameter estimation on another. This sample-splitting (cross-fitting) procedure helps ensure that the estimation step is not biased by the same data used to fit the nuisance models. This process of sample-splitting and estimation is repeated, and the results are averaged to produce a final, stable estimate. DC-TMLE maintains double robustness, meaning it remains consistent if either the treatment or outcome model is correctly specified, and it provides valid statistical inference even in high-dimensional settings where traditional TMLE may be unstable.\nResidual confounding remains a persistent challenge in observational studies, particularly with high-dimensional data (M. E. Karim and Lei 2025). Recent work evaluates traditional and machine learning-based extensions of hdPS methods, including Super Learner (SL), TMLE, and Double Cross-Fit TMLE (DC-TMLE).",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>DC-TMLE</span>"
    ]
  },
  {
    "objectID": "dctmle.html#background",
    "href": "dctmle.html#background",
    "title": "19  DC-TMLE",
    "section": "",
    "text": "Tip\n\n\n\n(M. E. Karim and Lei 2025)",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>DC-TMLE</span>"
    ]
  },
  {
    "objectID": "dctmle.html#simulation-design",
    "href": "dctmle.html#simulation-design",
    "title": "19  DC-TMLE",
    "section": "19.2 Simulation Design",
    "text": "19.2 Simulation Design\n\n\n\n\n\n\n\nElement\nDetails\n\n\n\n\nData Source\nNHANES 2013–2018\n\n\nSample Size\n3,000 per iteration\n\n\nIterations\n500\n\n\nExposure/Outcome Prevalence\n3 scenarios: (i) Frequent-Frequent, (ii) Rare-Frequent, (iii) Frequent-Rare\n\n\nTrue Effect\nOR = 1 (null); RD = 0\n\n\nProxies\n142 medication variables; 94 outcome-associated proxies and 48 noise variables\n\n\nConfounding Simulation\nUsed proxy-derived comorbidity index and complex transformations to mimic unmeasured confounding",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>DC-TMLE</span>"
    ]
  },
  {
    "objectID": "dctmle.html#methods-compared",
    "href": "dctmle.html#methods-compared",
    "title": "19  DC-TMLE",
    "section": "19.3 Methods Compared",
    "text": "19.3 Methods Compared\n\n\n\n\n\n\n\n\nMethod Group\nMethod\nDescription\n\n\n\n\nTMLE Methods with Proxies\nTMLE.ks, hdPS.TMLE, LASSO.TMLE, hdPS.LASSO.TMLE\nTMLE with various proxy selection strategies\n\n\n\nDC.TMLE\nDouble cross-fit TMLE\n\n\nSuper Learner Methods with Proxies\nhdPS.SL, LASSO.SL, hdPS.LASSO.SL, SL.ks\nSuper Learner with proxy selection options\n\n\nStandard Methods with Proxies\nPS.ks, hdPS, LASSO, hdPS.LASSO\nPropensity score and outcome models with proxy inclusion\n\n\nNo Proxy Methods\nTMLE.u, SL.u, PS.u\nOnly measured covariates, no proxies\n\n\n\nSuper Learner libraries included:\n\n1-learner: Logistic regression\n3-learners: Logistic regression, LASSO, MARS\n4-learners: Above + XGBoost (non-Donsker)",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>DC-TMLE</span>"
    ]
  },
  {
    "objectID": "dctmle.html#simulation-results",
    "href": "dctmle.html#simulation-results",
    "title": "19  DC-TMLE",
    "section": "19.4 Simulation Results",
    "text": "19.4 Simulation Results\n\n\n\n\n\nFigure 2. Bias across Methods in NHANES Plasmode Simulation\n\n\n\n\n\n\n\n\n\nFigure 3. Coverage across Methods in NHANES Plasmode Simulation\n\n\n\n\nResults are fully accessible via a Shiny app:\n👉 Interactive Causal Benchmark App\nExplore bias, SEs, and coverage metrics across methods and simulation conditions.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>DC-TMLE</span>"
    ]
  },
  {
    "objectID": "dctmle.html#conclusion",
    "href": "dctmle.html#conclusion",
    "title": "19  DC-TMLE",
    "section": "19.5 Conclusion",
    "text": "19.5 Conclusion\n\nSimpler models with structured proxy inclusion (hdPS, LASSO) remain competitive and stable.\nTMLE is effective for bias reduction but suffers under high-dimensional instability with complex libraries.\nSL performance is library-sensitive; 1- and 3-learner libraries performed best. Complex learners (e.g., XGBoost) should be used cautiously.\n\n\n\n\n\nKarim, ME, and MH Mondol. 2025. “Finding the Optimal Number of Splits and Repetitions in Double Cross-Fitting Targeted Maximum Likelihood Estimators.” Pharmaceutical Statistics.\n\n\nKarim, Mohammad Ehsanul, and Yang Lei. 2025. “How Effective Are Machine Learning and Doubly Robust Estimators in Incorporating High-Dimensional Proxies to Reduce Residual Confounding?” Pharmacoepidemiology and Drug Safety 34 (5): e70155.\n\n\nMondol, MH, and ME Karim. 2024. “Towards Robust Causal Inference in Epidemiological Research: Employing Double Cross-Fit TMLE in Right Heart Catheterization Data.” American Journal of Epidemiology, kwae447.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>DC-TMLE</span>"
    ]
  },
  {
    "objectID": "deep.html",
    "href": "deep.html",
    "title": "20  Deep Learning",
    "section": "",
    "text": "20.1 Plasmode Simulation\nRecent work extends traditional hdPS analyses by introducing and explaining neural representation learning methods for causal inference in observational studies. It focuses on NHANES data (2013–2018) and highlights how recent innovations in machine learning can address residual confounding and model misspecification challenges commonly encountered in high-dimensional data settings.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "deep.html#plasmode-simulation",
    "href": "deep.html#plasmode-simulation",
    "title": "20  Deep Learning",
    "section": "",
    "text": "Simulation Element\nDescription\n\n\n\n\nSource Dataset\nNHANES 2013–2018\n\n\nSimulation Framework\nPlasmode simulation preserving empirical covariate and exposure distributions\n\n\nSimulated Sample Size\n3,000 participants per iteration\n\n\nIterations\n500 replicates\n\n\nPrevalence Scenarios\n1. Frequent exposure & frequent outcome  2. Rare exposure & frequent outcome  3. Frequent exposure & rare outcome\n\n\nTrue Effect\nOR = 1 (null); RD = 0\n\n\nOutcome Generation\nLogistic regression model with:  - Nonlinear transformations (log, poly)  - Interactions  - Proxy-derived comorbidity index\n\n\nConfounding Simulation\nUnmeasured confounding mimicked using high-dimensional proxy variables",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "deep.html#estimators-compared",
    "href": "deep.html#estimators-compared",
    "title": "20  Deep Learning",
    "section": "20.2 Estimators Compared",
    "text": "20.2 Estimators Compared\n\n\n\n\n\n\n\n\n\n\nMethod\nCore Idea\nKey Features\nUse of Propensity Score\nOptimization & Regularization\n\n\n\n\nPSW (hdPS)\nBaseline method using logistic regression on investigator and proxy covariates\nHigh-dimensional covariates selected via hdPS\nExplicitly modeled via logistic regression\nNone\n\n\nTMLE (SL Smooth) (Balzer and Westling 2021)\nSemiparametric estimator using Super Learner\nCombines outcome and treatment models; uses smooth learners (logistic regression, LASSO, MARS)\nExplicitly modeled and used for targeting\nSuper Learner; Donsker-compliant learners\n\n\nTMLE (SL Unsmooth)\nMore flexible TMLE with XGBoost in Super Learner\nAllows complex nonlinearities; lower variance reliability in small samples\nExplicitly modeled and used for targeting\nSuper Learner including unsmooth learners (e.g., XGBoost)\n\n\nDCTMLE (Zivich and Breskin 2021)\nTMLE with double cross-fitting\nReduces overfitting in TMLE with flexible learners\nExplicitly modeled and used for targeting\nDouble cross-fitting for robustness\n\n\nTARNET (Shalit, Johansson, and Sontag 2017)\nNeural net with treatment-agnostic shared representation\nTwo heads for outcome under treatment/control; most precise in frequent exposure/outcome\nNot used explicitly\nTargeted regularization; Adam + SGD with early stopping\n\n\nDragonnet (Shi, Blei, and Veitch 2019)\nNeural net that jointly models outcomes and propensity score\nAdds third head for PS; enforces balance and semiparametric alignment\nModeled as an explicit third output\nTargeted regularization; multitask learning\n\n\nNEDnet (Shi, Blei, and Veitch 2019)\nSequential neural network for treatment then outcome\nStage 1: predict treatment; Stage 2: freeze representation and predict outcomes\nModeled separately in Stage 1\nTargeted regularization; two-stage optimization",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "deep.html#simulation-results",
    "href": "deep.html#simulation-results",
    "title": "20  Deep Learning",
    "section": "20.3 Simulation Results",
    "text": "20.3 Simulation Results\n\n\n\n\n\nFigure 1. Bias across Methods in NHANES Plasmode Simulation\n\n\n\n\n\n\n\n\n\nFigure 2. Relative error across Methods in NHANES Plasmode Simulation\n\n\n\n\nResults are fully accessible via a Shiny app:\n👉 Interactive Causal Benchmark App\nExplore bias, SEs, and coverage metrics across methods and simulation conditions.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "deep.html#conclusion",
    "href": "deep.html#conclusion",
    "title": "20  Deep Learning",
    "section": "20.4 Conclusion",
    "text": "20.4 Conclusion\n\nPSW remains an interpretable benchmark\nTMLE and neural methods extend this framework by improving bias-variance trade-offs and enabling better performance in complex settings\nAmong deep learning methods, Dragonnet offers the best average trade-off; NEDnet excels in coverage but is computationally heavy; TARNET offers precision\nThese methods are particularly useful when dealing with residual confounding, nonlinear effects, and proxy variable structures\n\n\n\n\n\nBalzer, Laura B, and Ted Westling. 2021. “Demystifying Statistical Inference When Using Machine Learning in Causal Research.” American Journal of Epidemiology.\n\n\nKarim, Mohammad Ehsanul, and Zining Annie Wang. 2025. “Are Neural Representation Learning Methods a Viable Alternative to TMLE for Causal Estimation?” Data Science in Science 4 (1): 2583507.\n\n\nShalit, Uri, Fredrik D Johansson, and David Sontag. 2017. “Estimating Individual Treatment Effect: Generalization Bounds and Algorithms.” In International Conference on Machine Learning, 3076–85. PMLR.\n\n\nShi, Claudia, David Blei, and Victor Veitch. 2019. “Adapting Neural Networks for the Estimation of Treatment Effects.” Advances in Neural Information Processing Systems 32.\n\n\nZivich, Paul N, and Alexander Breskin. 2021. “Machine Learning for Causal Inference: On the Use of Cross-Fit Estimators.” Epidemiology (Cambridge, Mass.) 32 (3): 393.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "extension2.html",
    "href": "extension2.html",
    "title": "Extensions in Survival and Longitudinal Analyses",
    "section": "",
    "text": "Time-to-event outcome\nThere are two components to a time-to-event (survival) outcome: (1) whether an event occurs and (2) the timing of the event.",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses"
    ]
  },
  {
    "objectID": "extension2.html#time-to-event-outcome",
    "href": "extension2.html#time-to-event-outcome",
    "title": "Extensions in Survival and Longitudinal Analyses",
    "section": "",
    "text": "Example: time-to-CVD, where we are interested both in CVD (cardiovascular disease) status and time from cohort entry to CVD development. The Cox proportional hazards (PH) model is widely used for modeling a time-to-event outcome.\n\n\n\nAcknowledgment: Md Belal Hossain contributed to drafting this chapter; some of the ideas presented here stem from his PhD thesis and subsequent publications.\n\n\n\n\n\n\nTime-to-event outcome\n\n\n\n\nBross formula requires the exposure, outcome, and proxy covariates to be binary\nIgnoring the time aspect in a time-to-event outcome leads to a loss of information, which can significantly impact the selection of the proxy covariates and can impact the effect estimate of interest\nML survival models could be useful in prioritizing and selecting recurrence covariates\n\n\n\n\n\n(Bross 1966; Schneeweiss et al. 2009)",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses"
    ]
  },
  {
    "objectID": "extension2.html#time-dependent-exposure",
    "href": "extension2.html#time-dependent-exposure",
    "title": "Extensions in Survival and Longitudinal Analyses",
    "section": "Time-dependent exposure",
    "text": "Time-dependent exposure\nUnlike a time-fixed exposure, whose values are known at study entry (time zero), values for a time-dependent exposure can change over the course of follow-up. Consider an example of a multiple sclerosis (MS) cohort, where we are interested in the relationship between disease-modifying drugs (DMDs) and long-term mortality. Not every patient is exposed to DMDs at their MS diagnosis. Instead, some patients are never exposed to DMDs, some may be exposed to DMDs at the time of their MS diagnosis, while others may be exposed many years later. In this case, the exposure status of patients is not fixed at time zero, but rather depends on time. Simultaneously dealing with time-dependent exposure and residual confounding bias can be challenging.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime-dependent exposure\n\n\n\n\nImmortal time bias occurs when a period during follow-up, in which a patient cannot experience the outcome (e.g., death), is misclassified as time under observation, often leading to an overestimation of the treatment’s effectiveness\nIn the MS example, immortal time can occur if patients must survive long enough to receive a DMD, which could falsely enhance the perceived survival benefit of these drugs\nEmploying Cox proportional hazards models with time-varying exposure to DMDs can help mitigate immortal time bias. However, this time-dependent Cox regression cannot deal with residual confounding bias.\n\n\n\n\n\n(Jones and Fowler 2016; Beyersmann, Wolkewitz, and Schumacher 2008; Karim et al. 2014)",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses"
    ]
  },
  {
    "objectID": "extension2.html#high-dimensional-disease-risk-score-hddrs",
    "href": "extension2.html#high-dimensional-disease-risk-score-hddrs",
    "title": "Extensions in Survival and Longitudinal Analyses",
    "section": "High-dimensional disease risk score (hdDRS)",
    "text": "High-dimensional disease risk score (hdDRS)\nThe hdPS technique might not reduce significant bias due to an overfitted exposure model, particularly with a rare exposure. An alternative confounding adjustment method to hdPS is hdDRS. In contrast, hdPS separates the exposure modelling from the outcome modelling, ultimately giving the end-user more flexibility in adjusting for confounding effects (e.g., via inverse-probability-weighting). On the other hand, the hdDRS achieves the balancing of the confounders by modelling the outcome.\n\n\n\n\n\n\nHigh-dimensional disease risk score\n\n\n\n\nhdDRS can be an alternative to hdPS for dealing with residual confounding bias\nhdDRS could be particularly helpful in situations where the exposure is rare or the outcome is a repeated measure\n\n\n\n\n\n\n\n(Kumamaru et al. 2016; Hossain 2025)\n\n\nBeyersmann, Jan, Martin Wolkewitz, and Martin Schumacher. 2008. “The Impact of Time-Dependent Bias in Proportional Hazards Modelling.” Statistics in Medicine 27 (30): 6439–54.\n\n\nBross, Irwin DJ. 1966. “Spurious Effects from an Extraneous Variable.” Journal of Chronic Diseases 19 (6): 637–47.\n\n\nHossain, Md Belal. 2025. “Chapter 2: High-Dimensional Disease Risk Score for Dealing with Residual Confounding in Estimating Treatment Effects with a Survival Outcome.” In. Harnessing the power of causal inference; predictive analytics for survival outcomes with health administrative data: applications to tuberculosis research.\n\n\nJones, Mark, and Robert Fowler. 2016. “Immortal Time Bias in Observational Studies of Time-to-Event Outcomes.” Journal of Critical Care 36: 195–99.\n\n\nKarim, Mohammad Ehsanul, Paul Gustafson, John Petkau, Yinshan Zhao, Afsaneh Shirani, Elaine Kingwell, Charity Evans, Mia Van Der Kop, Joel Oger, and Helen Tremlett. 2014. “Marginal Structural Cox Models for Estimating the Association Between \\(\\beta\\)-Interferon Exposure and Disease Progression in a Multiple Sclerosis Cohort.” American Journal of Epidemiology 180 (2): 160–71.\n\n\nKumamaru, Hiraku, Sebastian Schneeweiss, Robert J Glynn, Soko Setoguchi, and Joshua J Gagne. 2016. “Dimension Reduction and Shrinkage Methods for High Dimensional Disease Risk Scores in Historical Data.” Emerging Themes in Epidemiology 13: 1–10.\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn, Helen Mogun, and M Alan Brookhart. 2009. “High-Dimensional Propensity Score Adjustment in Studies of Treatment Effects Using Health Care Claims Data.” Epidemiology (Cambridge, Mass.) 20 (4): 512.",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses"
    ]
  },
  {
    "objectID": "hddrs1.html",
    "href": "hddrs1.html",
    "title": "21  hdDRS with a binary outcome",
    "section": "",
    "text": "21.1 Step 0: Analytic data\nSimilar to hdPS, there are seven steps in High-dimensional disease risk score (hdDRS). However, the sixth step is different. In the sixth step, we fit a propensity score model in the hdPS analysis, while we fit a disease risk score model in the hdDRS analysis. Moreover in the seventh step, estimating the treatment/exposure effect with the matched or inverse probability weighted data is a popular choice in the hdPS analysis, while adjusting for deciles of the disease risk score is a popular choice in the hdDRS analysis.\nWe will use the same NHANES data for this hdDRS demonstration as we did for the hdPS with a binary exposure and a binary outcome demonstration.\ndim(hdps.data)\n#&gt; [1] 3839   78",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>hdDRS with a binary outcome</span>"
    ]
  },
  {
    "objectID": "hddrs1.html#step-6-disease-risk-score-drs",
    "href": "hddrs1.html#step-6-disease-risk-score-drs",
    "title": "21  hdDRS with a binary outcome",
    "section": "21.2 Step 6: Disease risk score (DRS)",
    "text": "21.2 Step 6: Disease risk score (DRS)\nHansen (2008) shows that the DRS has a balancing property, called prognostic balance. Individuals sharing a similar DRS value can be regarded as having the same risk/prognosis for the outcome. In contrast, the propensity score has a covariate balance property.\n\n\n(Hansen 2008)\n\n21.2.1 Create DRS formula\nWith a binary outcome, there are two approaches to estimate the disease risk score (DRS):\n\nFitting DRS model on unexposed individuals: Covariates include the investigator-specified and empirical covariates\nFitting DRS model on the full cohort: Covariates include the exposure, investigator-specified and empirical covariates.\n\nIn this example, we will focus on fitting the DRS model on unexposed individuals:\n\nhdps.data$outcome &lt;- as.numeric(I(hdps.data$diabetes=='Yes'))\nproxy.list.sel &lt;- names(out3$autoselected_covariate_df[,-1])\nproxyform &lt;- paste0(proxy.list.sel, collapse = \"+\")\ncovform &lt;- paste0(investigator.specified.covariates, collapse = \" + \")\n\n\nrhsformula &lt;- paste0(c(covform, proxyform), collapse = \"+\")\ndrs.formula &lt;- as.formula(paste0(\"outcome\", \"~\", rhsformula))\ndrs.formula\n#&gt; outcome ~ age.cat + sex + education + race + marital + income + \n#&gt;     born + year + diabetes.family.history + medical.access + \n#&gt;     smoking + diet.healthy + physical.activity + sleep + uric.acid + \n#&gt;     protein.total + bilirubin.total + phosphorus + sodium + potassium + \n#&gt;     globulin + calcium.total + systolicBP + diastolicBP + high.cholesterol + \n#&gt;     rec_dx_I10_once + rec_dx_R73_once + rec_dx_I10_frequent + \n#&gt;     rec_dx_R60_once + rec_dx_E78_once + rec_dx_M79_once + rec_dx_I51_once + \n#&gt;     rec_dx_M10_once + rec_dx_I50_once + rec_dx_K21_once + rec_dx_D75_once + \n#&gt;     rec_dx_Z79_once + rec_dx_F41_once + rec_dx_M1A_once + rec_dx_E87_once + \n#&gt;     rec_dx_R12_once + rec_dx_R51_once + rec_dx_J45_once + rec_dx_I50_frequent + \n#&gt;     rec_dx_L70_once + rec_dx_M25_once + rec_dx_I63_once + rec_dx_R39_once + \n#&gt;     rec_dx_N28_once + rec_dx_K25_once + rec_dx_F90_once + rec_dx_B00_once + \n#&gt;     rec_dx_J42_once + rec_dx_R41_once + rec_dx_I20_once + rec_dx_M54_once + \n#&gt;     rec_dx_J44_once + rec_dx_K08_once + rec_dx_I21_once + rec_dx_F32_once + \n#&gt;     rec_dx_J30_once + rec_dx_F43_once + rec_dx_R06_once + rec_dx_I48_once + \n#&gt;     rec_dx_R32_once + rec_dx_R42_once + rec_dx_N92_once + rec_dx_N95_once + \n#&gt;     rec_dx_M19_once + rec_dx_E07_once + rec_dx_R25_once + rec_dx_G43_once + \n#&gt;     rec_dx_R52_once + rec_dx_M81_once + rec_dx_T78_once\n\n\n\n21.2.2 Unexposed cohort\n\n# Unexposed cohort\ndat.unexposed &lt;- subset(hdps.data, obese == \"No\")\n\n\n\n21.2.3 Fit DRS model\nHere we fit a logistic regression model on the unexposed individuals. Similar to the hdPS example, we are adding only the main effects in the non-transformed form.\n\nfit.drs &lt;- glm(drs.formula, data = dat.unexposed, family = binomial)\n\n\n\n21.2.4 Obtain DRS\nNow we can obtain the DRS for the full cohort and check the summary. Unlike in hdPS, balance checking is difficult in hdDRS analysis. There are some proposed matching and weighting techniques with hdDRS where balance checking is possible. These techniques are developed/proposed with only investigator-specified covariates. In this demonstration, we adjust our outcome model for deciles of the DRS, which is a common practice in the hdDRS literature.\n\n\n(Wyss et al. 2015; Nguyen et al. 2024)\n\nhdps.data$drs &lt;- predict(fit.drs, type = \"response\", newdata = hdps.data)\n\n# Sumamry\nsummary(hdps.data$drs)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.00000 0.01666 0.07709 0.19312 0.26457 0.99992\n\n# Summary by exposure status\ntapply(hdps.data$drs, hdps.data$obese, summary)\n#&gt; $No\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.00000 0.01664 0.06980 0.17814 0.23025 0.99963 \n#&gt; \n#&gt; $Yes\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.00000 0.01670 0.08679 0.21374 0.30472 0.99992",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>hdDRS with a binary outcome</span>"
    ]
  },
  {
    "objectID": "hddrs1.html#step-7-association",
    "href": "hddrs1.html#step-7-association",
    "title": "21  hdDRS with a binary outcome",
    "section": "21.3 Step 7: Association",
    "text": "21.3 Step 7: Association\n\n21.3.1 Adjtsing for deciles of DRS\n\n# Deciles of DRS\nhdps.data$drs.decile &lt;- as.factor(dplyr::ntile(hdps.data$drs, 10))\n\n# Outcome analysis\nfit.hddrs &lt;- glm(outcome ~ obese + drs.decile, \n                 data = hdps.data,\n                 family = binomial)\n\npublish(fit.hddrs, pvalue.method = \"robust\", confint.method = \"robust\", \n        print = F)$regressionTable[1:2,]\n\n\n  \n\n\n\n\n\n21.3.2 Adjtsing for deciles of DRS and covariates\nAnother popular approach is to adjust for deciles of the DRS as well as for the investigator-specified covariates:\n\n# Outcome analysis\nfit.hddrs1 &lt;- glm(outcome ~ obese + drs.decile + age.cat + sex + education + \n                   race + marital + income + born + year + \n                   diabetes.family.history + medical.access + smoking + \n                   diet.healthy + physical.activity + sleep + uric.acid + \n                   protein.total + bilirubin.total + phosphorus + sodium + \n                   potassium + globulin + calcium.total + systolicBP + \n                   diastolicBP + high.cholesterol, data = hdps.data,\n                 family = binomial)\n\npublish(fit.hddrs1, pvalue.method = \"robust\", confint.method = \"robust\", \n        print = F)$regressionTable[1:2,]\n\n\n  \n\n\n\n\n\n\n\nHansen, Ben B. 2008. “The Prognostic Analogue of the Propensity Score.” Biometrika 95 (2): 481–88.\n\n\nHossain, Md Belal. 2025. “Chapter 2: High-Dimensional Disease Risk Score for Dealing with Residual Confounding in Estimating Treatment Effects with a Survival Outcome.” In. Harnessing the power of causal inference; predictive analytics for survival outcomes with health administrative data: applications to tuberculosis research.\n\n\nKumamaru, Hiraku, Joshua J Gagne, Robert J Glynn, Soko Setoguchi, and Sebastian Schneeweiss. 2016. “Comparison of High-Dimensional Confounder Summary Scores in Comparative Studies of Newly Marketed Medications.” Journal of Clinical Epidemiology 76: 200–208.\n\n\nNguyen, Tri-Long, Thomas PA Debray, Bora Youn, Gabrielle Simoneau, and Gary S Collins. 2024. “Confounder Adjustment Using the Disease Risk Score: A Proposal for Weighting Methods.” American Journal of Epidemiology 193 (2): 377–88.\n\n\nWyss, Richard, Alan R Ellis, M Alan Brookhart, Michele Jonsson Funk, Cynthia J Girman, Ross J Simpson Jr, and Til Stürmer. 2015. “Matching on the Disease Risk Score in Comparative Effectiveness Research of New Treatments.” Pharmacoepidemiology and Drug Safety 24 (9): 951–61.",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>hdDRS with a binary outcome</span>"
    ]
  },
  {
    "objectID": "survival.html",
    "href": "survival.html",
    "title": "22  hdPS with a time-to-event outcome",
    "section": "",
    "text": "22.1 Step 0: Analytic data\nTo demonstrate the use of the hdPS analysis with a time-to-event outcome, we will use a simulated dataset. The example is to explore the relationship between arthritis (binary exposure) and CVD (time-to-event outcome).\nThe simulated dataset contains information on 3,000 individuals with the following variables:\nhead(simdat)",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>hdPS with a time-to-event outcome</span>"
    ]
  },
  {
    "objectID": "survival.html#step-1-proxy-sources",
    "href": "survival.html#step-1-proxy-sources",
    "title": "22  hdPS with a time-to-event outcome",
    "section": "22.2 Step 1: Proxy sources",
    "text": "22.2 Step 1: Proxy sources\n\n22.2.1 Data with investigator-specified covariates\nLet us check the summary statistics of the investigator-specified covariates, stratified by the exposure variable (arthritis).\n\n# Table 1\ntab1 &lt;- CreateTableOne(vars = c(\"age\", \"sex\", \"comorbidity\"), \n                       strata = \"arthritis\", \n                       data = simdat, \n                       test = FALSE)\nprint(tab1, showAllLevels = TRUE, noSpaces = TRUE, quote = FALSE, smd = TRUE)\n#&gt;                  Stratified by arthritis\n#&gt;                   level  No           Yes           SMD  \n#&gt;   n                      2143         857                \n#&gt;   age (mean (SD))        48.88 (9.53) 53.65 (10.03) 0.487\n#&gt;   sex (%)         Female 1189 (55.5)  592 (69.1)    0.283\n#&gt;                   Male   954 (44.5)   265 (30.9)         \n#&gt;   comorbidity (%) No     1629 (76.0)  596 (69.5)    0.146\n#&gt;                   Yes    514 (24.0)   261 (30.5)\n\n# Bivariate table\nround(prop.table(table(arthritis = simdat$arthritis, CVD = simdat$cvd),\n                 margin = 1)*100, 2)\n#&gt;          CVD\n#&gt; arthritis     0     1\n#&gt;       No  70.70 29.30\n#&gt;       Yes 43.52 56.48\n\n\n\n22.2.2 Proxy data\nIn this example, we will use four data dimensions:\n\n3-digit diagnostic codes from hospital database (diag)\n3-digit procedure codes from hospital database (proc)\n3-digit icd codes from physician claim database (msp)\nDINPIN from drug dispensation database (din)\n\n\ntable(dat.proxy$dim)\n#&gt; \n#&gt;  diag   din   msp  proc \n#&gt; 20000   269 44179   321\n\n\ndat.proxy &lt;- dat.proxy[order(dat.proxy$studyid),]\ndat.proxy[5001:5010,]",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>hdPS with a time-to-event outcome</span>"
    ]
  },
  {
    "objectID": "survival.html#step-2-empirical-covariates",
    "href": "survival.html#step-2-empirical-covariates",
    "title": "22  hdPS with a time-to-event outcome",
    "section": "22.3 Step 2: Empirical covariates",
    "text": "22.3 Step 2: Empirical covariates\nThe same as before, top 200 covariates with highest prevalence are chosen.\n\nlibrary(autoCovariateSelection)\nid &lt;- simdat$studyid\n\nstep1 &lt;- get_candidate_covariates(df = dat.proxy, domainVarname = \"dim\", \n                                  eventCodeVarname = \"code\", \n                                  patientIdVarname = \"studyid\", \n                                  patientIdVector = id, \n                                  n = 200, \n                                  min_num_patients = 20)\nout1 &lt;- step1$covars_data\nhead(out1)",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>hdPS with a time-to-event outcome</span>"
    ]
  },
  {
    "objectID": "survival.html#step-3-recurrence",
    "href": "survival.html#step-3-recurrence",
    "title": "22  hdPS with a time-to-event outcome",
    "section": "22.4 Step 3: Recurrence",
    "text": "22.4 Step 3: Recurrence\nIn this step, we generate a maximum of 3 binary recurrence covariates for each of the candidate proxy/code. We observed 401 recurrence covariates in this analysis.\n\n22.4.1 Assessing recurrence of codes\n\nall.equal(id, step1$patientIds)\n#&gt; [1] TRUE\n\nstep2 &lt;- get_recurrence_covariates(df = out1, \n                                   eventCodeVarname = \"code\", \n                                   patientIdVarname = \"studyid\",\n                                   patientIdVector = id)\nout2 &lt;- step2$recurrence_data\ndim(out2)\n#&gt; [1] 3000  402\n\n\n\n22.4.2 Recurrence covariates\n\nvars.empirical &lt;- names(out2)[-1]\nhead(vars.empirical)\n#&gt; [1] \"rec_diag_C44_once\" \"rec_diag_C81_once\" \"rec_diag_C83_once\"\n#&gt; [4] \"rec_diag_E08_once\" \"rec_diag_E09_once\" \"rec_diag_E10_once\"\n\n\n\n22.4.3 Merging all recurrence covariates with the analytic dataset\n\nhdps.data &lt;- merge(simdat, out2, by = \"studyid\", all.x = T)\ndim(hdps.data)\n#&gt; [1] 3000  408",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>hdPS with a time-to-event outcome</span>"
    ]
  },
  {
    "objectID": "survival.html#step-4-prioritize",
    "href": "survival.html#step-4-prioritize",
    "title": "22  hdPS with a time-to-event outcome",
    "section": "22.5 Step 4: Prioritize",
    "text": "22.5 Step 4: Prioritize\nThe Bross formula requires the exposure, outcome, and proxy covariates to be binary. With the time-to-event outcome, we will use Cox-PH with LASSO regularization to prioritize the empirical covariates. The hyperparameter (\\(\\lambda\\)) will be selected using 5-fold cross-validation.\n\n22.5.1 Hyperparameter tuning\n\n# Formula with only empirical covariates\nformula.out &lt;- as.formula(paste(\"Surv(follow_up, cvd) ~ \", \n                                paste(vars.empirical, collapse = \" + \")))\n\n# Model matrix for fitting Cox with LASSO regularization\nX &lt;- model.matrix(formula.out, data = hdps.data)[,-1]\nY &lt;- as.matrix(data.frame(time = hdps.data$follow_up, status = hdps.data$cvd))\n\n# Detect the number of cores\nn_cores &lt;- parallel::detectCores()\n\n# Create a cluster of cores\ncl &lt;- makeCluster(n_cores - 1)\n\n# Register the cluster for parallel processing\nregisterDoParallel(cl)\n\n# Hyperparameter tuning with 5-fold cross-validation \nset.seed(123)\nfit.lasso &lt;- cv.glmnet(x = X, y = Y, nfolds = 5, parallel = T, alpha = 1,\n                       family = \"cox\")\nstopCluster(cl)\n\nplot(fit.lasso)\n\n\n\n\n\n\n\n\n## Best lambda\nfit.lasso$lambda.min\n#&gt; [1] 0.03093478\n\n\n\n22.5.2 Variable ranking based on Cox-LASSO\n\nempvars.lasso &lt;- coef(fit.lasso, s = fit.lasso$lambda.min) \nempvars.lasso &lt;- data.frame(as.matrix(empvars.lasso))\nempvars.lasso &lt;- data.frame(vars = rownames(empvars.lasso),\n                            coef = empvars.lasso)\ncolnames(empvars.lasso) &lt;- c(\"vars\", \"coef\")\nrownames(empvars.lasso) &lt;- NULL\n\n# Number of non-zero coefficients\ntable(empvars.lasso$coef != 0)\n#&gt; \n#&gt; FALSE \n#&gt;   401\n\nSince proxies were random and unrelated to the simulated data, LASSO produced all zero coefficients. Let choose an arbitrary value as to demonstrate the process of variable selection.\n\nempvars.lasso &lt;- coef(fit.lasso, s = exp(-6)) \nempvars.lasso &lt;- data.frame(as.matrix(empvars.lasso))\nempvars.lasso &lt;- data.frame(vars = rownames(empvars.lasso), \n                            coef = empvars.lasso)\ncolnames(empvars.lasso) &lt;- c(\"vars\", \"coef\")\nrownames(empvars.lasso) &lt;- NULL\nhead(empvars.lasso)\n\n\n  \n\n\n\n# Number of non-zero coefficients\ntable(empvars.lasso$coef != 0)\n#&gt; \n#&gt; FALSE  TRUE \n#&gt;    71   330\n\n\n\n22.5.3 Rank empirical covariates\nNow we will rank the empirical covariates based on absolute value of log hazard ratio.\n\nempvars.lasso$coef.abs &lt;- abs(empvars.lasso$coef)\nempvars.lasso &lt;- empvars.lasso[order(empvars.lasso$coef.abs, decreasing = T),]\nhead(empvars.lasso)",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>hdPS with a time-to-event outcome</span>"
    ]
  },
  {
    "objectID": "survival.html#step-5-covariates",
    "href": "survival.html#step-5-covariates",
    "title": "22  hdPS with a time-to-event outcome",
    "section": "22.6 Step 5: Covariates",
    "text": "22.6 Step 5: Covariates\nWe used all investigator-specified covariates and the top 200 empirical covariates for the PS model. Again, this is a simplistic scenario where we only consider the main effects of the covariates.\n\n# Investigator-specified covariates\ninvestigator.vars &lt;- c(\"age\", \"sex\", \"comorbidity\")\n\n# Top 200 empirical covariates section based on Cox-LASSO\nempirical.vars.lasso &lt;- empvars.lasso$vars[1:200]\n\n# Investigator-specified and empirical covariates\nvars.hsps &lt;- c(investigator.vars, empirical.vars.lasso)\nhead(vars.hsps)\n#&gt; [1] \"age\"               \"sex\"               \"comorbidity\"      \n#&gt; [4] \"rec_diag_V03_once\" \"rec_diag_S24_once\" \"rec_diag_W61_once\"",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>hdPS with a time-to-event outcome</span>"
    ]
  },
  {
    "objectID": "survival.html#step-6-propensity-score",
    "href": "survival.html#step-6-propensity-score",
    "title": "22  hdPS with a time-to-event outcome",
    "section": "22.7 Step 6: Propensity score",
    "text": "22.7 Step 6: Propensity score\n\n22.7.1 Create propensity score formula\n\nps.formula &lt;- as.formula(paste0(\"I(arthritis == 'Yes') ~ \", \n                                paste(vars.hsps, collapse = \"+\")))\n\n\n\n22.7.2 Fit PS model\n\nrequire(WeightIt)\nW.out &lt;- weightit(ps.formula, \n                    data = hdps.data, \n                    estimand = \"ATE\",\n                    method = \"ps\", \n                  stabilize = T)\n\n\n\n22.7.3 Obtain PS\n\nhdps.data$ps &lt;- W.out$ps\n\n\n\n22.7.4 Obtain weights\n\nhdps.data$w &lt;- W.out$weights\n\n\n\n\n\n\n\n\n\n\n\n\n22.7.5 Assessing balance",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>hdPS with a time-to-event outcome</span>"
    ]
  },
  {
    "objectID": "survival.html#step-7-association",
    "href": "survival.html#step-7-association",
    "title": "22  hdPS with a time-to-event outcome",
    "section": "22.8 Step 7: Association",
    "text": "22.8 Step 7: Association\n\n22.8.1 Obtain HR\n\nlibrary(survival)\nlibrary(Publish)\nfit.hdps &lt;- coxph(Surv(follow_up, cvd) ~ arthritis, \n                  weights = w,\n                  data = hdps.data)\npublish(fit.hdps, pvalue.method = \"robust\", confint.method = \"robust\", \n        print = F)$regressionTable[1:2,]\n\n\n  \n\n\n\n\n\n22.8.2 Obtain HR with survey package\n\nlibrary(survey)\n# Create a design\nsvy.design &lt;- svydesign(id = ~1, weights = ~w, data = hdps.data)\n\n# Model\nfit.hdps1 &lt;- svycoxph(Surv(follow_up, cvd) ~ arthritis, \n                      design = svy.design)\npublish(fit.hdps1, print = F)$regressionTable[1:2,]\n#&gt; Independent Sampling design (with replacement)\n#&gt; svydesign(id = ~1, weights = ~w, data = hdps.data)",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>hdPS with a time-to-event outcome</span>"
    ]
  },
  {
    "objectID": "hddrs2.html",
    "href": "hddrs2.html",
    "title": "23  hdDRS with a survival outcome",
    "section": "",
    "text": "23.1 Step 0: Analytic data\nTo demonstrate the high-dimensional disease risk score (hdDRS) with a survival/time-to-event outcome, we will use the same simulated data for this hdDRS demonstration as we did for the hdPS with survival outcome demonstration.\ndim(hdps.data)\n#&gt; [1] 3000  410\n\n# Investigator-specified covariates\ninvestigator.vars\n#&gt; [1] \"age\"         \"sex\"         \"comorbidity\"\n\n# Top 200 empirical covariates section based on Cox-LASSO\nhead(empirical.vars.lasso)\n#&gt; [1] \"rec_diag_V03_once\" \"rec_diag_S24_once\" \"rec_diag_W61_once\"\n#&gt; [4] \"rec_diag_C81_once\" \"rec_diag_M85_once\" \"rec_diag_H18_once\"\n\n# Investigator-specified and empirical covariates\nhead(vars.hsps)\n#&gt; [1] \"age\"               \"sex\"               \"comorbidity\"      \n#&gt; [4] \"rec_diag_V03_once\" \"rec_diag_S24_once\" \"rec_diag_W61_once\"",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>hdDRS with a survival outcome</span>"
    ]
  },
  {
    "objectID": "hddrs2.html#step-6-disease-risk-score-drs",
    "href": "hddrs2.html#step-6-disease-risk-score-drs",
    "title": "23  hdDRS with a survival outcome",
    "section": "23.2 Step 6: Disease risk score (DRS)",
    "text": "23.2 Step 6: Disease risk score (DRS)\nThere are at least eight approaches to estimate the disease risk score (DRS):\n\nhdDRS-Full-Logistic: On the full cohort (both exposed and unexposed), fit logistic regression without considering the follow-up time. This model included the exposure, investigator-specified measured confounders, and the recurrence covariates. The DRS is calculated as the probability of the outcome by setting everyone as unexposed.\nhdDRS-Full-Survival: On the full cohort, fit the Cox-PH model with the exposure, investigator-specified measured confounders and the recurrence covariates. The DRS is calculated as the survival probability of the outcome by setting everyone as unexposed.\nhdDRS-Full-Hazard: On the full cohort, fit the Cox-PH model with the exposure, investigator-specified measured confounders and the recurrence covariates. The DRS is calculated as the hazard of the outcome by setting everyone unexposed.\nhdDRS-Full-Rate: On the full cohort, fit the modified Poisson regression with the exposure, an offset by the natural logarithm of follow-up time, investigator-specified measured confounders and the recurrence covariates. The DRS is calculated as the rate of the outcome by setting everyone as unexposed.\nhdDRS-Unexposed-Logistic: On the cohort with only unexposed, fit the logistic regression with the investigator-specified measured confounders and the recurrence covariates. The DRS is calculated as the probability of the outcome on the full cohort.\nhdDRS-Unexposed-Survival: On the cohort with only unexposed, fit the Cox-PH model with the investigator-specified measured confounders and the recurrence covariates. The DRS is calculated as the survival probability of the outcome on the full cohort.\nhdDRS-Unexposed-Hazard: On the cohort with only unexposed, fit the Cox-PH model with the investigator-specified measured confounders and the recurrence covariates. The DRS is calculated as the hazard of the outcome on the full cohort.\nhdDRS-Unexposed-Rate: On the cohort with only unexposed, fit the modified Poisson regression with an offset by the natural logarithm of follow-up time, investigator-specified measured confounders and the recurrence covariates. The DRS is calculated as the rate of the outcome on the full cohort.\n\nIn this example, we will focus on the rate-based approach using the unexposed cohort (hdDRS-Unexposed-Rate). To demonstrate how to apply all these eight methods in a given scenario, reproducible R codes on a simulated dataset are provided in the GitHub folder.\n\n\n(Hansen 2008; Zhang and Kim 2019; Hossain 2025)\n\n23.2.1 Unexposed cohort\n\n# Unexposed cohort\ndat.unexposed &lt;- subset(hdps.data, arthritis == \"No\")\n\n# Offset \ndat.unexposed$log.offset &lt;- log(1)\n\n# Full cohort\ndat.full &lt;- hdps.data\n\n# Offset  \ndat.full$log.offset &lt;- log(1)\n\n\n\n23.2.2 Create DRS formula\n\n# Covariates\nvars.hsdrs &lt;- c(investigator.vars, empirical.vars.lasso)\n\n# Formula\ndrs.formula &lt;- as.formula(paste0(\"cvd ~ offset(log.offset) + \", \n                                 paste(vars.hsdrs, collapse = \"+\")))\n\n\n\n23.2.3 Fit DRS model\n\nfit.drs &lt;- glm(drs.formula, data = dat.unexposed, family = poisson)\nfit.drs$coefficients[is.na(fit.drs$coefficients)] &lt;- 0\n\n\n\n23.2.4 Obtain DRS\n\ndat.full$drs &lt;- predict(fit.drs, type = \"response\", newdata = dat.full)\n\n# Sumamry\nsummary(dat.full$drs)\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004198 0.089762 0.232163 0.324618 0.466890 4.085130\n\n# Summary by exposure status\ntapply(dat.full$drs, dat.full$arthritis, summary)\n#&gt; $No\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004198 0.081598 0.206067 0.293047 0.433756 1.776528 \n#&gt; \n#&gt; $Yes\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.006824 0.129409 0.298506 0.403565 0.568061 4.085130",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>hdDRS with a survival outcome</span>"
    ]
  },
  {
    "objectID": "hddrs2.html#step-7-association",
    "href": "hddrs2.html#step-7-association",
    "title": "23  hdDRS with a survival outcome",
    "section": "23.3 Step 7: Association",
    "text": "23.3 Step 7: Association\n\n# Deciles of DRS\ndat.full$drs.decile &lt;- as.factor(dplyr::ntile(dat.full$drs, 10))\n\n# Outcome analysis\nfit.hddrs &lt;- coxph(Surv(follow_up, cvd) ~ arthritis + drs.decile + age + sex + \n                     comorbidity, data = dat.full)\n\npublish(fit.hddrs, pvalue.method = \"robust\", confint.method = \"robust\", \n        print = F)$regressionTable[1:2,]\n\n\n  \n\n\n\n\n\n\n\nHansen, Ben B. 2008. “The Prognostic Analogue of the Propensity Score.” Biometrika 95 (2): 481–88.\n\n\nHossain, Md Belal. 2025. “Chapter 2: High-Dimensional Disease Risk Score for Dealing with Residual Confounding in Estimating Treatment Effects with a Survival Outcome.” In. Harnessing the power of causal inference; predictive analytics for survival outcomes with health administrative data: applications to tuberculosis research.\n\n\nZhang, Di, and Jessica Kim. 2019. “Use of Propensity Score and Disease Risk Score for Multiple Treatments with Time-to-Event Outcome: A Simulation Study.” Journal of Biopharmaceutical Statistics 29 (6): 1103–15.",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>hdDRS with a survival outcome</span>"
    ]
  },
  {
    "objectID": "survival2.html",
    "href": "survival2.html",
    "title": "24  Application in MS",
    "section": "",
    "text": "A recent article summarizes the estimated hazard ratios (HRs) for the association between exposure to any disease-modifying drug (DMD) and time from cohort entry (index date) to all-cause mortality among individuals with multiple sclerosis (MS) in British Columbia, Canada (1996–2017), using different methods of confounding adjustment (Karim et al. 2025).\n\nUnadjusted analysis suggested a strong protective effect (HR 0.31, 95% CI: 0.27–0.36).\nInvestigator-specified covariate adjustment attenuated this effect (aHR 0.76, 95% CI: 0.65–0.89).\nHigh-dimensional propensity score (hdPS) methods further adjusted using empirically selected covariates:\n\nhdPS-1 to hdPS-3 yielded aHRs ranging from 0.77 to 0.80.\n\nHigh-dimensional disease risk score (hdDRS) methods produced similar estimates:\n\nhdDRS-1 to hdDRS-3 resulted in aHRs between 0.79 and 0.81.\n\n\n\n\n\n\n\n\nTip\n\n\n\n(Karim et al. 2025)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDespite adding high-dimensional proxy data, effect estimates changed only slightly from those using conventional covariate adjustment. These results suggest that residual confounding may be modest or that available proxies are insufficient to capture unmeasured confounders fully.\n\n\n\n\nKarim, Mohammad Ehsanul, Md Belal Hossain, Huah Shin Ng, Feng Zhu, Hanna A Frank, and Helen Tremlett. 2025. “Evaluating the Role of High-Dimensional Proxy Data in Confounding Adjustment in Multiple Sclerosis Research: A Case Study.” Pharmacoepidemiology and Drug Safety 34 (2): e70112.",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Application in MS</span>"
    ]
  },
  {
    "objectID": "ncc.html",
    "href": "ncc.html",
    "title": "25  Time-dependent exposure",
    "section": "",
    "text": "25.1 Time-dependent Cox regression\nLet us start with an example of exploring the relationship between disease-modifying drugs (DMDs) for multiple sclerosis and long-term mortality. The DMD exposure is a time-dependent variable, and the mortality outcome is a time-to-event outcome. Employing Cox proportional hazards models with time-varying exposure to DMDs can address immortal time bias in this example.",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Time-dependent exposure</span>"
    ]
  },
  {
    "objectID": "ncc.html#time-dependent-cox-regression",
    "href": "ncc.html#time-dependent-cox-regression",
    "title": "25  Time-dependent exposure",
    "section": "",
    "text": "Time-dependent exposure\n\n\n\n\nTime-dependent Cox regression with time-varying exposure can help mitigate immortal time bias.\nThe hdPS approach is used to deal with residual confounding with a binary ‘time-fixed’ treatment\nWith a ‘time-dependent’ exposure, implementing the hdPS in conjunction with the time-dependent Cox regression presents a methodological and practical challenge.\n\nSee the associated article for more details (Hossain et al. 2025).",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Time-dependent exposure</span>"
    ]
  },
  {
    "objectID": "ncc.html#nested-case-control-ncc",
    "href": "ncc.html#nested-case-control-ncc",
    "title": "25  Time-dependent exposure",
    "section": "25.2 Nested case-control (NCC)",
    "text": "25.2 Nested case-control (NCC)\nThe nested case-control (NCC) design is a well-established method for addressing immortal time bias with a time-dependent exposure. The NCC framework provides a robust alternative for addressing immortal time bias, while allowing for the integration of hdPS analysis to minimize the residual confounding bias.\n\n\n(Austin et al. 2012; Ernster 1994; Hossain et al. 2024)\n\n\n\n\n\n\nNCC\n\n\n\n\nMatch subjects who experienced the event of interest (called cases) to a subset of event-free subjects (called controls) using incidence density sampling\nSome controls could later become cases themselves and also serve as controls for other cases\nFour controls per case has been shown to provide near-optimal statistical efficiency without the need for the full cohort analysis",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Time-dependent exposure</span>"
    ]
  },
  {
    "objectID": "ncc.html#hdps-in-the-ncc-framework",
    "href": "ncc.html#hdps-in-the-ncc-framework",
    "title": "25  Time-dependent exposure",
    "section": "25.3 hdPS in the NCC framework",
    "text": "25.3 hdPS in the NCC framework\nThe time-dependent exposure status becomes a time-independent exposure variable in the NCC analysis. Hence, we could implement the hdPS technique in the NCC framework to deal with residual confounding bias.\n\n\n(Hossain et al. 2025)\n\n25.3.1 Step 0: Analytic data\nTo demonstrate the use of hdPS analysis with a time-dependent exposure, we will use a simulated dataset. This example explores the relationship between exposure to disease-modifying drugs (DMDs) for multiple sclerosis and all-cause mortality.\n\n25.3.1.1 Dataset with time-dependent exposure\n\nhead(simdat)\n\n\n  \n\n\n\n\n\n25.3.1.2 NCC with 4 control per case\nLet us use the nested case-control (NCC) design with 4 controls per case. The ccwc function from the Epi package is used to create the nested case-control dataset. The ccwc function requires the following arguments:\n\norigin: The time origin for the study\nentry: The time of entry into the study\nexit: The follow-up time\nfail: The event of interest\ncontrols: The number of controls per case\nmatch: The variables to match on\n\n\nlibrary(Epi)\nset.seed(100)\n\ndat.ncc &lt;- ccwc(\n  origin = 0,\n  entry = 0,\n  exit = follow_up,\n  fail = mortality_outcome,\n  controls = 4, \n  match = list(ses, cci, year),\n  include = list(id, follow_up, mortality_outcome, anyDMD, yrs_anyDMD, \n                 sex, age),\n  data = simdat,\n  silent = T\n  )\n\n# Drop those experienced the event before being exposed\ndat.ncc$anyDMD[dat.ncc$yrs_anyDMD &gt; dat.ncc$Time] &lt;- NA\ndat.ncc &lt;- dat.ncc[complete.cases(dat.ncc$anyDMD),]\n\ndat.ncc[1:10,]\n\n\n  \n\n\n\n\n# Rows\ndim(simdat)\n#&gt; [1] 19000    10\ndim(dat.ncc)\n#&gt; [1] 14370    14\n\n# Mortality status\ntable(simdat$mortality_outcome)\n#&gt; \n#&gt;     0     1 \n#&gt; 15947  3053\ntable(dat.ncc$Fail)\n#&gt; \n#&gt;     0     1 \n#&gt; 11317  3053\n\n\n\n\n25.3.2 Step 1: Proxy sources\nIn this example, we will use four data dimensions:\n\n3-digit diagnostic codes from hospital database (diag)\n3-digit procedure codes from hospital database (proc)\n3-digit icd codes from physician claim database (msp)\nDINPIN from drug dispensation database (din)\n\n\ntable(dat.proxy$dim)\n#&gt; \n#&gt;  diag   din   msp  proc \n#&gt; 10000   125 22135   158\n\n\n\n25.3.3 Step 2: Empirical covariates\n\nlibrary(autoCovariateSelection)\nid &lt;- simdat$id\n\nstep1 &lt;- get_candidate_covariates(df = dat.proxy, domainVarname = \"dim\", \n                                  eventCodeVarname = \"code\", \n                                  patientIdVarname = \"id\", \n                                  patientIdVector = id, \n                                  n = 1000, \n                                  min_num_patients = 20)\nout1 &lt;- step1$covars_data\nhead(out1)\n\n\n  \n\n\n\n\n\n25.3.4 Step 3: Recurrence\nLet us generate the binary recurrence covariates.\n\nall.equal(id, step1$patientIds)\n#&gt; [1] TRUE\n\n# Assessing recurrence of codes\nstep2 &lt;- get_recurrence_covariates(df = out1, \n                                   eventCodeVarname = \"code\", \n                                   patientIdVarname = \"id\",\n                                   patientIdVector = id)\nout2 &lt;- step2$recurrence_data\ndim(out2)\n#&gt; [1] 19000   454\n\n\n# Recurrence covariates\nvars.empirical &lt;- names(out2)[-1]\nhead(vars.empirical)\n#&gt; [1] \"rec_diag_H02_once\" \"rec_diag_H18_once\" \"rec_diag_H35_once\"\n#&gt; [4] \"rec_diag_H40_once\" \"rec_diag_H44_once\" \"rec_diag_I69_once\"\n\n\n25.3.4.1 Merging all recurrence covariates with the analytic dataset\n\nhdps.data &lt;- merge(dat.ncc, out2, by = \"id\", all.x = T)\ndim(hdps.data)\n#&gt; [1] 14370   467\n\n\n\n\n25.3.5 Step 4: Prioritize\nWe will use Cox-PH with LASSO regularization to prioritize the empirical covariates. The hyperparameter (\\(\\lambda\\)) will be selected using 5-fold cross-validation.\n\n25.3.5.1 Hyperparameter tuning\n\n# Formula with only empirical covariates\nformula.out &lt;- as.formula(paste(\"Surv(Time, Fail) ~ \", \n                                paste(vars.empirical, collapse = \" + \")))\n\n# Model matrix for fitting Cox with LASSO regularization\nX &lt;- model.matrix(formula.out, data = hdps.data)[,-1]\nY &lt;- as.matrix(data.frame(time = hdps.data$Time, status = hdps.data$Fail))\n\n# Detect the number of cores\nn_cores &lt;- parallel::detectCores()\n\n# Create a cluster of cores\ncl &lt;- makeCluster(n_cores - 1)\n\n# Register the cluster for parallel processing\nregisterDoParallel(cl)\n\n# Hyperparameter tuning with 5-fold cross-validation \nset.seed(123)\nfit.lasso &lt;- cv.glmnet(x = X, y = Y, nfolds = 5, parallel = T, alpha = 1, \n                       family = \"cox\")\nstopCluster(cl)\n\nplot(fit.lasso)\n\n\n\n\n\n\n\n\n## Best lambda\nfit.lasso$lambda.min\n#&gt; [1] 0.01042345\n\n\n\n25.3.5.2 Variable ranking based on Cox-LASSO\n\nempvars.lasso &lt;- coef(fit.lasso, s = fit.lasso$lambda.min) \nempvars.lasso &lt;- data.frame(as.matrix(empvars.lasso))\nempvars.lasso &lt;- data.frame(vars = rownames(empvars.lasso), \n                            coef = empvars.lasso)\ncolnames(empvars.lasso) &lt;- c(\"vars\", \"coef\")\nrownames(empvars.lasso) &lt;- NULL\n\n# Number of non-zero coefficients\ntable(empvars.lasso$coef != 0)\n#&gt; \n#&gt; FALSE  TRUE \n#&gt;   442    11\n\nSince proxies were random and unrelated to the simulated data, LASSO produced only 11 non-zero coefficients. Let choose an arbitrary value as to demonstrate the process of variable selection.\n\nempvars.lasso &lt;- coef(fit.lasso, s = exp(-6)) \nempvars.lasso &lt;- data.frame(as.matrix(empvars.lasso))\nempvars.lasso &lt;- data.frame(vars = rownames(empvars.lasso), \n                            coef = empvars.lasso)\ncolnames(empvars.lasso) &lt;- c(\"vars\", \"coef\")\nrownames(empvars.lasso) &lt;- NULL\nhead(empvars.lasso)\n\n\n  \n\n\n\n# Number of non-zero coefficients\ntable(empvars.lasso$coef != 0)\n#&gt; \n#&gt; FALSE  TRUE \n#&gt;   196   257\n\n\n\n25.3.5.3 Rank empirical covariates\nNow we will rank the empirical covariates based on absolute value of log hazard ratio.\n\nempvars.lasso$coef.abs &lt;- abs(empvars.lasso$coef)\nempvars.lasso &lt;- empvars.lasso[order(empvars.lasso$coef.abs, decreasing = T),]\nhead(empvars.lasso)\n\n\n  \n\n\n\n\n\n\n25.3.6 Step 5: Covariates\nWe used all investigator-specified covariates and the top 200 empirical covariates for the PS model. Again, this is a simplistic scenario where we only consider the main effects of the covariates.\n\n# Investigator-specified covariates\ninvestigator.vars &lt;- c(\"sex\", \"age\")\n\n# Top 200 empirical covariates section based on Cox-LASSO\nempirical.vars.lasso &lt;- empvars.lasso$vars[1:200]\n\n# Investigator-specified and empirical covariates\nvars.hsps &lt;- c(investigator.vars, empirical.vars.lasso)\nhead(vars.hsps)\n#&gt; [1] \"sex\"               \"age\"               \"rec_diag_T44_once\"\n#&gt; [4] \"rec_diag_O41_once\" \"rec_msp_793_once\"  \"rec_msp_459_once\"\n\n\n\n25.3.7 Step 6: Propensity score\n\n25.3.7.1 Create propensity score formula\n\nps.formula &lt;- as.formula(paste0(\"I(anyDMD == 'Yes') ~ \", \n                                paste(vars.hsps, collapse = \"+\")))\n\n\n\n25.3.7.2 Fit PS model\n\nrequire(WeightIt)\nW.out &lt;- weightit(ps.formula, \n                    data = hdps.data, \n                    estimand = \"ATE\",\n                    method = \"ps\",\n                  stabilize = T)\n\n\n\n25.3.7.3 Obtain PS\n\nhdps.data$ps &lt;- W.out$ps\n\n\n\n25.3.7.4 Obtain weights\n\nhdps.data$w &lt;- W.out$weights\n\n\n\n\n\n\n\n\n\n\n\n\n25.3.7.5 Assessing balance\n\n\n\n\n\n\n\n\n\n\nlibrary(survey)\n\n# Balance checking for investigator-specified covariates\ndesign.ipw &lt;- svydesign(ids = ~id, weights = ~w, data = hdps.data)\ntab.ipw &lt;- svyCreateTableOne(vars = investigator.vars, \n                             strata = \"anyDMD\", \n                             data = design.ipw, \n                             test = F)\nprint(tab.ipw, smd = T) # Age and sex are balanced\n#&gt;                  Stratified by anyDMD\n#&gt;                   No               Yes             SMD   \n#&gt;   n                11035.4          3324.4               \n#&gt;   sex = Male (%)    3199.3 (29.0)   1007.3 (30.3)   0.029\n#&gt;   age (mean (SD))    45.58 (13.75)   45.67 (13.48)  0.007\n\n\n\n\n25.3.8 Step 7: Association\n\n25.3.8.1 Obtain HR\nWe can fit the Cox-PH model, adjusting for the matched strata.\n\nlibrary(survival)\nlibrary(Publish)\nfit.hdps &lt;- coxph(Surv(Time, Fail) ~ anyDMD + strata(Set), \n                  weights = w,\n                  data = hdps.data)\npublish(fit.hdps, pvalue.method = \"robust\", confint.method = \"robust\", \n        print = F)$regressionTable[1:2,]\n\n\n  \n\n\n\n\n\n25.3.8.2 Obtain HR with conditional logistic\nFor the NCC analysis, an alternative to the stratified Cox-PH model is to use the conditional logistic regression. The HR and SE from both models should be similar under the proportional hazards assumption.\n\n\n(Liu et al. 2010)\n\nfit.hdps1 &lt;- clogit(Fail ~ anyDMD + strata(Set), \n                    weights = w, \n                    data = hdps.data, \n                    method = \"efron\")\npublish(fit.hdps1, pvalue.method = \"robust\", confint.method = \"robust\", \n        print = F)$regressionTable[1:2,]\n\n\n  \n\n\n\n\n\n\n\nAustin, Peter C, Geoffrey M Anderson, Candemir Cigsar, and Andrea Gruneir. 2012. “Comparing the Cohort Design and the Nested Case–Control Design in the Presence of Both Time-Invariant and Time-Dependent Treatment and Competing Risks: Bias and Precision.” Pharmacoepidemiology and Drug Safety 21 (7): 714–24.\n\n\nErnster, Virginia L. 1994. “Nested Case-Control Studies.” Preventive Medicine 23 (5): 587–90.\n\n\nHossain, Md Belal, Huah Shin Ng, Feng Zhu, Helen Tremlett, and Mohammad Ehsanul Karim. 2025. “Simultaneously Dealing with Immortal Time Bias and Residual Confounding: A Case Study of a High-Dimensional Propensity Score Approach with a Nested Case–Control Framework in Multiple Sclerosis Research.” Pharmacoepidemiology and Drug Safety 34 (7): e70174.\n\n\nHossain, Md Belal, Hubert Wong, Mohsen Sadatsafavi, James C Johnston, Victoria J Cook, and Mohammad Ehsanul Karim. 2024. “Benefits of Repeated Matched-Cohort and Nested Case–Control Analyses with Time-Dependent Exposure in Observational Studies.” Statistics in Biosciences, 1–29.\n\n\nLiu, Mengling, Wenbin Lu, Roy E Shore, and Anne Zeleniuch-Jacquotte. 2010. “Cox Regression Model with Time-Varying Coefficients in Nested Case–Control Studies.” Biostatistics 11 (4): 693–706.",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Time-dependent exposure</span>"
    ]
  },
  {
    "objectID": "prediction.html",
    "href": "prediction.html",
    "title": "26  High-Dimensional Prediction Models",
    "section": "",
    "text": "Recent article proposed and evaluated high-dimensional prediction models (hdPMs) using linked health administrative data to predict long-term mortality risk (Hossain et al. 2025). Their key objective was to assess whether hdPMs could compensate for the absence of important clinical predictors (e.g., age, smoking, BMI) by using a large number of routinely collected health-care variables (e.g., ICD-9/10 codes).\n\n\n\n\n\n\nTip\n\n\n\n(Hossain et al. 2025)\n\n\n\n\n\n\n\n\n\n\n\nBased on simulations, their findings showed that Cox-LASSO hdPMs consistently outperformed conventional models in both discrimination (time-dependent c-statistic) and calibration, especially when strong clinical predictors were missing. For example, the c-statistic improved from 0.78 (conventional model) to 0.90 (LASSO-based hdPM) in simulations.\n\n\n\n\n\n\n\n\nFeature\nhdPM\nhdPS / hdDRS\n\n\n\n\nGoal\nRisk prediction (e.g., mortality stratification)\nConfounding adjustment in causal inference\n\n\nTarget\nOutcome model (e.g., Cox model for time to death)\nExposure model (PS) or outcome model (DRS)\n\n\nUse of empirical vars\nYes, extensive ICD-9/10 code-based variables\nYes, but fewer (typically 500 top-ranking)\n\n\nShrinkage used\nRegularization (LASSO) crucial for performance\nOften none; or simple score summaries\n\n\nInterpretability\nLess interpretable, not designed for clinical use\nOften more interpretable in PS/DRS context\n\n\nMain application\nStratification, risk targeting at population level\nAdjustment in comparative effectiveness studies\n\n\n\nThis study shows that hdPMs are promising tools for population-level risk prediction, especially when clinical data are sparse. While hdPS/hdDRS target confounding control in causal inference, hdPMs aim to optimize outcome prediction, even when important variables are missing. The use of LASSO-based regularization is a key differentiator that enables hdPMs to avoid overfitting in high-dimensional spaces.\n\n\n\n\nHossain, Md Belal, Mohsen Sadatsafavi, Hubert Wong, Victoria J Cook, James C Johnston, and Mohammad Ehsanul Karim. 2025. “Enhancing Risk Prediction Base on Health Administrative Data Using High-Dimensional Prediction Model.” Journal of Clinical Epidemiology, 111857.",
    "crumbs": [
      "Extensions in Survival and Longitudinal Analyses",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>High-Dimensional Prediction Models</span>"
    ]
  },
  {
    "objectID": "guideline.html",
    "href": "guideline.html",
    "title": "Guideline",
    "section": "",
    "text": "Reviews and Guidelines",
    "crumbs": [
      "Guideline"
    ]
  },
  {
    "objectID": "guideline.html#reviews-and-guidelines",
    "href": "guideline.html#reviews-and-guidelines",
    "title": "Guideline",
    "section": "",
    "text": "flowchart LR\n  r[Reviews] --&gt; p1(Wyss et al. 2022&lt;br&gt;Pharmacoepidemiol Drug Saf.)\n  r --&gt; p0(Schneeweiss et al. 2018&lt;br&gt;Clin Epidemiol.)\n\n  g[Guideline] --&gt; p2(Rassen et al. 2022&lt;br&gt;Pharmacoepidemiol Drug Saf.)\n  g --&gt; p3(Tazare et al. 2022&lt;br&gt;Pharmacoepidemiol Drug Saf.)\n\n  %% Define style classes\n  classDef redNode fill:#f44,stroke-width:2px,stroke:#f00,color:#fff\n  classDef maroonNode fill:#b03,stroke-width:2px,stroke:#600,color:#fff\n\n  %% Apply classes to nodes\n  class p0,p1 redNode\n  class p2,p3 maroonNode\n\n\n\n\n\n\n\n\n\n\n\n(Wyss et al. 2022; Rassen et al. 2023; Tazare et al. 2022; Schneeweiss 2018)\n\n\nRassen, Jeremy A, Patrick Blin, Sebastian Kloss, Romain S Neugebauer, Robert W Platt, Anton Pottegård, Sebastian Schneeweiss, and Sengwee Toh. 2023. “High-Dimensional Propensity Scores for Empirical Covariate Selection in Secondary Database Studies: Planning, Implementation, and Reporting.” Pharmacoepidemiology and Drug Safety 32 (2): 93–106.\n\n\nSchneeweiss, Sebastian. 2018. “Automated Data-Adaptive Analytics for Electronic Healthcare Data to Study Causal Treatment Effects.” Clinical Epidemiology, 771–88.\n\n\nTazare, John, Richard Wyss, Jessica M Franklin, Liam Smeeth, Stephen JW Evans, Shirley V Wang, Sebastian Schneeweiss, Ian J Douglas, Joshua J Gagne, and Elizabeth J Williamson. 2022. “Transparency of High-Dimensional Propensity Score Analyses: Guidance for Diagnostics and Reporting.” Pharmacoepidemiology and Drug Safety 31 (4): 411–23.\n\n\nWyss, Richard, Chen Yanover, Tal El-Hay, Dimitri Bennett, Robert W Platt, Andrew R Zullo, Grammati Sari, et al. 2022. “Machine Learning for Improving High-Dimensional Proxy Confounder Adjustment in Healthcare Database Studies: An Overview of the Current Literature.” Pharmacoepidemiology and Drug Safety 31 (9): 932–43.",
    "crumbs": [
      "Guideline"
    ]
  },
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "27  Reporting",
    "section": "",
    "text": "27.1 Analysis Information\nMany reporting guideline already exists about what to report in a propensity score analysis. Most of the reporting guideline should be applicable in the hdPS context as well. On top of those, we also need to consider reporting the following information about hdPS for transparency.",
    "crumbs": [
      "Guideline",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Reporting</span>"
    ]
  },
  {
    "objectID": "report.html#analysis-information",
    "href": "report.html#analysis-information",
    "title": "27  Reporting",
    "section": "",
    "text": "(Karim et al. 2022; Simoneau et al. 2022)",
    "crumbs": [
      "Guideline",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Reporting</span>"
    ]
  },
  {
    "objectID": "report.html#information-about-hdps",
    "href": "report.html#information-about-hdps",
    "title": "27  Reporting",
    "section": "27.2 Information about hdPS",
    "text": "27.2 Information about hdPS\n\n\n\n\nInformation\nDescription\nOur example\n\n\n\n\nProxy data dimensions\nThe number of data dimensions (p) used.\np = 1 as only one proxy data dimension (dx) was available from medication usage\n\n\nWhat was done to remove proxies that are problematic\nUsually proxies of outcome, exposure as well as those identified as IV or mediator or collider are discarded.\nobesity and diabetes related codes removed\n\n\nProxy feature parameters\nThe parameters used to select proxy features, including granularity [g], prevalence filter [n], and the minimum number of patients [m]\ng = 3, n = 200, m = 20. This resulted in 126 empirical covariates.\n\n\nRecurrence parameters\nHow many recurrence variables per code [r] and the covariate assessment period [CAP]\nr = 3, CAP = 30 days. This resulted in 143 distinct recurrence covariates.\n\n\nPrioritization process\nThe process used to prioritize proxy features, such as machine learning (ML), Bross, or hybrid methods\nWe used all of these, but used Bross formula for hdPS to calculate absolute log of the multiplicative bias, and then ranked based on magnitude to select / prioritize recurrence covariates.\n\n\nSelected proxies\nThe number of proxies selected (k) for the model\nk = 100 for the hdPS\n\n\nSoftware\nThe software used to perform the analysis\nR: autoCovariateSelection package\n\n\n\n\n\n(Rassen et al. 2023; Tazare et al. 2022)",
    "crumbs": [
      "Guideline",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Reporting</span>"
    ]
  },
  {
    "objectID": "report.html#diagnostics",
    "href": "report.html#diagnostics",
    "title": "27  Reporting",
    "section": "27.3 Diagnostics",
    "text": "27.3 Diagnostics\n\n\n\nInformation\nDescription\nOur example\n\n\n\n\nDiagnostics used to assess the model\nStandardized mean differences (SMD)\nWithin 0.1 in hdPS analysis\n\n\n\nWeight (IPW) summary assessment\nSomewhat reasonable (maximum approximately 54) within hdPS analysis\n\n\n\nComparison of propensity score distributions between each exposure group\nOverlapping (common support) does not seem to be an issue.\n\n\n\nAssess distribution of absolute log bias\nMost bias multiplier values are close to null (0), only a few values seem to deviate from null.\n\n\n\nComparison with regular propensity score\nEstimates slightly towards null",
    "crumbs": [
      "Guideline",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Reporting</span>"
    ]
  },
  {
    "objectID": "report.html#sensitivity-analysis",
    "href": "report.html#sensitivity-analysis",
    "title": "27  Reporting",
    "section": "27.4 Sensitivity analysis",
    "text": "27.4 Sensitivity analysis\n\n\n\nInformation\nDescription\nOur example\n\n\n\n\nSensitivity analysis\nVarying the number of selected proxies [k].\nOR estimates stabilizes around 1.5, shows variability below k = 50 and above 110\n\n\nSensitivity analysis\nVarying the prevalence filter [n].\nOR estimates stabilizes around 1.5 for above n = 60.\n\n\n\n\n\n\n\nKarim, Mohammad Ehsanul, Fabio Pellegrini, Robert W Platt, Gabrielle Simoneau, Julie Rouette, and Carl de Moor. 2022. “The Use and Quality of Reporting of Propensity Score Methods in Multiple Sclerosis Literature: A Review.” Multiple Sclerosis Journal 28 (9): 1317–23.\n\n\nRassen, Jeremy A, Patrick Blin, Sebastian Kloss, Romain S Neugebauer, Robert W Platt, Anton Pottegård, Sebastian Schneeweiss, and Sengwee Toh. 2023. “High-Dimensional Propensity Scores for Empirical Covariate Selection in Secondary Database Studies: Planning, Implementation, and Reporting.” Pharmacoepidemiology and Drug Safety 32 (2): 93–106.\n\n\nSimoneau, Gabrielle, Fabio Pellegrini, Thomas PA Debray, Julie Rouette, Johanna Muñoz, Robert W Platt, John Petkau, et al. 2022. “Recommendations for the Use of Propensity Score Methods in Multiple Sclerosis Research.” Multiple Sclerosis Journal 28 (9): 1467–80.\n\n\nTazare, John, Richard Wyss, Jessica M Franklin, Liam Smeeth, Stephen JW Evans, Shirley V Wang, Sebastian Schneeweiss, Ian J Douglas, Joshua J Gagne, and Elizabeth J Williamson. 2022. “Transparency of High-Dimensional Propensity Score Analyses: Guidance for Diagnostics and Reporting.” Pharmacoepidemiology and Drug Safety 31 (4): 411–23.",
    "crumbs": [
      "Guideline",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Reporting</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Austin, Peter C, Geoffrey M Anderson, Candemir Cigsar, and Andrea\nGruneir. 2012. “Comparing the Cohort Design and the Nested\nCase–Control Design in the Presence of Both Time-Invariant and\nTime-Dependent Treatment and Competing Risks: Bias and\nPrecision.” Pharmacoepidemiology and Drug Safety 21 (7):\n714–24.\n\n\nBalzer, Laura B, and Ted Westling. 2021. “Demystifying Statistical\nInference When Using Machine Learning in Causal Research.”\nAmerican Journal of Epidemiology.\n\n\nBenasseur, Imane, Denis Talbot, Madeleine Durand, Anne Holbrook, Alexis\nMatteau, Brian J Potter, Christel Renoux, Mireille E Schnitzer,\nJean-Éric Tarride, and Jason R Guertin. 2022. “A Comparison of\nConfounder Selection and Adjustment Methods for Estimating Causal\nEffects Using Large Healthcare Databases.”\nPharmacoepidemiology and Drug Safety 31 (4): 424–33.\n\n\nBeyersmann, Jan, Martin Wolkewitz, and Martin Schumacher. 2008.\n“The Impact of Time-Dependent Bias in Proportional Hazards\nModelling.” Statistics in Medicine 27 (30): 6439–54.\n\n\nBrookhart, M Alan, Sebastian Schneeweiss, Kenneth J Rothman, Robert J\nGlynn, Jerry Avorn, and Til Stürmer. 2006. “Variable Selection for\nPropensity Score Models.” American Journal of\nEpidemiology 163 (12): 1149–56.\n\n\nBross, Irwin DJ. 1966. “Spurious Effects from an Extraneous\nVariable.” Journal of Chronic Diseases 19 (6): 637–47.\n\n\nCharlson, Mary E, Peter Pompei, Kathy L Ales, and C Ronald MacKenzie.\n1987. “A New Method of Classifying Prognostic Comorbidity in\nLongitudinal Studies: Development and Validation.” Journal of\nChronic Diseases 40 (5): 373–83.\n\n\nChoi, BCK, and F Shi. 2001. “Risk Factors for Diabetes Mellitus by\nAge and Sex: Results of the National Population Health Survey.”\nDiabetologia 44: 1221–31.\n\n\nConnolly, John G, Sebastian Schneeweiss, Robert J Glynn, and Joshua J\nGagne. 2019. “Quantifying Bias Reduction with Fixed-Duration\nVersus All-Available Covariate Assessment Periods.”\nPharmacoepidemiology and Drug Safety 28 (5): 665–70.\n\n\nDisease Control, Centers for, and Prevention. 2021. “National\nHealth and Nutrition Examination Survey (NHANES).” National\nCenter for Health Statistics.\n\n\nElixhauser, Anne, Claudia Steiner, D Robert Harris, and Rosanna M\nCoffey. 1998. “Comorbidity Measures for Use with Administrative\nData.” Medical Care, 8–27.\n\n\nErnster, Virginia L. 1994. “Nested Case-Control Studies.”\nPreventive Medicine 23 (5): 587–90.\n\n\nFranklin, Jessica M, Wesley Eddings, Robert J Glynn, and Sebastian\nSchneeweiss. 2015. “Regularized Regression Versus the\nHigh-Dimensional Propensity Score for Confounding Adjustment in\nSecondary Database Analyses.” American Journal of\nEpidemiology 182 (7): 651–59.\n\n\nGreenland, Sander, Judea Pearl, and James M Robins. 1999. “Causal\nDiagrams for Epidemiologic Research.” Epidemiology,\n37–48.\n\n\nHansen, Ben B. 2008. “The Prognostic Analogue of the Propensity\nScore.” Biometrika 95 (2): 481–88.\n\n\nHernán, Miguel A, and Sarah L Taubman. 2008. “Does Obesity Shorten\nLife? The Importance of Well-Defined Interventions to Answer Causal\nQuestions.” International Journal of Obesity 32 (3):\nS8–14.\n\n\nHossain, Md Belal. 2025. “Chapter 2: High-Dimensional Disease Risk\nScore for Dealing with Residual Confounding in Estimating Treatment\nEffects with a Survival Outcome.” In. Harnessing the power of\ncausal inference; predictive analytics for survival outcomes with health\nadministrative data: applications to tuberculosis research.\n\n\nHossain, Md Belal, Huah Shin Ng, Feng Zhu, Helen Tremlett, and Mohammad\nEhsanul Karim. 2025. “Simultaneously Dealing with Immortal Time\nBias and Residual Confounding: A Case Study of a High-Dimensional\nPropensity Score Approach with a Nested Case–Control Framework in\nMultiple Sclerosis Research.” Pharmacoepidemiology and Drug\nSafety 34 (7): e70174.\n\n\nHossain, Md Belal, Mohsen Sadatsafavi, Hubert Wong, Victoria J Cook,\nJames C Johnston, and Mohammad Ehsanul Karim. 2025. “Enhancing\nRisk Prediction Base on Health Administrative Data Using\nHigh-Dimensional Prediction Model.” Journal of Clinical\nEpidemiology, 111857.\n\n\nHossain, Md Belal, Hubert Wong, Mohsen Sadatsafavi, James C Johnston,\nVictoria J Cook, and Mohammad Ehsanul Karim. 2024. “Benefits of\nRepeated Matched-Cohort and Nested Case–Control Analyses with\nTime-Dependent Exposure in Observational Studies.” Statistics\nin Biosciences, 1–29.\n\n\nJones, Mark, and Robert Fowler. 2016. “Immortal Time Bias in\nObservational Studies of Time-to-Event Outcomes.” Journal of\nCritical Care 36: 195–99.\n\n\nJu, Cheng, Mary Combs, Samuel D Lendle, Jessica M Franklin, Richard\nWyss, Sebastian Schneeweiss, and Mark J van der Laan. 2019.\n“Propensity Score Prediction for Electronic Healthcare Databases\nUsing Super Learner and High-Dimensional Propensity Score\nMethods.” Journal of Applied Statistics 46 (12):\n2216–36.\n\n\nJu, Cheng, Susan Gruber, Samuel D Lendle, Antoine Chambaz, Jessica M\nFranklin, Richard Wyss, Sebastian Schneeweiss, and Mark J van Der Laan.\n2019. “Scalable Collaborative Targeted Learning for\nHigh-Dimensional Data.” Statistical Methods in Medical\nResearch 28 (2): 532–54.\n\n\nKarim, ME. 2023. “Rethinking Residual Confounding Bias Reduction:\nWhy Vanilla hdPS Alone Is No Longer Enough.”\n\n\nKarim, ME, and Y Lei. 2025. “Is There a Competitive Advantage to\nUsing Multivariate Statistical or Machine Learning Methods over the\nBross Formula in the hdPS Framework for Bias and Variance\nEstimation?” PLoS One 20 (5): e0324639.\n\n\nKarim, ME, and MH Mondol. 2025. “Finding the Optimal Number of\nSplits and Repetitions in Double Cross-Fitting Targeted Maximum\nLikelihood Estimators.” Pharmaceutical Statistics.\n\n\nKarim, Mohammad Ehsanul, Paul Gustafson, John Petkau, Yinshan Zhao,\nAfsaneh Shirani, Elaine Kingwell, Charity Evans, Mia Van Der Kop, Joel\nOger, and Helen Tremlett. 2014. “Marginal Structural Cox Models\nfor Estimating the Association Between β-Interferon Exposure and Disease\nProgression in a Multiple Sclerosis Cohort.” American Journal\nof Epidemiology 180 (2): 160–71.\n\n\nKarim, Mohammad Ehsanul, Md Belal Hossain, Huah Shin Ng, Feng Zhu, Hanna\nA Frank, and Helen Tremlett. 2025. “Evaluating the Role of\nHigh-Dimensional Proxy Data in Confounding Adjustment in Multiple\nSclerosis Research: A Case Study.” Pharmacoepidemiology and\nDrug Safety 34 (2): e70112.\n\n\nKarim, Mohammad Ehsanul, and Yang Lei. 2025. “How Effective Are\nMachine Learning and Doubly Robust Estimators in Incorporating\nHigh-Dimensional Proxies to Reduce Residual Confounding?”\nPharmacoepidemiology and Drug Safety 34 (5): e70155.\n\n\nKarim, Mohammad Ehsanul, Menglan Pang, and Robert W Platt. 2018.\n“Can We Train Machine Learning Methods to Outperform the\nHigh-Dimensional Propensity Score Algorithm?”\nEpidemiology 29 (2): 191–98.\n\n\nKarim, Mohammad Ehsanul, Fabio Pellegrini, Robert W Platt, Gabrielle\nSimoneau, Julie Rouette, and Carl de Moor. 2022. “The Use and\nQuality of Reporting of Propensity Score Methods in Multiple Sclerosis\nLiterature: A Review.” Multiple Sclerosis Journal 28\n(9): 1317–23.\n\n\nKarim, Mohammad Ehsanul, and Zining Annie Wang. 2025. “Are Neural\nRepresentation Learning Methods a Viable Alternative to TMLE for Causal\nEstimation?” Data Science in Science 4 (1): 2583507.\n\n\nKlein, Samuel, Amalia Gastaldelli, Hannele Yki-Järvinen, and Philipp E\nScherer. 2022. “Why Does Obesity Cause Diabetes?” Cell\nMetabolism 34 (1): 11–20.\n\n\nKumamaru, Hiraku, Joshua J Gagne, Robert J Glynn, Soko Setoguchi, and\nSebastian Schneeweiss. 2016. “Comparison of High-Dimensional\nConfounder Summary Scores in Comparative Studies of Newly Marketed\nMedications.” Journal of Clinical Epidemiology 76:\n200–208.\n\n\nKumamaru, Hiraku, Sebastian Schneeweiss, Robert J Glynn, Soko Setoguchi,\nand Joshua J Gagne. 2016. “Dimension Reduction and Shrinkage\nMethods for High Dimensional Disease Risk Scores in Historical\nData.” Emerging Themes in Epidemiology 13: 1–10.\n\n\nLiu, Mengling, Wenbin Lu, Roy E Shore, and Anne Zeleniuch-Jacquotte.\n2010. “Cox Regression Model with Time-Varying Coefficients in\nNested Case–Control Studies.” Biostatistics 11 (4):\n693–706.\n\n\nLix, Lisa M, Jacqueline Quail, Opeyemi Fadahunsi, and Gary F Teare.\n2013. “Predictive Performance of Comorbidity Measures in\nAdministrative Databases for Diabetes Cohorts.” BMC Health\nServices Research 13: 1–12.\n\n\nLix, LM, J Quail, G Teare, and B Acan. 2011. “Performance of\nComorbidity Measures for Predicting Outcomes in Population-Based\nOsteoporosis Cohorts.” Osteoporosis International 22:\n2633–43.\n\n\nLow, Yen Sia, Blanca Gallego, and Nigam Haresh Shah. 2016.\n“Comparing High-Dimensional Confounder Control Methods for Rapid\nCohort Studies from Electronic Health Records.” Journal of\nComparative Effectiveness Research 5 (2): 179–92.\n\n\nMondol, MH, and ME Karim. 2024. “Towards Robust Causal Inference\nin Epidemiological Research: Employing Double Cross-Fit TMLE in Right\nHeart Catheterization Data.” American Journal of\nEpidemiology, kwae447.\n\n\nNaimi, Ashley I, and Brian W Whitcomb. 2020. “Estimating Risk\nRatios and Risk Differences Using Regression.” American\nJournal of Epidemiology 189 (6): 508–10.\n\n\nNeugebauer, Romain, Julie A Schmittdiel, Zheng Zhu, Jeremy A Rassen,\nJohn D Seeger, and Sebastian Schneeweiss. 2015. “High-Dimensional\nPropensity Score Algorithm in Comparative Effectiveness Research with\nTime-Varying Interventions.” Statistics in Medicine 34\n(5): 753–81.\n\n\nNguyen, Tri-Long, Thomas PA Debray, Bora Youn, Gabrielle Simoneau, and\nGary S Collins. 2024. “Confounder Adjustment Using the Disease\nRisk Score: A Proposal for Weighting Methods.” American\nJournal of Epidemiology 193 (2): 377–88.\n\n\nPang, Menglan, Tibor Schuster, Kristian B Filion, Maria Eberg, and\nRobert W Platt. 2016. “Targeted Maximum Likelihood Estimation for\nPharmacoepidemiologic Research.” Epidemiology (Cambridge,\nMass.) 27 (4): 570.\n\n\nPang, Menglan, Tibor Schuster, Kristian B Filion, Mireille E Schnitzer,\nMaria Eberg, and Robert W Platt. 2016. “Effect Estimation in\nPoint-Exposure Studies with Binary Outcomes and High-Dimensional\nCovariate Data–a Comparison of Targeted Maximum Likelihood Estimation\nand Inverse Probability of Treatment Weighting.” The\nInternational Journal of Biostatistics 12 (2).\n\n\nRassen, Jeremy A, Patrick Blin, Sebastian Kloss, Romain S Neugebauer,\nRobert W Platt, Anton Pottegård, Sebastian Schneeweiss, and Sengwee Toh.\n2023. “High-Dimensional Propensity Scores for Empirical Covariate\nSelection in Secondary Database Studies: Planning, Implementation, and\nReporting.” Pharmacoepidemiology and Drug Safety 32 (2):\n93–106.\n\n\nRobert, Dennis. 2020. autoCovariateSelection: Automatic Covariate\nSelection. https://CRAN.R-project.org/package=autoCovariateSelection.\n\n\nRubin, Donald B. 1997. “Estimating Causal Effects from Large Data\nSets Using Propensity Scores.” Annals of Internal\nMedicine 127 (8_Part_2): 757–63.\n\n\nRubin, Donald B, and Neal Thomas. 1996. “Matching Using Estimated\nPropensity Scores: Relating Theory to Practice.”\nBiometrics, 249–64.\n\n\nSchneeweiss, Sebastian. 2006. “Sensitivity Analysis and External\nAdjustment for Unmeasured Confounders in Epidemiologic Database Studies\nof Therapeutics.” Pharmacoepidemiology and Drug Safety\n15 (5): 291–303.\n\n\n———. 2018. “Automated Data-Adaptive Analytics for Electronic\nHealthcare Data to Study Causal Treatment Effects.” Clinical\nEpidemiology, 771–88.\n\n\nSchneeweiss, Sebastian, Wesley Eddings, Robert J Glynn, Elisabetta\nPatorno, Jeremy Rassen, and Jessica M Franklin. 2017. “Variable\nSelection for Confounding Adjustment in High-Dimensional Covariate\nSpaces When Analyzing Healthcare Databases.”\nEpidemiology 28 (2): 237–48.\n\n\nSchneeweiss, Sebastian, and Malcolm Maclure. 2000. “Use of\nComorbidity Scores for Control of Confounding in Studies Using\nAdministrative Databases.” International Journal of\nEpidemiology 29 (5): 891–98.\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn,\nHelen Mogun, and M Alan Brookhart. 2009. “High-Dimensional\nPropensity Score Adjustment in Studies of Treatment Effects Using Health\nCare Claims Data.” Epidemiology (Cambridge, Mass.) 20\n(4): 512.\n\n\nSchuster, Tibor, Wilfrid Kouokam Lowe, and Robert W Platt. 2016.\n“Propensity Score Model Overfitting Led to Inflated Variance of\nEstimated Odds Ratios.” Journal of Clinical Epidemiology\n80: 97–106.\n\n\nSchuster, Tibor, Menglan Pang, and Robert W Platt. 2015. “On the\nRole of Marginal Confounder Prevalence–Implications for the\nHigh-Dimensional Propensity Score Algorithm.”\nPharmacoepidemiology and Drug Safety 24 (9): 1004–7.\n\n\nShalit, Uri, Fredrik D Johansson, and David Sontag. 2017.\n“Estimating Individual Treatment Effect: Generalization Bounds and\nAlgorithms.” In International Conference on Machine\nLearning, 3076–85. PMLR.\n\n\nShi, Claudia, David Blei, and Victor Veitch. 2019. “Adapting\nNeural Networks for the Estimation of Treatment Effects.”\nAdvances in Neural Information Processing Systems 32.\n\n\nSimoneau, Gabrielle, Fabio Pellegrini, Thomas PA Debray, Julie Rouette,\nJohanna Muñoz, Robert W Platt, John Petkau, et al. 2022.\n“Recommendations for the Use of Propensity Score Methods in\nMultiple Sclerosis Research.” Multiple Sclerosis Journal\n28 (9): 1467–80.\n\n\nStuart, Elizabeth A, Brian K Lee, and Finbarr P Leacy. 2013.\n“Prognostic Score–Based Balance Measures Can Be a Useful\nDiagnostic for Propensity Score Methods in Comparative Effectiveness\nResearch.” Journal of Clinical Epidemiology 66 (8):\nS84–90.\n\n\nTazare, John, Richard Wyss, Jessica M Franklin, Liam Smeeth, Stephen JW\nEvans, Shirley V Wang, Sebastian Schneeweiss, Ian J Douglas, Joshua J\nGagne, and Elizabeth J Williamson. 2022. “Transparency of\nHigh-Dimensional Propensity Score Analyses: Guidance for Diagnostics and\nReporting.” Pharmacoepidemiology and Drug Safety 31 (4):\n411–23.\n\n\nTian, Yuxi, Martijn J Schuemie, and Marc A Suchard. 2018.\n“Evaluating Large-Scale Propensity Score Performance Through\nReal-World and Synthetic Data Experiments.” International\nJournal of Epidemiology 47 (6): 2005–14.\n\n\nVanderWeele, Tyler J. 2019. “Principles of Confounder\nSelection.” European Journal of Epidemiology 34: 211–19.\n\n\nVon Korff, Michael, Edward H Wagner, and Kathleen Saunders. 1992.\n“A Chronic Disease Score from Automated Pharmacy Data.”\nJournal of Clinical Epidemiology 45 (2): 197–203.\n\n\nWeberpals, Janick, Tim Becker, Jessica Davies, Fabian Schmich, Dominik\nRüttinger, Fabian J Theis, and Anna Bauer-Mehren. 2021. “Deep\nLearning-Based Propensity Scores for Confounding Control in Comparative\nEffectiveness Research: A Large-Scale, Real-World Data Study.”\nEpidemiology 32 (3): 378–88.\n\n\nWestreich, Daniel, Stephen R Cole, Michele Jonsson Funk, M Alan\nBrookhart, and Til Stürmer. 2011. “The Role of the c-Statistic in\nVariable Selection for Propensity Score Models.”\nPharmacoepidemiology and Drug Safety 20 (3): 317–20.\n\n\nWyss, Richard, Alan R Ellis, M Alan Brookhart, Michele Jonsson Funk,\nCynthia J Girman, Ross J Simpson Jr, and Til Stürmer. 2015.\n“Matching on the Disease Risk Score in Comparative Effectiveness\nResearch of New Treatments.” Pharmacoepidemiology and Drug\nSafety 24 (9): 951–61.\n\n\nWyss, Richard, Sebastian Schneeweiss, Mark Van Der Laan, Samuel D\nLendle, Cheng Ju, and Jessica M Franklin. 2018. “Using Super\nLearner Prediction Modeling to Improve High-Dimensional Propensity Score\nEstimation.” Epidemiology 29 (1): 96–106.\n\n\nWyss, Richard, Chen Yanover, Tal El-Hay, Dimitri Bennett, Robert W\nPlatt, Andrew R Zullo, Grammati Sari, et al. 2022. “Machine\nLearning for Improving High-Dimensional Proxy Confounder Adjustment in\nHealthcare Database Studies: An Overview of the Current\nLiterature.” Pharmacoepidemiology and Drug Safety 31\n(9): 932–43.\n\n\nZhang, Di, and Jessica Kim. 2019. “Use of Propensity Score and\nDisease Risk Score for Multiple Treatments with Time-to-Event Outcome: A\nSimulation Study.” Journal of Biopharmaceutical\nStatistics 29 (6): 1103–15.\n\n\nZivich, Paul N, and Alexander Breskin. 2021. “Machine Learning for\nCausal Inference: On the Use of Cross-Fit Estimators.”\nEpidemiology (Cambridge, Mass.) 32 (3): 393.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "NHANES.html",
    "href": "NHANES.html",
    "title": "Appendix: NHANES",
    "section": "",
    "text": "NHANES\nThe National Health and Nutrition Examination Survey (NHANES) is a cross-sectional survey that is designed to provide nationally representative data on the health and nutritional status of the non-institutionalized, civilian US population. This survey is conducted by the National Center for Health Statistics, that provides valuable data on a wide range of health issues, such as diabetes, obesity, as well as nutrition, physical activity, and environmental exposures.",
    "crumbs": [
      "Appendix: NHANES"
    ]
  },
  {
    "objectID": "NHANES.html#components",
    "href": "NHANES.html#components",
    "title": "Appendix: NHANES",
    "section": "Components",
    "text": "Components\nNHANES combines interviews, physical examinations, and laboratory tests to gather comprehensive health and nutrition information about participants.",
    "crumbs": [
      "Appendix: NHANES"
    ]
  },
  {
    "objectID": "NHANES.html#design",
    "href": "NHANES.html#design",
    "title": "Appendix: NHANES",
    "section": "Design",
    "text": "Design\nThe survey selects participants using a complex sampling design, which allows researchers to make inferences about the overall health of the US population. The survey uses a complex, multistage probability sampling design to select participants from US households.",
    "crumbs": [
      "Appendix: NHANES"
    ]
  },
  {
    "objectID": "NHANES.html#usefulness",
    "href": "NHANES.html#usefulness",
    "title": "Appendix: NHANES",
    "section": "Usefulness",
    "text": "Usefulness\nThe collected data is used by many researchers, policymakers, and public health officials to identify emerging health issues, monitor trends in health and inform public health policies and initiatives to improve health-related outcomes in the US.",
    "crumbs": [
      "Appendix: NHANES"
    ]
  },
  {
    "objectID": "NHANES.html#cylces-we-used",
    "href": "NHANES.html#cylces-we-used",
    "title": "Appendix: NHANES",
    "section": "Cylces we used",
    "text": "Cylces we used\nNHANES is an ongoing annual survey, continuous NHANES cycles started from 1999-2000 (cycle 1; see SDDSRVYR variable in the Demographic Variables & Sample Weights component). NHANES cycles 8, 9, and 10 (that we used in this tutorial) refer to the 8th, 9th, and 10th rounds of surveys, which took place from 2013-2014, 2015-2016, and 2017-2018, respectively. Each cycle involves a nationally representative sample of the US population.",
    "crumbs": [
      "Appendix: NHANES"
    ]
  },
  {
    "objectID": "index13.html",
    "href": "index13.html",
    "title": "28  Download cycle 8",
    "section": "",
    "text": "28.1 Download and Subsetting to retain only the useful variables\nDownloading NHANES 2013-14 cycle components",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Download cycle 8</span>"
    ]
  },
  {
    "objectID": "index13.html#download-and-subsetting-to-retain-only-the-useful-variables",
    "href": "index13.html#download-and-subsetting-to-retain-only-the-useful-variables",
    "title": "28  Download cycle 8",
    "section": "",
    "text": "28.1.1 Demographic\nDemographic Variables and Sample Weights (DEMO_H): The 2-year sample weights (WTINT2YR, WTMEC2YR) should be used. 15 masked variance strata and 30 masked primary sampling units (PSUs) are included in the demographics file. Each stratum has 2 PSUs.\n\ndemo &lt;- nhanes('DEMO_H')    # Both males and females 0 YEARS - 150 YEARS\ndemo1 &lt;- demo[c(\"SEQN\",     # Respondent sequence number\n                \"RIDAGEYR\", # Age in years at screening\n                \"RIAGENDR\", # gender\n                \"DMDEDUC2\", # Education level - Adults 20+\n                \"RIDRETH1\", # race/ethnicity\n                \"DMDMARTL\", # marital status    \n                \"INDHHIN2\", # Annual household income\n                \"DMDBORN4\", # where born\n                \"RIDEXPRG\", # Pregnancy status at exam (released for 20-44 yrs)\n                \"SDDSRVYR\", # survey cycle\n                \"WTINT2YR\", # Full sample 2 year weights\n                \"WTMEC2YR\", # Full sample 2 year MEC exam weight\n                \"SDMVPSU\",  # Masked variance pseudo-PSU\n                \"SDMVSTRA\")]# Masked variance pseudo-stratum\ndemo_vars &lt;- names(demo1) \ndemo2 &lt;- nhanesTranslate('DEMO_H', demo_vars, data = demo1)\n#&gt; Translated columns: RIAGENDR DMDEDUC2 RIDRETH1 DMDMARTL INDHHIN2 DMDBORN4 RIDEXPRG SDDSRVYR\nsaveRDS(demo2, file = \"data/components/demo13.RData\")\n\n\n\n28.1.2 BMI\nBody Measures (BMX_H): The NHANES examination sample weights should be used to analyze the body measurement data. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\nbmx &lt;- nhanes('BMX_H')\nbmx1 &lt;- bmx[c(\"SEQN\", # Respondent sequence number\n              \"BMXBMI\")] # Body Mass Index (kg/m**2): 2 YEARS - 150 YEARS\nbmx_vars &lt;- names(bmx1)\nbmx2 &lt;- nhanesTranslate('BMX_H', bmx_vars, data = bmx1)\n#&gt; Warning in nhanesTranslate(\"BMX_H\", bmx_vars, data = bmx1): No columns were\n#&gt; translated\nsaveRDS(bmx2, file = \"data/components/bmx13.RData\")\n\n\n\n28.1.3 Diabetes\nDiabetes (DIQ_H): diabetes questionnaire data must be conducted using the appropriate survey design variables, sample weights, and the basic demographic variables. Interview weights should only be used if questionnaire data are analyzed by themselves. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\ndiq &lt;- nhanes('DIQ_H')\ndiq1 &lt;- diq[c(\"SEQN\", # Respondent sequence number\n              \"DIQ010\", # Doctor told you have diabetes\n              \"DIQ050\", # Taking insulin now\n              \"DIQ070\", # Take diabetic pills to lower blood sugar\n              \"DIQ175A\")] # Family history\ndiq_vars &lt;- names(diq1)\ndiq2 &lt;- nhanesTranslate('DIQ_H', diq_vars, data = diq1)\n#&gt; Translated columns: DIQ010 DIQ050 DIQ070 DIQ175A\nsaveRDS(diq2, file = \"data/components/diq13.RData\")\n\n\n\n28.1.4 Smoking\nSmoking - Cigarette Use (SMQ_H): Interview weights should only be used if questionnaire data are analyzed by themselves. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\nsmq &lt;- nhanes('SMQ_H')\nsmq1 &lt;- smq[c(\"SEQN\", # Respondent sequence number\n              \"SMQ020\", # Smoked at least 100 cigarettes in life\n              \"SMQ040\")] # Do you now smoke cigarettes?: 18 YEARS - 150 YEARS\nsmq_vars &lt;- names(smq1)\nsmq2 &lt;- nhanesTranslate('SMQ_H', smq_vars, data = smq1)\n#&gt; Translated columns: SMQ020 SMQ040\nsaveRDS(smq2, file = \"data/components/smq13.RData\")\n\n\n\n28.1.5 Diet\nDiet Behavior & Nutrition (DBQ_H): interview sample weights may be used in their analysis. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\ndbq &lt;- nhanes('DBQ_H')\ndbq1 &lt;- dbq[c(\"SEQN\", # Respondent sequence number\n              \"DBQ700\")] # How healthy is the diet: 16 YEARS - 150 YEARS\ndbq_vars &lt;- names(dbq1)\ndbq2 &lt;- nhanesTranslate('DBQ_H', dbq_vars, data = dbq1)\n#&gt; Translated columns: DBQ700\nsaveRDS(dbq2, file = \"data/components/dbq13.RData\")\n\n\n\n28.1.6 Physical activity\nPhysical Activity (PAQ_H): the interview sample weights should be used in their analysis. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\npaq &lt;- nhanes('PAQ_H')\npaq1 &lt;- paq[c(\"SEQN\", # Respondent sequence number\n              \"PAQ605\")] # Vigorous work activity: 18 YEARS150 YEARS\npaq_vars &lt;- names(paq1)\npaq2 &lt;- nhanesTranslate('PAQ_H', paq_vars, data = paq1)\n#&gt; Translated columns: PAQ605\nsaveRDS(paq2, file = \"data/components/paq13.RData\")\n\n\n\n28.1.7 Access to healthcare\nHospital Utilization & Access to Care (HUQ_H): Although these data were collected as part of the household questionnaire, if they are merged with the MEC exam data, exam sample weights should be used for the analyses.\n\nhuq &lt;- nhanes('HUQ_H')\nhuq1 &lt;- huq[c(\"SEQN\", # Respondent sequence number\n              \"HUQ030\")] # Routine place to go for healthcare\nhuq_vars &lt;- names(huq1)\nhuq2 &lt;- nhanesTranslate('HUQ_H', huq_vars, data = huq1)\n#&gt; Translated columns: HUQ030\nsaveRDS(huq2, file = \"data/components/huq13.RData\")\n\n\n\n28.1.8 Blood pressure\nBlood Pressure (BPX_H): Exam sample weights should be used for analyses.\n\nSystolic blood pressure and maximum inflation level cannot be greater than 300 mmHg;\nSystolic and diastolic blood pressure measurements and the maximum inflation level can be even numbers only;\nSystolic blood pressure must be greater than diastolic blood pressure;\nIf there is no systolic blood pressure, there can be no diastolic blood pressure. (There can be a systolic measurement without a diastolic measurement.); and\nDiastolic blood pressure can be zero.\n\n\nbpx &lt;- nhanes('BPX_H')\nbpx1 &lt;- bpx[c(\"SEQN\", # Respondent sequence number\n              \"BPXSY1\", # Systolic Blood pres (1st rdg) mmHg: 8 - 150 YEARS\n              \"BPXSY2\", # Systolic: Blood pres (2nd rdg) mm Hg\n              \"BPXSY3\", # Systolic: Blood pres (3rd rdg) mm Hg\n              \"BPXSY4\", # Systolic: Blood pres (4th rdg) mm Hg\n              \"BPXDI1\", # Diastolic Blood pres (1st rdg) mmHg: 8 - 150 YEARS\n              \"BPXDI2\", # Diastolic: Blood pres (2nd rdg) mm Hg\n              \"BPXDI3\", # Diastolic: Blood pres (3rd rdg) mm Hg\n              \"BPXDI4\")] # Diastolic: Blood pres (4th rdg) mm Hg\nbpx_vars &lt;- names(bpx1)\nbpx2 &lt;- nhanesTranslate('BPX_H', bpx_vars, data = bpx1)\n#&gt; Warning in nhanesTranslate(\"BPX_H\", bpx_vars, data = bpx1): No columns were\n#&gt; translated\nsaveRDS(bpx2, file = \"data/components/bpx13.RData\")\n\nBlood Pressure & Cholesterol (BPQ_H): Although these data were collected as part of the household questionnaire, if they are merged with the MEC exam data, exam sample weights should be used for the analyses.\n\nbpq &lt;- nhanes('BPQ_H')\nbpq1 &lt;- bpq[c(\"SEQN\", # Respondent sequence number\n              \"BPQ080\")] # high cholesterol\nbpq_vars &lt;- names(bpq1)\nbpq2 &lt;- nhanesTranslate('BPQ_H', bpq_vars, data = bpq1)\n#&gt; Translated columns: BPQ080\nsaveRDS(bpq2, file = \"data/components/bpq13.RData\")\n\n\n\n28.1.9 Sleep\nSleep Disorders (SLQ_H):\n\nslq &lt;- nhanes('SLQ_H')\nslq1 &lt;- slq[c(\"SEQN\", # Respondent sequence number\n              \"SLD010H\")] # Sleep hours \nslq_vars &lt;- names(slq1)\nslq2 &lt;- nhanesTranslate('SLQ_H', slq_vars, data = slq1)\n#&gt; Warning in nhanesTranslate(\"SLQ_H\", slq_vars, data = slq1): No columns were\n#&gt; translated\nsaveRDS(slq2, file = \"data/components/slq13.RData\")\n\n\n\n28.1.10 Laboratory data\nStandard Biochemistry Profile (BIOPRO_H): Exam sample weights should be used for analyses.\n\n# Standard Biochemistry Profile\nbiopro &lt;- nhanes('BIOPRO_H') # 12 YEARS - 150 YEARS\nbiopro1 &lt;- biopro[c(\"SEQN\", # Respondent sequence number\n                    #\"LBXSTR\", # Triglycerides, refrigerated (mg/dL)\n                    \"LBXSUA\", # Uric acid (mg/dL)\n                    \"LBXSTP\", # Total protein (g/dL)\n                    \"LBXSTB\", # Total bilirubin (mg/dL)\n                    \"LBXSPH\", # Phosphorus (mg/dL)\n                    \"LBXSNASI\", # Sodium (mmol/L)\n                    \"LBXSKSI\", # Potassium (mmol/L)\n                    \"LBXSGB\", # Globulin (g/dL)\n                    \"LBXSCA\")] # Total Calcium (mg/dL)\nbiopro_vars &lt;- names(biopro1) \nbiopro2 &lt;- nhanesTranslate('BIOPRO_H', biopro_vars, data = biopro1)\n#&gt; Warning in nhanesTranslate(\"BIOPRO_H\", biopro_vars, data = biopro1): No columns\n#&gt; were translated\nsaveRDS(biopro2, file = \"data/components/biopro13.RData\")\n\n\n\n28.1.11 ICD-10-CM codes\nPrescription Medications (RXQ_RX_H): The Prescription Medications subsection provides personal interview data on use of prescription medications during a one-month period prior to the participant’s interview date. During the household SP interview, survey participants are asked if they have taken medications in the past 30 days for which they needed a prescription. Those who answer “yes” are asked to show the interviewer the medication containers of all the products used.\n\nrxq &lt;- nhanes('RXQ_RX_H')\nrxq10 &lt;- rxq[c(\"SEQN\", # Respondent sequence number\n               \"RXDRSC1\")] # ICD-10-CM code 1\nrxq11 &lt;- names(rxq10) \nrxq12 &lt;- nhanesTranslate('RXQ_RX_H', rxq11, data = rxq10)\n#&gt; Translated columns: RXDRSC1\n\nrxq20 &lt;- rxq[c(\"SEQN\", # Respondent sequence number\n               \"RXDRSC2\")] # ICD-10-CM code 2\nrxq21 &lt;- names(rxq20) \nrxq22 &lt;- nhanesTranslate('RXQ_RX_H', rxq21, data = rxq20)\n#&gt; Translated columns: RXDRSC2\n\nrxq30 &lt;- rxq[c(\"SEQN\", # Respondent sequence number\n               \"RXDRSC3\")] # ICD-10-CM code 3\nrxq31 &lt;- names(rxq30) \nrxq32 &lt;- nhanesTranslate('RXQ_RX_H', rxq31, data = rxq30)\n#&gt; Translated columns: RXDRSC3\n\nsaveRDS(rxq12, file = \"data/components/rxq1213.RData\")\nsaveRDS(rxq22, file = \"data/components/rxq2213.RData\")\nsaveRDS(rxq32, file = \"data/components/rxq3213.RData\")",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Download cycle 8</span>"
    ]
  },
  {
    "objectID": "index13.html#merging-all-the-datasets---except-for-icd-10-codes",
    "href": "index13.html#merging-all-the-datasets---except-for-icd-10-codes",
    "title": "28  Download cycle 8",
    "section": "28.2 Merging all the datasets - except for ICD-10 codes",
    "text": "28.2 Merging all the datasets - except for ICD-10 codes\n\ndat &lt;- join_all(list(demo2, bmx2, diq2, smq2, dbq2, paq2, \n                     huq2, bpx2, bpq2, slq2, biopro2),\n                by = \"SEQN\", type='full')\nnhanes13 &lt;- dat\n\n\n28.2.1 Save dataset for later use\n\ndim(nhanes13)\n#&gt; [1] 10175    42\nsave(nhanes13, rxq12, rxq22, rxq32, file = \"data/analytic13.RData\")",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Download cycle 8</span>"
    ]
  },
  {
    "objectID": "index15.html",
    "href": "index15.html",
    "title": "29  Download cycle 9",
    "section": "",
    "text": "29.1 Download and Subsetting to retain only the useful variables\nDownloading NHANES 2015-16 cycle components",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Download cycle 9</span>"
    ]
  },
  {
    "objectID": "index15.html#download-and-subsetting-to-retain-only-the-useful-variables",
    "href": "index15.html#download-and-subsetting-to-retain-only-the-useful-variables",
    "title": "29  Download cycle 9",
    "section": "",
    "text": "29.1.1 Demographic\nDemographic Variables and Sample Weights (DEMO_H): The 2-year sample weights (WTINT2YR, WTMEC2YR) should be used. 15 masked variance strata and 30 masked primary sampling units (PSUs) are included in the demographics file. Each stratum has 2 PSUs.\n\ndemo &lt;- nhanes('DEMO_I')    # Both males and females 0 YEARS - 150 YEARS\ndemo1 &lt;- demo[c(\"SEQN\",     # Respondent sequence number\n                \"RIDAGEYR\", # Age in years at screening\n                \"RIAGENDR\", # gender\n                \"DMDEDUC2\", # Education level - Adults 20+\n                \"RIDRETH1\", # race/ethnicity\n                \"DMDMARTL\", # marital status    \n                \"INDHHIN2\", # Annual household income\n                \"DMDBORN4\", # where born\n                \"RIDEXPRG\", # Pregnancy status at exam (released for 20-44 yrs)\n                \"SDDSRVYR\", # survey cycle\n                \"WTINT2YR\", # Full sample 2 year weights\n                \"WTMEC2YR\", # Full sample 2 year MEC exam weight\n                \"SDMVPSU\",  # Masked variance pseudo-PSU\n                \"SDMVSTRA\")]# Masked variance pseudo-stratum\ndemo_vars &lt;- names(demo1) \ndemo2 &lt;- nhanesTranslate('DEMO_I', demo_vars, data = demo1)\n#&gt; Translated columns: RIAGENDR DMDEDUC2 RIDRETH1 DMDMARTL INDHHIN2 DMDBORN4 RIDEXPRG SDDSRVYR\nsaveRDS(demo2, file = \"data/components/demo15.RData\")\n\n\n\n29.1.2 BMI\nBody Measures (BMX_H): The NHANES examination sample weights should be used to analyze the body measurement data. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\nbmx &lt;- nhanes('BMX_I')\nbmx1 &lt;- bmx[c(\"SEQN\", # Respondent sequence number\n              \"BMXBMI\")] # Body Mass Index (kg/m**2): 2 YEARS - 150 YEARS\nbmx_vars &lt;- names(bmx1)\nbmx2 &lt;- nhanesTranslate('BMX_I', bmx_vars, data = bmx1)\n#&gt; Warning in nhanesTranslate(\"BMX_I\", bmx_vars, data = bmx1): No columns were\n#&gt; translated\nsaveRDS(bmx2, file = \"data/components/bmx15.RData\")\n\n\n\n29.1.3 Diabetes\nDiabetes (DIQ_H): diabetes questionnaire data must be conducted using the appropriate survey design variables, sample weights, and the basic demographic variables. Interview weights should only be used if questionnaire data are analyzed by themselves. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\ndiq &lt;- nhanes('DIQ_I')\ndiq1 &lt;- diq[c(\"SEQN\", # Respondent sequence number\n              \"DIQ010\", # Doctor told you have diabetes \n              \"DIQ050\", # Taking insulin now\n              \"DIQ070\", # Take diabetic pills to lower blood sugar\n              \"DIQ175A\")] # Family history\ndiq_vars &lt;- names(diq1)\ndiq2 &lt;- nhanesTranslate('DIQ_I', diq_vars, data = diq1)\n#&gt; Translated columns: DIQ010 DIQ050 DIQ070 DIQ175A\nsaveRDS(diq2, file = \"data/components/diq15.RData\")\n\n\n\n29.1.4 Smoking\nSmoking - Cigarette Use (SMQ_H): Interview weights should only be used if questionnaire data are analyzed by themselves. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\nsmq &lt;- nhanes('SMQ_I')\nsmq1 &lt;- smq[c(\"SEQN\", # Respondent sequence number\n              \"SMQ020\", # Smoked at least 100 cigarettes in life\n              \"SMQ040\")] # Do you now smoke cigarettes?: 18 YEARS - 150 YEARS\nsmq_vars &lt;- names(smq1)\nsmq2 &lt;- nhanesTranslate('SMQ_I', smq_vars, data = smq1)\n#&gt; Translated columns: SMQ020 SMQ040\nsaveRDS(smq2, file = \"data/components/smq15.RData\")\n\n\n\n29.1.5 Diet\nDiet Behavior & Nutrition (DBQ_H): interview sample weights may be used in their analysis. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\ndbq &lt;- nhanes('DBQ_I')\ndbq1 &lt;- dbq[c(\"SEQN\", # Respondent sequence number\n              \"DBQ700\")] # How healthy is the diet: 16 YEARS - 150 YEARS\ndbq_vars &lt;- names(dbq1)\ndbq2 &lt;- nhanesTranslate('DBQ_I', dbq_vars, data = dbq1)\n#&gt; Translated columns: DBQ700\nsaveRDS(dbq2, file = \"data/components/dbq15.RData\")\n\n\n\n29.1.6 Physical activity\nPhysical Activity (PAQ_H): the interview sample weights should be used in their analysis. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\npaq &lt;- nhanes('PAQ_I')\npaq1 &lt;- paq[c(\"SEQN\", # Respondent sequence number\n              \"PAQ605\")] # Vigorous work activity: 18 YEARS150 YEARS\npaq_vars &lt;- names(paq1)\npaq2 &lt;- nhanesTranslate('PAQ_I', paq_vars, data = paq1)\n#&gt; Translated columns: PAQ605\nsaveRDS(paq2, file = \"data/components/paq15.RData\")\n\n\n\n29.1.7 Access to healthcare\nHospital Utilization & Access to Care (HUQ_H): Although these data were collected as part of the household questionnaire, if they are merged with the MEC exam data, exam sample weights should be used for the analyses.\n\nhuq &lt;- nhanes('HUQ_I')\nhuq1 &lt;- huq[c(\"SEQN\", # Respondent sequence number\n              \"HUQ030\")] # Routine place to go for healthcare\nhuq_vars &lt;- names(huq1)\nhuq2 &lt;- nhanesTranslate('HUQ_I', huq_vars, data = huq1)\n#&gt; Translated columns: HUQ030\nsaveRDS(huq2, file = \"data/components/huq15.RData\")\n\n\n\n29.1.8 Blood pressure\nBlood Pressure (BPX_H): Exam sample weights should be used for analyses.\n\nSystolic blood pressure and maximum inflation level cannot be greater than 300 mmHg;\nSystolic and diastolic blood pressure measurements and the maximum inflation level can be even numbers only;\nSystolic blood pressure must be greater than diastolic blood pressure;\nIf there is no systolic blood pressure, there can be no diastolic blood pressure. (There can be a systolic measurement without a diastolic measurement.); and\nDiastolic blood pressure can be zero.\n\n\nbpx &lt;- nhanes('BPX_I')\nbpx1 &lt;- bpx[c(\"SEQN\", # Respondent sequence number\n              \"BPXSY1\", # Systolic Blood pres (1st rdg) mmHg: 8 - 150 YEARS\n              \"BPXSY2\", # Systolic: Blood pres (2nd rdg) mm Hg\n              \"BPXSY3\", # Systolic: Blood pres (3rd rdg) mm Hg\n              \"BPXSY4\", # Systolic: Blood pres (4th rdg) mm Hg\n              \"BPXDI1\", # Diastolic Blood pres (1st rdg) mmHg: 8 - 150 YEARS\n              \"BPXDI2\", # Diastolic: Blood pres (2nd rdg) mm Hg\n              \"BPXDI3\", # Diastolic: Blood pres (3rd rdg) mm Hg\n              \"BPXDI4\")] # Diastolic: Blood pres (4th rdg) mm Hg\nbpx_vars &lt;- names(bpx1)\nbpx2 &lt;- nhanesTranslate('BPX_I', bpx_vars, data = bpx1)\n#&gt; Warning in nhanesTranslate(\"BPX_I\", bpx_vars, data = bpx1): No columns were\n#&gt; translated\nsaveRDS(bpx2, file = \"data/components/bpx15.RData\")\n\nBlood Pressure & Cholesterol (BPQ_H): Although these data were collected as part of the household questionnaire, if they are merged with the MEC exam data, exam sample weights should be used for the analyses.\n\nbpq &lt;- nhanes('BPQ_I')\nbpq1 &lt;- bpq[c(\"SEQN\", # Respondent sequence number\n              \"BPQ080\")] # high cholesterol\nbpq_vars &lt;- names(bpq1)\nbpq2 &lt;- nhanesTranslate('BPQ_I', bpq_vars, data = bpq1)\n#&gt; Translated columns: BPQ080\nsaveRDS(bpq2, file = \"data/components/bpq15.RData\")\n\n\n\n29.1.9 Sleep\nSleep Disorders (SLQ_H):\n\nslq &lt;- nhanes('SLQ_I')\nslq1 &lt;- slq[c(\"SEQN\", # Respondent sequence number\n              \"SLD012\")] # Sleep hours - weekdays or workdays\nslq_vars &lt;- names(slq1)\nslq2 &lt;- nhanesTranslate('SLQ_I', slq_vars, data = slq1)\n#&gt; Warning in nhanesTranslate(\"SLQ_I\", slq_vars, data = slq1): No columns were\n#&gt; translated\nsaveRDS(slq2, file = \"data/components/slq15.RData\")\n\n\n\n29.1.10 Laboratory data\nStandard Biochemistry Profile (BIOPRO_H): Exam sample weights should be used for analyses.\n\n# Standard Biochemistry Profile\nbiopro &lt;- nhanes('BIOPRO_I') # 12 YEARS - 150 YEARS\nbiopro1 &lt;- biopro[c(\"SEQN\", # Respondent sequence number\n                    #\"LBXSTR\", # Triglycerides, refrigerated (mg/dL)\n                    \"LBXSUA\", # Uric acid (mg/dL)\n                    \"LBXSTP\", # Total protein (g/dL)\n                    \"LBXSTB\", # Total bilirubin (mg/dL)\n                    \"LBXSPH\", # Phosphorus (mg/dL)\n                    \"LBXSNASI\", # Sodium (mmol/L)\n                    \"LBXSKSI\", # Potassium (mmol/L)\n                    \"LBXSGB\", # Globulin (g/dL)\n                    \"LBXSCA\")] # Total Calcium (mg/dL)\nbiopro_vars &lt;- names(biopro1) \nbiopro2 &lt;- nhanesTranslate('BIOPRO_I', biopro_vars, data = biopro1)\n#&gt; Warning in nhanesTranslate(\"BIOPRO_I\", biopro_vars, data = biopro1): No columns\n#&gt; were translated\nsaveRDS(biopro2, file = \"data/components/biopro15.RData\")\n\n\n\n29.1.11 ICD-10-CM codes\nPrescription Medications (RXQ_RX_H): The Prescription Medications subsection provides personal interview data on use of prescription medications during a one-month period prior to the participant’s interview date. During the household SP interview, survey participants are asked if they have taken medications in the past 30 days for which they needed a prescription. Those who answer “yes” are asked to show the interviewer the medication containers of all the products used.\n\nrxq &lt;- nhanes('RXQ_RX_I')\nrxq10 &lt;- rxq[c(\"SEQN\", # Respondent sequence number\n               \"RXDRSC1\")] # ICD-10-CM code 1\nrxq11 &lt;- names(rxq10) \nrxq12 &lt;- nhanesTranslate('RXQ_RX_I', rxq11, data = rxq10)\n#&gt; Translated columns: RXDRSC1\n\nrxq20 &lt;- rxq[c(\"SEQN\", # Respondent sequence number\n               \"RXDRSC2\")] # ICD-10-CM code 2\nrxq21 &lt;- names(rxq20) \nrxq22 &lt;- nhanesTranslate('RXQ_RX_I', rxq21, data = rxq20)\n#&gt; Translated columns: RXDRSC2\n\nrxq30 &lt;- rxq[c(\"SEQN\", # Respondent sequence number\n               \"RXDRSC3\")] # ICD-10-CM code 3\nrxq31 &lt;- names(rxq30) \nrxq32 &lt;- nhanesTranslate('RXQ_RX_I', rxq31, data = rxq30)\n#&gt; Translated columns: RXDRSC3\n\nsaveRDS(rxq12, file = \"data/components/rxq1215.RData\")\nsaveRDS(rxq22, file = \"data/components/rxq2215.RData\")\nsaveRDS(rxq32, file = \"data/components/rxq3215.RData\")",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Download cycle 9</span>"
    ]
  },
  {
    "objectID": "index15.html#merging-all-the-datasets---except-for-icd-10-codes",
    "href": "index15.html#merging-all-the-datasets---except-for-icd-10-codes",
    "title": "29  Download cycle 9",
    "section": "29.2 Merging all the datasets - except for ICD-10 codes",
    "text": "29.2 Merging all the datasets - except for ICD-10 codes\n\ndat &lt;- join_all(list(demo2, bmx2, diq2, smq2, dbq2, paq2, \n                     huq2, bpx2, bpq2, slq2, biopro2),\n                by = \"SEQN\", type='full')\nnhanes15 &lt;- dat \n\n\n29.2.1 Save dataset for later use\n\ndim(nhanes15)\n#&gt; [1] 9971   42\nsave(nhanes15, rxq12, rxq22, rxq32, file = \"data/analytic15.RData\")",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Download cycle 9</span>"
    ]
  },
  {
    "objectID": "index17.html",
    "href": "index17.html",
    "title": "30  Download cycle 10",
    "section": "",
    "text": "30.1 Download and Subsetting to retain only the useful variables\nDownloading NHANES 2017-18 cycle components",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Download cycle 10</span>"
    ]
  },
  {
    "objectID": "index17.html#download-and-subsetting-to-retain-only-the-useful-variables",
    "href": "index17.html#download-and-subsetting-to-retain-only-the-useful-variables",
    "title": "30  Download cycle 10",
    "section": "",
    "text": "30.1.1 Demographic\nDemographic Variables and Sample Weights (DEMO_H): The 2-year sample weights (WTINT2YR, WTMEC2YR) should be used. 15 masked variance strata and 30 masked primary sampling units (PSUs) are included in the demographics file. Each stratum has 2 PSUs.\n\ndemo &lt;- nhanes('DEMO_J')    # Both males and females 0 YEARS - 150 YEARS\ndemo1 &lt;- demo[c(\"SEQN\",     # Respondent sequence number\n                \"RIDAGEYR\", # Age in years at screening\n                \"RIAGENDR\", # gender\n                \"DMDEDUC2\", # Education level - Adults 20+\n                \"RIDRETH1\", # race/ethnicity\n                \"DMDMARTL\", # marital status    \n                \"INDHHIN2\", # Annual household income\n                \"DMDBORN4\", # where born\n                \"RIDEXPRG\", # Pregnancy status at exam (released for 20-44 yrs)\n                \"SDDSRVYR\", # survey cycle\n                \"WTINT2YR\", # Full sample 2 year weights\n                \"WTMEC2YR\", # Full sample 2 year MEC exam weight\n                \"SDMVPSU\",  # Masked variance pseudo-PSU\n                \"SDMVSTRA\")]# Masked variance pseudo-stratum\ndemo_vars &lt;- names(demo1) \ndemo2 &lt;- nhanesTranslate('DEMO_J', demo_vars, data = demo1)\n#&gt; Translated columns: RIAGENDR DMDEDUC2 RIDRETH1 DMDMARTL INDHHIN2 DMDBORN4 RIDEXPRG SDDSRVYR\nsaveRDS(demo2, file = \"data/components/demo17.RData\")\n\n\n\n30.1.2 BMI\nBody Measures (BMX_H): The NHANES examination sample weights should be used to analyze the body measurement data. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\nbmx &lt;- nhanes('BMX_J')\nbmx1 &lt;- bmx[c(\"SEQN\", # Respondent sequence number\n              \"BMXBMI\")] # Body Mass Index (kg/m**2): 2 YEARS - 150 YEARS\nbmx_vars &lt;- names(bmx1)\nbmx2 &lt;- nhanesTranslate('BMX_J', bmx_vars, data = bmx1)\n#&gt; Warning in nhanesTranslate(\"BMX_J\", bmx_vars, data = bmx1): No columns were\n#&gt; translated\nsaveRDS(bmx2, file = \"data/components/bmx17.RData\")\n\n\n\n30.1.3 Diabetes\nDiabetes (DIQ_H): diabetes questionnaire data must be conducted using the appropriate survey design variables, sample weights, and the basic demographic variables. Interview weights should only be used if questionnaire data are analyzed by themselves. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\ndiq &lt;- nhanes('DIQ_J')\ndiq1 &lt;- diq[c(\"SEQN\", # Respondent sequence number\n              \"DIQ010\", # Doctor told you have diabetes\n              \"DIQ050\", # Taking insulin now\n              \"DIQ070\", # Take diabetic pills to lower blood sugar\n              \"DIQ175A\")] # Family history\ndiq_vars &lt;- names(diq1)\ndiq2 &lt;- nhanesTranslate('DIQ_J', diq_vars, data = diq1)\n#&gt; Translated columns: DIQ010 DIQ050 DIQ070 DIQ175A\nsaveRDS(diq2, file = \"data/components/diq17.RData\")\n\n\n\n30.1.4 Smoking\nSmoking - Cigarette Use (SMQ_H): Interview weights should only be used if questionnaire data are analyzed by themselves. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\nsmq &lt;- nhanes('SMQ_J')\nsmq1 &lt;- smq[c(\"SEQN\", # Respondent sequence number\n              \"SMQ020\", # Smoked at least 100 cigarettes in life\n              \"SMQ040\")] # Do you now smoke cigarettes?: 18 YEARS - 150 YEARS\nsmq_vars &lt;- names(smq1)\nsmq2 &lt;- nhanesTranslate('SMQ_J', smq_vars, data = smq1)\n#&gt; Translated columns: SMQ020 SMQ040\nsaveRDS(smq2, file = \"data/components/smq17.RData\")\n\n\n\n30.1.5 Diet\nDiet Behavior & Nutrition (DBQ_H): interview sample weights may be used in their analysis. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\ndbq &lt;- nhanes('DBQ_J')\ndbq1 &lt;- dbq[c(\"SEQN\", # Respondent sequence number\n              \"DBQ700\")] # How healthy is the diet: 16 YEARS - 150 YEARS\ndbq_vars &lt;- names(dbq1)\ndbq2 &lt;- nhanesTranslate('DBQ_J', dbq_vars, data = dbq1)\n#&gt; Translated columns: DBQ700\nsaveRDS(dbq2, file = \"data/components/dbq17.RData\")\n\n\n\n30.1.6 Physical activity\nPhysical Activity (PAQ_H): the interview sample weights should be used in their analysis. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\npaq &lt;- nhanes('PAQ_J')\npaq1 &lt;- paq[c(\"SEQN\", # Respondent sequence number\n              \"PAQ605\")] # Vigorous work activity: 18 YEARS150 YEARS\npaq_vars &lt;- names(paq1)\npaq2 &lt;- nhanesTranslate('PAQ_J', paq_vars, data = paq1)\n#&gt; Translated columns: PAQ605\nsaveRDS(paq2, file = \"data/components/paq17.RData\")\n\n\n\n30.1.7 Access to healthcare\nHospital Utilization & Access to Care (HUQ_H): Although these data were collected as part of the household questionnaire, if they are merged with the MEC exam data, exam sample weights should be used for the analyses.\n\nhuq &lt;- nhanes('HUQ_J')\nhuq1 &lt;- huq[c(\"SEQN\", # Respondent sequence number\n              \"HUQ030\")] # Routine place to go for healthcare\nhuq_vars &lt;- names(huq1)\nhuq2 &lt;- nhanesTranslate('HUQ_J', huq_vars, data = huq1)\n#&gt; Translated columns: HUQ030\nsaveRDS(huq2, file = \"data/components/huq17.RData\")\n\n\n\n30.1.8 Blood pressure\nBlood Pressure (BPX_H): Exam sample weights should be used for analyses.\n\nSystolic blood pressure and maximum inflation level cannot be greater than 300 mmHg;\nSystolic and diastolic blood pressure measurements and the maximum inflation level can be even numbers only;\nSystolic blood pressure must be greater than diastolic blood pressure;\nIf there is no systolic blood pressure, there can be no diastolic blood pressure. (There can be a systolic measurement without a diastolic measurement.); and\nDiastolic blood pressure can be zero.\n\n\nbpx &lt;- nhanes('BPX_J')\nbpx1 &lt;- bpx[c(\"SEQN\", # Respondent sequence number\n              \"BPXSY1\", # Systolic Blood pres (1st rdg) mmHg: 8 - 150 YEARS\n              \"BPXSY2\", # Systolic: Blood pres (2nd rdg) mm Hg\n              \"BPXSY3\", # Systolic: Blood pres (3rd rdg) mm Hg\n              \"BPXSY4\", # Systolic: Blood pres (4th rdg) mm Hg\n              \"BPXDI1\", # Diastolic Blood pres (1st rdg) mmHg: 8 - 150 YEARS\n              \"BPXDI2\", # Diastolic: Blood pres (2nd rdg) mm Hg\n              \"BPXDI3\", # Diastolic: Blood pres (3rd rdg) mm Hg\n              \"BPXDI4\")] # Diastolic: Blood pres (4th rdg) mm Hg\nbpx_vars &lt;- names(bpx1)\nbpx2 &lt;- nhanesTranslate('BPX_J', bpx_vars, data = bpx1)\n#&gt; Warning in nhanesTranslate(\"BPX_J\", bpx_vars, data = bpx1): No columns were\n#&gt; translated\nsaveRDS(bpx2, file = \"data/components/bpx17.RData\")\n\nBlood Pressure & Cholesterol (BPQ_H): Although these data were collected as part of the household questionnaire, if they are merged with the MEC exam data, exam sample weights should be used for the analyses.\n\nbpq &lt;- nhanes('BPQ_J')\nbpq1 &lt;- bpq[c(\"SEQN\", # Respondent sequence number\n              \"BPQ080\")] # high cholesterol\nbpq_vars &lt;- names(bpq1)\nbpq2 &lt;- nhanesTranslate('BPQ_J', bpq_vars, data = bpq1)\n#&gt; Translated columns: BPQ080\nsaveRDS(bpq2, file = \"data/components/bpq17.RData\")\n\n\n\n30.1.9 Sleep\nSleep Disorders (SLQ_H):\n\nslq &lt;- nhanes('SLQ_J')\nslq1 &lt;- slq[c(\"SEQN\", # Respondent sequence number\n              \"SLD012\")] # Sleep hours - weekdays or workdays\nslq_vars &lt;- names(slq1)\nslq2 &lt;- nhanesTranslate('SLQ_J', slq_vars, data = slq1)\n#&gt; Warning in nhanesTranslate(\"SLQ_J\", slq_vars, data = slq1): No columns were\n#&gt; translated\nsaveRDS(slq2, file = \"data/components/slq17.RData\")\n\n\n\n30.1.10 Laboratory data\nStandard Biochemistry Profile (BIOPRO_H): Exam sample weights should be used for analyses.\n\n# Standard Biochemistry Profile\nbiopro &lt;- nhanes('BIOPRO_J') # 12 YEARS - 150 YEARS\nbiopro1 &lt;- biopro[c(\"SEQN\", # Respondent sequence number\n                    #\"LBXSTR\", # Triglycerides, refrigerated (mg/dL)\n                    \"LBXSUA\", # Uric acid (mg/dL)\n                    \"LBXSTP\", # Total protein (g/dL)\n                    \"LBXSTB\", # Total bilirubin (mg/dL)\n                    \"LBXSPH\", # Phosphorus (mg/dL)\n                    \"LBXSNASI\", # Sodium (mmol/L)\n                    \"LBXSKSI\", # Potassium (mmol/L)\n                    \"LBXSGB\", # Globulin (g/dL)\n                    \"LBXSCA\")] # Total Calcium (mg/dL)\nbiopro_vars &lt;- names(biopro1) \nbiopro2 &lt;- nhanesTranslate('BIOPRO_J', biopro_vars, data = biopro1)\n#&gt; Warning in nhanesTranslate(\"BIOPRO_J\", biopro_vars, data = biopro1): No columns\n#&gt; were translated\nsaveRDS(biopro2, file = \"data/components/biopro17.RData\")\n\n\n\n30.1.11 ICD-10-CM codes\nPrescription Medications (RXQ_RX_H): The Prescription Medications subsection provides personal interview data on use of prescription medications during a one-month period prior to the participant’s interview date. During the household SP interview, survey participants are asked if they have taken medications in the past 30 days for which they needed a prescription. Those who answer “yes” are asked to show the interviewer the medication containers of all the products used.\n\nrxq &lt;- nhanes('RXQ_RX_J')\nrxq10 &lt;- rxq[c(\"SEQN\", # Respondent sequence number\n               \"RXDRSC1\")] # ICD-10-CM code 1\nrxq11 &lt;- names(rxq10) \nrxq12 &lt;- nhanesTranslate('RXQ_RX_J', rxq11, data = rxq10)\n#&gt; Translated columns: RXDRSC1\n\nrxq20 &lt;- rxq[c(\"SEQN\", # Respondent sequence number\n               \"RXDRSC2\")] # ICD-10-CM code 2\nrxq21 &lt;- names(rxq20) \nrxq22 &lt;- nhanesTranslate('RXQ_RX_J', rxq21, data = rxq20)\n#&gt; Translated columns: RXDRSC2\n\nrxq30 &lt;- rxq[c(\"SEQN\", # Respondent sequence number\n               \"RXDRSC3\")] # ICD-10-CM code 3\nrxq31 &lt;- names(rxq30) \nrxq32 &lt;- nhanesTranslate('RXQ_RX_J', rxq31, data = rxq30)\n#&gt; Translated columns: RXDRSC3\n\nsaveRDS(rxq12, file = \"data/components/rxq1217.RData\")\nsaveRDS(rxq22, file = \"data/components/rxq2217.RData\")\nsaveRDS(rxq32, file = \"data/components/rxq3217.RData\")",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Download cycle 10</span>"
    ]
  },
  {
    "objectID": "index17.html#merging-all-the-datasets---except-for-icd-10-codes",
    "href": "index17.html#merging-all-the-datasets---except-for-icd-10-codes",
    "title": "30  Download cycle 10",
    "section": "30.2 Merging all the datasets - except for ICD-10 codes",
    "text": "30.2 Merging all the datasets - except for ICD-10 codes\n\ndat &lt;- join_all(list(demo2, bmx2, diq2, smq2, dbq2, paq2, \n                     huq2, bpx2, bpq2, slq2, biopro2),\n                by = \"SEQN\", type='full')\nnhanes17 &lt;- dat\n\n\n30.2.1 Save dataset for later use\n\ndim(nhanes17)\n#&gt; [1] 9254   42\nsave(nhanes17, rxq12, rxq22, rxq32, file = \"data/analytic17.RData\")",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Download cycle 10</span>"
    ]
  },
  {
    "objectID": "analytic13.html",
    "href": "analytic13.html",
    "title": "31  Recoding cycle 8",
    "section": "",
    "text": "31.1 Load downloaded dataset\nCreating analytic dataset from 2013-14 cycle\nload(file = \"data/analytic13.RData\")",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Recoding cycle 8</span>"
    ]
  },
  {
    "objectID": "analytic13.html#recoding",
    "href": "analytic13.html#recoding",
    "title": "31  Recoding cycle 8",
    "section": "31.2 Recoding",
    "text": "31.2 Recoding\n\n31.2.1 ID\n\ndat2 &lt;- nhanes13\ndat2$id &lt;- dat2$SEQN\n\n\n\n31.2.2 Demographic\n\n31.2.2.1 Age\n\ndat2$age &lt;- dat2$RIDAGEYR\ndat2$age.cat &lt;- car::recode(dat2$age, \" 0:19 = '&lt;20'; 20:49 = '20-49'; 50:64 = '50-64'; \n                            65:80 = '65+'; else = NA \")\ndat2$age.cat &lt;- factor(dat2$age.cat, levels = c(\"&lt;20\", \"20-49\", \"50-64\", \"65+\"))\ntable(dat2$age.cat, useNA = \"always\")\n#&gt; \n#&gt;   &lt;20 20-49 50-64   65+  &lt;NA&gt; \n#&gt;  4406  2989  1474  1306     0\n\n\n\n31.2.2.2 Sex\n\ndat2$sex &lt;- dat2$RIAGENDR\ntable(dat2$sex, useNA = \"always\")\n#&gt; \n#&gt;   Male Female   &lt;NA&gt; \n#&gt;   5003   5172      0\n\n\n\n31.2.2.3 Education\n\ndat2$education &lt;- dat2$DMDEDUC2\ndat2$education &lt;- as.factor(dat2$education)\ndat2$education &lt;- car::recode(dat2$education, recodes = \" c('College graduate or above') = \n'College graduate or above'; c('Some college or AA degree', 'High school graduate/GED or equi') = \n'High school'; c('Less than 9th grade', '9-11th grade (Includes 12th grad') = \n'Less than high school'; else = NA \")\ndat2$education &lt;- factor(dat2$education, \n                         levels = c(\"Less than high school\", \"High school\", \n                                    \"College graduate or above\"))\ntable(dat2$education, useNA = \"always\")\n#&gt; \n#&gt;     Less than high school               High school College graduate or above \n#&gt;                       455                      1770                      1443 \n#&gt;                      &lt;NA&gt; \n#&gt;                      6507\n\n\n\n31.2.2.4 Race/ethnicity\n\ndat2$race &lt;- dat2$RIDRETH1\ndat2$race &lt;- car::recode(dat2$race, recodes = \" 'Non-Hispanic White'='White';\n                    'Non-Hispanic Black'='Black'; c('Mexican American',\n                    'Other Hispanic')= 'Hispanic'; else='Others' \")\ndat2$race &lt;- factor(dat2$race, levels = c(\"White\", \"Black\", \"Hispanic\", \"Others\"))\ntable(dat2$race, useNA = \"always\")\n#&gt; \n#&gt;    White    Black Hispanic   Others     &lt;NA&gt; \n#&gt;     3674     2267     2690     1544        0\n\n\n\n31.2.2.5 Marital status\n\ndat2$marital &lt;- dat2$DMDMARTL\ndat2$marital &lt;- car::recode(dat2$marital, recodes = \" 'Never married'='Never married';\nc('Married', 'Living with partner') = 'Married/with partner'; \n                            c('Widowed', 'Divorced', 'Separated')='Other'; else=NA \")\ndat2$marital &lt;- factor(dat2$marital, levels = c(\"Never married\", \"Married/with partner\",\n                                                \"Other\"))\ntable(dat2$marital, useNA = \"always\")\n#&gt; \n#&gt;        Never married Married/with partner                Other \n#&gt;                 1112                 3382                 1272 \n#&gt;                 &lt;NA&gt; \n#&gt;                 4409\n\n\n\n31.2.2.6 Income\n\ndat2$income &lt;- dat2$INDHHIN2\ndat2$income  &lt;- car::recode(dat2$income, recodes = \" c('$ 0 to $ 4,999', '$ 5,000 to $ 9,999',\n'$10,000 to $14,999', '$15,000 to $19,999', 'Under $20,000')='less than $20,000';\n                       c('Over $20,000','$20,000 and Over', '$20,000 to $24,999', \n                       '$25,000 to $34,999', '$35,000 to $44,999', '$45,000 to $54,999', \n                       '$55,000 to $64,999', '$65,000 to $74,999')='$20,000 to $74,999';\n                       c('$75,000 to $99,999','$100,000 and Over')='$75,000 and Over'; \n                            else=NA \")\ndat2$income  &lt;- factor(dat2$income , levels=c(\"less than $20,000\", \"$20,000 to $74,999\", \n                                              \"$75,000 and Over\"))\ntable(dat2$income, useNA = \"always\")\n#&gt; \n#&gt;  less than $20,000 $20,000 to $74,999   $75,000 and Over               &lt;NA&gt; \n#&gt;               2110               4964               2641                460\n\n\n\n31.2.2.7 Where born / citizenship\n\ndat2$born &lt;- dat2$DMDBORN4\ndat2$born &lt;- car::recode(dat2$born, recodes = \" 'Others'='Other place';\n                       'Born in 50 US states or Washington, DC'= 'Born in US'; else=NA\")\ndat2$born &lt;- factor(dat2$born, levels = c(\"Born in US\", \"Other place\"))\ntable(dat2$born, useNA = \"always\") \n#&gt; \n#&gt;  Born in US Other place        &lt;NA&gt; \n#&gt;        8262        1908           5\n\n\n\n31.2.2.8 Pregnancy\n\ndat2$pregnancy &lt;- dat2$RIDEXPRG\ndat2$pregnancy &lt;- car::recode(dat2$pregnancy, \n                      recodes = \" 'Yes, positive lab pregnancy test' = 'Yes';\n                       'The participant was not pregnant' = 'No'; \n                       'Cannot ascertain if the particip' = 'inconclusive';\n                       else= 'outside of target population'  \")\ntable(dat2$pregnancy, useNA = \"always\") \n#&gt; \n#&gt; outside of target population                         &lt;NA&gt; \n#&gt;                        10175                            0\n\n\n\n\n31.2.3 BMI\n\n31.2.3.1 BMI and Obesity\n\ndat2$bmi &lt;- dat2$BMXBMI\nsummary(dat2$bmi)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   12.10   19.70   24.70   25.68   30.20   82.90    1120\ndat2$obese &lt;- ifelse(dat2$BMXBMI &gt;= 30, \"Yes\", \"No\")\ndat2$obese &lt;- factor(dat2$obese, levels = c(\"No\", \"Yes\"))\ntable(dat2$obese, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 6708 2347 1120\n\n\n\n\n31.2.4 Diabetes\n\ndat2$diabetes &lt;- dat2$DIQ010\ndat2$diabetes &lt;- car::recode(dat2$diabetes, \" 'Yes'='Yes'; c('No','Borderline')='No';\n                             else=NA \")\n\n# Taking insulin now or diabetic pills to lower blood sugar - they have diabetes\ndat2$diabetes[dat2$DIQ050 == \"Yes\"] &lt;- \"Yes\"\ndat2$diabetes[dat2$DIQ070 == \"Yes\"] &lt;- \"Yes\"\ntable(dat2$diabetes, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 8981  782  412\n\n\n\n31.2.5 Family history of diabetes\n\ndat2$diabetes.family.history &lt;- car::recode(dat2$DIQ175A, \" 'Family history' = 'Yes'; \n                             else = 'No' \")\ndat2$diabetes.family.history &lt;- factor(dat2$diabetes.family.history, levels = c(\"No\", \"Yes\"))\ndat2$diabetes.family.history[dat2$DIQ175A==\"Don't know\"] &lt;- NA\ntable(dat2$diabetes.family.history, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 8837 1337    1\n\n\n\n31.2.6 Smoking\n\ndat2$smoking &lt;- dat2$SMQ020\ndat2$smoking &lt;- car::recode(dat2$smoking, \" 'Yes' = 'Current smoker'; 'No' = 'Never smoker'; else=NA  \")\ndat2$smoking &lt;- factor(dat2$smoking, levels = c(\"Never smoker\", \"Previous smoker\", \"Current smoker\"))\ndat2$smoking[dat2$SMQ040 == \"Not at all\"] &lt;- \"Previous smoker\"\ntable(dat2$smoking, useNA = \"always\")\n#&gt; \n#&gt;    Never smoker Previous smoker  Current smoker            &lt;NA&gt; \n#&gt;            3532            1347            1232            4064\n\n\n\n31.2.7 Diet\n\n31.2.7.1 How healthy is the diet\n\ndat2$diet.healthy &lt;- dat2$DBQ700\ndat2$diet.healthy &lt;- car::recode(dat2$diet.healthy, recodes = \" c('Excellent', 'Very good')=\n                    'Very good or excellent'; 'Good'='Good'; c('Fair', 'Poor')=\n                    'Poor or fair'; else = NA \")\ndat2$diet.healthy &lt;- factor(dat2$diet.healthy, levels = c(\"Poor or fair\", \"Good\", \n                                                          \"Very good or excellent\"))\ntable(dat2$diet.healthy, useNA = \"always\")\n#&gt; \n#&gt;           Poor or fair                   Good Very good or excellent \n#&gt;                   1824                   2743                   1896 \n#&gt;                   &lt;NA&gt; \n#&gt;                   3712\n\n\n\n\n31.2.8 Vigorous physical activity\n\ndat2$physical.activity &lt;- dat2$PAQ605\ndat2$physical.activity &lt;- car::recode(dat2$physical.activity, recodes = \" 'No' = 'No'; \n                                      'Yes' = 'Yes'; else=NA\")\ndat2$physical.activity &lt;- factor(dat2$physical.activity, levels = c(\"No\", \"Yes\"))\ntable(dat2$physical.activity, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 5975 1172 3028\n\n\n\n31.2.9 Access to medical services\n\ndat2$medical.access &lt;- dat2$HUQ030\ndat2$medical.access &lt;- car::recode(dat2$medical.access, recodes = \" c('Yes',\n                              'There is more than one place')='Yes'; 'There is no place'=\n                              'No'; else=NA\")\ntable(dat2$medical.access, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 1194 8981    0\n\n\n\n31.2.10 Hypertension/high blood pressure\n\n31.2.10.1 Systolic BP\n\ndat2$systolic1 &lt;- dat2$BPXSY1\ndat2$systolic2 &lt;- dat2$BPXSY2\ndat2$systolic3 &lt;- dat2$BPXSY3\ndat2$systolic4 &lt;- dat2$BPXSY4\n\ndat2 &lt;- dat2 %&gt;% \n  mutate(systolicBP = rowMeans(dat2[, c(\"systolic1\", \"systolic2\", \n                                        \"systolic3\", \"systolic4\")], \n                             na.rm = TRUE))\nsummary(dat2$systolicBP)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   64.67  106.00  115.33  118.31  128.00  228.67    2644\n\n\n\n31.2.10.2 Diastolic BP\n\ndat2$diastolic1 &lt;- dat2$BPXDI1\ndat2$diastolic2 &lt;- dat2$BPXDI2\ndat2$diastolic3 &lt;- dat2$BPXDI3\ndat2$diastolic4 &lt;- dat2$BPXDI4\ndatX &lt;- dat2[, c(\"diastolic1\", \"diastolic2\", \n                 \"diastolic3\", \"diastolic4\")]\ndatX[datX ==0] &lt;- NA\ndat2$diastolicBP &lt;- rowMeans(datX[, c(\"diastolic1\", \"diastolic2\", \n                                      \"diastolic3\", \"diastolic4\")], \n                             na.rm = TRUE)\nsummary(dat2$diastolicBP)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   3.333  58.000  66.667  66.329  74.667 128.000    2688\n\n\n\n\n31.2.11 Sleep (daily in hours)\n\ndat2$sleep &lt;- dat2$SLD010H\ndat2$sleep[dat2$sleep == 99] &lt;- NA\nsummary(dat2$sleep)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   2.000   6.000   7.000   6.951   8.000  12.000    3721\n\n\n\n31.2.12 Laboratory data\n\n31.2.12.1 Uric acid (mg/dL)\n\ndat2$uric.acid &lt;- dat2$LBXSUA\nsummary(dat2$uric.acid)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;    0.70    4.30    5.20    5.35    6.20   13.30    3624\n\n\n\n31.2.12.2 Total protein (g/dL)\n\ndat2$protein.total &lt;- dat2$LBXSTP\nsummary(dat2$protein.total)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   4.700   6.800   7.100   7.108   7.400  10.200    3631\n\n\n\n31.2.12.3 Total bilirubin (mg/dL)\n\ndat2$bilirubin.total &lt;- dat2$LBXSTB\nsummary(dat2$bilirubin.total)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;  0.1000  0.4000  0.6000  0.6392  0.8000  7.1000    3626\n\n\n\n31.2.12.4 Phosphorus (mg/dL)\n\ndat2$phosphorus &lt;- dat2$LBXSPH\nsummary(dat2$phosphorus)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   1.800   3.500   3.900   3.929   4.300  10.900    3623\n\n\n\n31.2.12.5 Sodium (mmol/L)\n\ndat2$sodium &lt;- dat2$LBXSNASI\nsummary(dat2$sodium)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   119.0   139.0   140.0   139.8   141.0   154.0    3622\n\n\n\n31.2.12.6 Potassium (mmol/L)\n\ndat2$potassium &lt;- dat2$LBXSKSI\nsummary(dat2$potassium)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   2.800   3.800   4.000   4.027   4.200   5.800    3623\n\n\n\n31.2.12.7 Globulin (g/dL)\n\ndat2$globulin &lt;- dat2$LBXSGB\nsummary(dat2$globulin)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   1.400   2.500   2.800   2.826   3.100   6.500    3631\n\n\n\n31.2.12.8 Total calcium (mg/dL)\n\ndat2$calcium.total &lt;- dat2$LBXSCA\nsummary(dat2$calcium.total)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   7.600   9.200   9.500   9.486   9.700  14.800    3664\n\n\n\n31.2.12.9 High cholesterol\n\ndat2$high.cholesterol &lt;- dat2$BPQ080\ndat2$high.cholesterol &lt;- car::recode(dat2$high.cholesterol, recodes = \" 'Yes'='Yes';\n                                     'No'='No'; else = NA\")\ntable(dat2$high.cholesterol, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 4391 2037 3747\n\n\n\n\n31.2.13 Survey features\n\n31.2.13.1 Weight\n\ndat2$survey.weight &lt;- dat2$WTINT2YR\nsummary(dat2$survey.weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    3698   12754   20233   30585   36280  167885\ndat2$survey.weight.mec &lt;- dat2$WTMEC2YR\nsummary(dat2$survey.weight.mec)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;       0   12562   20175   30585   36748  171395\n\n\n\n31.2.13.2 PSU\n\ndat2$psu &lt;- as.factor(dat2$SDMVPSU)\ntable(dat2$psu)\n#&gt; \n#&gt;    1    2 \n#&gt; 5249 4926\n\n\n\n31.2.13.3 Strata\n\ndat2$strata &lt;- as.factor(dat2$SDMVSTRA)\ntable(dat2$strata)\n#&gt; \n#&gt; 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 \n#&gt; 674 646 671 732 674 752 664 663 723 665 741 681 700 500 689\n\n\n\n\n31.2.14 Survey year\n\ndat2$year &lt;- dat2$SDDSRVYR\ntable(dat2$year, useNA = \"always\") \n#&gt; \n#&gt; NHANES 2013-2014 public release                            &lt;NA&gt; \n#&gt;                           10175                               0\n\n\n\n31.2.15 ICD-10-CM codes\n\ncolnames(rxq12) &lt;- c(\"id\", \"icd10\")\ncolnames(rxq22) &lt;- c(\"id\", \"icd10\")\ncolnames(rxq32) &lt;- c(\"id\", \"icd10\")\n\nrx2013 &lt;- rbind(rxq12, rxq22, rxq32)\nrx2013 &lt;- rx2013[order(rx2013$id),]\n\nrx2013$icd10[rx2013$icd10 == \"Unknown\"] &lt;- NA\nrx2013$icd10[rx2013$icd10 == \"Refused\"] &lt;- NA\nrx2013$icd10[rx2013$icd10 == \"Don't know\"] &lt;- NA\nrx2013$icd10[rx2013$icd10 == \"\"] &lt;- NA\nrx2013$icd10.new &lt;- substr(rx2013$icd10, start = 1, stop = 3)\n\nrx2013 &lt;- na.omit(rx2013)",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Recoding cycle 8</span>"
    ]
  },
  {
    "objectID": "analytic13.html#analytic-data",
    "href": "analytic13.html#analytic-data",
    "title": "31  Recoding cycle 8",
    "section": "31.3 Analytic data",
    "text": "31.3 Analytic data\n\n31.3.1 Full dataset\n\nnhanes13r &lt;- dat2\n\n\n\n31.3.2 Analytic datset - adults 20 years of more\n\nvars &lt;- c(\n  # ID\n  \"id\",\n  \n  # Demographic\n  \"age\", \"age.cat\", \"sex\", \"education\", \"race\", \n  \"marital\", \"income\", \"born\", \"pregnancy\",\n  \n  # obesity\n  \"obese\", \n  \n  # Diabetes\n  \"diabetes\", \"diabetes.family.history\",\n  \n  # Smoking\n  \"smoking\", \n  \n  # Diet\n  \"diet.healthy\", \n\n  # Physical activity\n  \"physical.activity\", \n  \n  # Access to routine healthcare\n  \"medical.access\",\n  \n  # Blood pressure and Hypertension\n  \"systolicBP\", \"diastolicBP\", \n  \n  # Sleep \n  \"sleep\",\n\n  # Laboratory \n  \"uric.acid\", \"protein.total\", \"bilirubin.total\", \"phosphorus\",\n  \"sodium\", \"potassium\", \"globulin\", \"calcium.total\", \n  \"high.cholesterol\",\n  \n  # Survey features\n  \"survey.weight\", \"survey.weight.mec\", \"psu\", \"strata\", \n  \n  # Survey year\n  \"year\"\n)\n\nnhanes13r.sel &lt;- nhanes13r[, vars]\n\n\n# Adults 20 years of more and not pregnant\ndim(nhanes13r.sel)\n#&gt; [1] 10175    34\nanalytic13 &lt;- subset(nhanes13r.sel, age &gt;= 20 & \n                       pregnancy != 'yes')\ndim(analytic13)\n#&gt; [1] 5769   34\n\n\n\n31.3.3 Save dataset for later use\n\ndim(analytic13)\n#&gt; [1] 5769   34\ndim(rx2013)\n#&gt; [1] 14474     3\nsave(analytic13, rx2013, file = \"data/analytic13recoded.RData\")",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Recoding cycle 8</span>"
    ]
  },
  {
    "objectID": "analytic15.html",
    "href": "analytic15.html",
    "title": "32  Recoding cycle 9",
    "section": "",
    "text": "32.1 Load downloaded dataset\nCreating analytic dataset from 2015-16 cycle\nload(file = \"data/analytic15.RData\")",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Recoding cycle 9</span>"
    ]
  },
  {
    "objectID": "analytic15.html#recoding",
    "href": "analytic15.html#recoding",
    "title": "32  Recoding cycle 9",
    "section": "32.2 Recoding",
    "text": "32.2 Recoding\n\n32.2.1 ID\n\ndat2 &lt;- nhanes15\ndat2$id &lt;- dat2$SEQN\n\n\n\n32.2.2 Demographic\n\n32.2.2.1 Age\n\ndat2$age &lt;- dat2$RIDAGEYR\ndat2$age.cat &lt;- car::recode(dat2$age, \" 0:19 = '&lt;20'; 20:49 = '20-49'; 50:64 = '50-64'; \n                            65:80 = '65+'; else = NA \")\ndat2$age.cat &lt;- factor(dat2$age.cat, levels = c(\"&lt;20\", \"20-49\", \"50-64\", \"65+\"))\ntable(dat2$age.cat, useNA = \"always\")\n#&gt; \n#&gt;   &lt;20 20-49 50-64   65+  &lt;NA&gt; \n#&gt;  4252  2894  1447  1378     0\n\n\n\n32.2.2.2 Sex\n\ndat2$sex &lt;- dat2$RIAGENDR\ntable(dat2$sex, useNA = \"always\")\n#&gt; \n#&gt;   Male Female   &lt;NA&gt; \n#&gt;   4892   5079      0\n\n\n\n32.2.2.3 Education\n\ndat2$education &lt;- dat2$DMDEDUC2\ndat2$education &lt;- as.factor(dat2$education)\ndat2$education &lt;- car::recode(dat2$education, recodes = \" c('College graduate or above') = \n'College graduate or above'; c('Some college or AA degree', 'High school graduate/GED or equi') = \n'High school'; c('Less than 9th grade', '9-11th grade (Includes 12th grad') = \n'Less than high school'; else = NA \")\ndat2$education &lt;- factor(dat2$education, \n                         levels = c(\"Less than high school\", \"High school\", \n                                    \"College graduate or above\"))\ntable(dat2$education, useNA = \"always\")\n#&gt; \n#&gt;     Less than high school               High school College graduate or above \n#&gt;                       688                      1692                      1422 \n#&gt;                      &lt;NA&gt; \n#&gt;                      6169\n\n\n\n32.2.2.4 Race/ethnicity\n\ndat2$race &lt;- dat2$RIDRETH1\ndat2$race &lt;- car::recode(dat2$race, recodes = \" 'Non-Hispanic White'='White';\n                    'Non-Hispanic Black'='Black'; c('Mexican American',\n                    'Other Hispanic')= 'Hispanic'; else='Others' \")\ndat2$race &lt;- factor(dat2$race, levels = c(\"White\", \"Black\", \"Hispanic\", \"Others\"))\ntable(dat2$race, useNA = \"always\")\n#&gt; \n#&gt;    White    Black Hispanic   Others     &lt;NA&gt; \n#&gt;     3066     2129     3229     1547        0\n\n\n\n32.2.2.5 Marital status\n\ndat2$marital &lt;- dat2$DMDMARTL\ndat2$marital &lt;- car::recode(dat2$marital, recodes = \" 'Never married'='Never married';\nc('Married', 'Living with partner') = 'Married/with partner'; \n                            c('Widowed', 'Divorced', 'Separated')='Other'; else=NA \")\ndat2$marital &lt;- factor(dat2$marital, levels = c(\"Never married\", \"Married/with partner\",\n                                                \"Other\"))\ntable(dat2$marital, useNA = \"always\")\n#&gt; \n#&gt;        Never married Married/with partner                Other \n#&gt;                 1048                 3441                 1227 \n#&gt;                 &lt;NA&gt; \n#&gt;                 4255\n\n\n\n32.2.2.6 Income\n\ndat2$income &lt;- dat2$INDHHIN2\ndat2$income  &lt;- car::recode(dat2$income, recodes = \" c('$ 0 to $ 4,999', '$ 5,000 to $ 9,999',\n'$10,000 to $14,999', '$15,000 to $19,999', 'Under $20,000')='less than $20,000';\n                       c('Over $20,000','$20,000 and Over', '$20,000 to $24,999', \n                       '$25,000 to $34,999', '$35,000 to $44,999', '$45,000 to $54,999', \n                       '$55,000 to $64,999', '$65,000 to $74,999')='$20,000 to $74,999';\n                       c('$75,000 to $99,999','$100,000 and Over')='$75,000 and Over'; \n                            else=NA \")\ndat2$income  &lt;- factor(dat2$income , levels=c(\"less than $20,000\", \"$20,000 to $74,999\", \n                                              \"$75,000 and Over\"))\ntable(dat2$income, useNA = \"always\")\n#&gt; \n#&gt;  less than $20,000 $20,000 to $74,999   $75,000 and Over               &lt;NA&gt; \n#&gt;               1906               4812               2554                699\n\n\n\n32.2.2.7 Where born / citizenship\n\ndat2$born &lt;- dat2$DMDBORN4\ndat2$born &lt;- car::recode(dat2$born, recodes = \" 'Others'='Other place';\n                       'Born in 50 US states or Washingt'= 'Born in US'; else=NA\")\ndat2$born &lt;- factor(dat2$born, levels = c(\"Born in US\", \"Other place\"))\ntable(dat2$born, useNA = \"always\") \n#&gt; \n#&gt;  Born in US Other place        &lt;NA&gt; \n#&gt;           0        2236        7735\n\n\n\n32.2.2.8 Pregnancy\n\ndat2$pregnancy &lt;- dat2$RIDEXPRG\ndat2$pregnancy &lt;- car::recode(dat2$pregnancy, \n                      recodes = \" 'Yes, positive lab pregnancy test' = 'Yes';\n                       'The participant was not pregnant' = 'No'; \n                       'Cannot ascertain if the particip' = 'inconclusive';\n                       else= 'outside of target population'  \")\ntable(dat2$pregnancy, useNA = \"always\") \n#&gt; \n#&gt; outside of target population                         &lt;NA&gt; \n#&gt;                         9971                            0\n\n\n\n\n32.2.3 BMI\n\n32.2.3.1 BMI and Obesity\n\ndat2$bmi &lt;- dat2$BMXBMI\nsummary(dat2$bmi)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   11.50   19.90   25.20   26.02   30.60   67.30    1215\ndat2$obese &lt;- ifelse(dat2$BMXBMI &gt;= 30, \"Yes\", \"No\")\ndat2$obese &lt;- factor(dat2$obese, levels = c(\"No\", \"Yes\"))\ntable(dat2$obese, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 6346 2410 1215\n\n\n\n\n32.2.4 Diabetes\n\ndat2$diabetes &lt;- dat2$DIQ010\ndat2$diabetes &lt;- car::recode(dat2$diabetes, \" 'Yes'='Yes'; c('No','Borderline')='No';\n                             else=NA \")\n\n# Taking insulin now or diabetic pills to lower blood sugar - they have diabetes\ndat2$diabetes[dat2$DIQ050 == \"Yes\"] &lt;- \"Yes\"\ndat2$diabetes[dat2$DIQ070 == \"Yes\"] &lt;- \"Yes\"\ntable(dat2$diabetes, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 8648  923  400\n\n\n\n32.2.5 Family history of diabetes\n\ntable(dat2$DIQ175A, useNA = \"always\")\n#&gt; \n#&gt; Family history           &lt;NA&gt; \n#&gt;           1186           8785\ndat2$diabetes.family.history &lt;- dat2$DIQ175A\ndat2$diabetes.family.history &lt;- car::recode(dat2$diabetes.family.history, \" 10 = 'Yes'; \n                             else = 'No' \")\ndat2$diabetes.family.history &lt;- factor(dat2$diabetes.family.history, levels = c(\"No\", \"Yes\"))\ndat2$diabetes.family.history[dat2$DIQ175A==\"Don't know\"] &lt;- NA\ntable(dat2$diabetes.family.history, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 9971    0    0\n\n\n\n32.2.6 Smoking\n\ndat2$smoking &lt;- dat2$SMQ020\ndat2$smoking &lt;- car::recode(dat2$smoking, \" 'Yes' = 'Current smoker'; 'No' = 'Never smoker'; else=NA  \")\ndat2$smoking &lt;- factor(dat2$smoking, levels = c(\"Never smoker\", \"Previous smoker\", \"Current smoker\"))\ndat2$smoking[dat2$SMQ040 == \"Not at all\"] &lt;- \"Previous smoker\"\ntable(dat2$smoking, useNA = \"always\")\n#&gt; \n#&gt;    Never smoker Previous smoker  Current smoker            &lt;NA&gt; \n#&gt;            3559            1322            1100            3990\n\n\n\n32.2.7 Diet\n\n32.2.7.1 How healthy is the diet\n\ndat2$diet.healthy &lt;- dat2$DBQ700\ndat2$diet.healthy &lt;- car::recode(dat2$diet.healthy, recodes = \" c('Excellent', 'Very good')=\n                    'Very good or excellent'; 'Good'='Good'; c('Fair', 'Poor')=\n                    'Poor or fair'; else = NA \")\ndat2$diet.healthy &lt;- factor(dat2$diet.healthy, levels = c(\"Poor or fair\", \"Good\", \n                                                          \"Very good or excellent\"))\ntable(dat2$diet.healthy, useNA = \"always\")\n#&gt; \n#&gt;           Poor or fair                   Good Very good or excellent \n#&gt;                   2105                   2524                   1697 \n#&gt;                   &lt;NA&gt; \n#&gt;                   3645\n\n\n\n\n32.2.8 Vigorous physical activity\n\ndat2$physical.activity &lt;- dat2$PAQ605\ndat2$physical.activity &lt;- car::recode(dat2$physical.activity, recodes = \" 'No' = 'No'; \n                                      'Yes' = 'Yes'; else=NA\")\ndat2$physical.activity &lt;- factor(dat2$physical.activity, levels = c(\"No\", \"Yes\"))\ntable(dat2$physical.activity, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 5596 1366 3009\n\n\n\n32.2.9 Access to medical services\n\ndat2$medical.access &lt;- dat2$HUQ030\ndat2$medical.access &lt;- car::recode(dat2$medical.access, recodes = \" c('Yes',\n                              'There is more than one place')='Yes'; 'There is no place'=\n                              'No'; else=NA\")\ntable(dat2$medical.access, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 1340 8631    0\n\n\n\n32.2.10 Hypertension/high blood pressure\n\n32.2.10.1 Systolic BP\n\ndat2$systolic1 &lt;- dat2$BPXSY1\ndat2$systolic2 &lt;- dat2$BPXSY2\ndat2$systolic3 &lt;- dat2$BPXSY3\ndat2$systolic4 &lt;- dat2$BPXSY4\n\ndat2 &lt;- dat2 %&gt;% \n  mutate(systolicBP = rowMeans(dat2[, c(\"systolic1\", \"systolic2\", \n                                        \"systolic3\", \"systolic4\")], \n                             na.rm = TRUE))\nsummary(dat2$systolicBP)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;    74.0   107.3   117.3   120.4   130.0   231.3    2608\n\n\n\n32.2.10.2 Diastolic BP\n\ndat2$diastolic1 &lt;- dat2$BPXDI1\ndat2$diastolic2 &lt;- dat2$BPXDI2\ndat2$diastolic3 &lt;- dat2$BPXDI3\ndat2$diastolic4 &lt;- dat2$BPXDI4\ndatX &lt;- dat2[, c(\"diastolic1\", \"diastolic2\", \n                 \"diastolic3\", \"diastolic4\")]\ndatX[datX ==0] &lt;- NA\ndat2$diastolicBP &lt;- rowMeans(datX[, c(\"diastolic1\", \"diastolic2\", \n                                      \"diastolic3\", \"diastolic4\")], \n                             na.rm = TRUE)\nsummary(dat2$diastolicBP)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;    2.00   58.00   66.67   66.64   74.67  138.67    2636\n\n\n\n\n32.2.11 Sleep (daily in hours)\n\ndat2$sleep &lt;- dat2$SLD012\nsummary(dat2$sleep)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   2.000   7.000   8.000   7.753   8.500  14.500    3677\n\n\n\n32.2.12 Laboratory data\n\n32.2.12.1 Uric acid (mg/dL)\n\ndat2$uric.acid &lt;- dat2$LBXSUA\nsummary(dat2$uric.acid)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   1.600   4.300   5.200   5.335   6.200  18.000    3717\n\n\n\n32.2.12.2 Total protein (g/dL)\n\ndat2$protein.total &lt;- dat2$LBXSTP\nsummary(dat2$protein.total)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   5.200   6.900   7.200   7.201   7.500  10.100    3718\n\n\n\n32.2.12.3 Total bilirubin (mg/dL)\n\ndat2$bilirubin.total &lt;- dat2$LBXSTB\nsummary(dat2$bilirubin.total)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;  0.0000  0.4000  0.5000  0.5518  0.7000  3.5000    3717\n\n\n\n32.2.12.4 Phosphorus (mg/dL)\n\ndat2$phosphorus &lt;- dat2$LBXSPH\nsummary(dat2$phosphorus)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   1.000   3.400   3.800   3.796   4.200   9.700    3715\n\n\n\n32.2.12.5 Sodium (mmol/L)\n\ndat2$sodium &lt;- dat2$LBXSNASI\nsummary(dat2$sodium)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   124.0   137.0   139.0   138.7   140.0   161.0    3714\n\n\n\n32.2.12.6 Potassium (mmol/L)\n\ndat2$potassium &lt;- dat2$LBXSKSI\nsummary(dat2$potassium)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   2.600   3.740   3.930   3.952   4.150   5.860    3714\n\n\n\n32.2.12.7 Globulin (g/dL)\n\ndat2$globulin &lt;- dat2$LBXSGB\nsummary(dat2$globulin)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   0.600   2.600   2.800   2.857   3.100   7.000    3719\n\n\n\n32.2.12.8 Total calcium (mg/dL)\n\ndat2$calcium.total &lt;- dat2$LBXSCA\nsummary(dat2$calcium.total)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   7.300   9.100   9.400   9.375   9.600  11.500    3714\n\n\n\n32.2.12.9 High cholesterol\n\ndat2$high.cholesterol &lt;- dat2$BPQ080\ndat2$high.cholesterol &lt;- car::recode(dat2$high.cholesterol, recodes = \" 'Yes'='Yes';\n                                     'No'='No'; else = NA\")\ntable(dat2$high.cholesterol, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 4323 1960 3688\n\n\n\n\n32.2.13 Survey features\n\n32.2.13.1 Weight\n\ndat2$survey.weight &lt;- dat2$WTINT2YR\nsummary(dat2$survey.weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    3294   12879   20160   31740   33257  233756\ndat2$survey.weight.mec &lt;- dat2$WTMEC2YR\nsummary(dat2$survey.weight.mec)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;       0   12551   20281   31740   33708  242387\n\n\n\n32.2.13.2 PSU\n\ndat2$psu &lt;- as.factor(dat2$SDMVPSU)\ntable(dat2$psu)\n#&gt; \n#&gt;    1    2 \n#&gt; 5127 4844\n\n\n\n32.2.13.3 Strata\n\ndat2$strata &lt;- as.factor(dat2$SDMVSTRA)\ntable(dat2$strata)\n#&gt; \n#&gt; 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 \n#&gt; 462 685 694 629 612 571 673 723 665 688 681 759 805 773 551\n\n\n\n\n32.2.14 Survey year\n\ndat2$year &lt;- dat2$SDDSRVYR\ntable(dat2$year, useNA = \"always\") \n#&gt; \n#&gt; NHANES 2015-2016 public release                            &lt;NA&gt; \n#&gt;                            9971                               0\n\n\n\n32.2.15 ICD-10-CM codes\n\ncolnames(rxq12) &lt;- c(\"id\", \"icd10\")\ncolnames(rxq22) &lt;- c(\"id\", \"icd10\")\ncolnames(rxq32) &lt;- c(\"id\", \"icd10\")\n\nrx2015 &lt;- rbind(rxq12, rxq22, rxq32)\nrx2015 &lt;- rx2015[order(rx2015$id),]\n\nrx2015$icd10[rx2015$icd10 == \"Unknown\"] &lt;- NA\nrx2015$icd10[rx2015$icd10 == \"Refused\"] &lt;- NA\nrx2015$icd10[rx2015$icd10 == \"Don't know\"] &lt;- NA\nrx2015$icd10[rx2015$icd10 == \"\"] &lt;- NA\nrx2015$icd10.new &lt;- substr(rx2015$icd10, start = 1, stop = 3)\n\nrx2015 &lt;- na.omit(rx2015)",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Recoding cycle 9</span>"
    ]
  },
  {
    "objectID": "analytic15.html#analytic-data",
    "href": "analytic15.html#analytic-data",
    "title": "32  Recoding cycle 9",
    "section": "32.3 Analytic data",
    "text": "32.3 Analytic data\n\n32.3.1 Full dataset\n\nnhanes15r &lt;- dat2\n\n\n\n32.3.2 Analytic datset - adults 20 years of more\n\nvars &lt;- c(\n  # ID\n  \"id\",\n  \n  # Demographic\n  \"age\", \"age.cat\", \"sex\", \"education\", \"race\", \n  \"marital\", \"income\", \"born\", \"pregnancy\",\n  \n  # obesity\n  \"obese\", \n  \n  # Diabetes\n  \"diabetes\", \"diabetes.family.history\",\n  \n  # Smoking\n  \"smoking\", \n  \n  # Diet\n  \"diet.healthy\", \n\n  # Physical activity\n  \"physical.activity\", \n  \n  # Access to routine healthcare\n  \"medical.access\",\n  \n  # Blood pressure and Hypertension\n  \"systolicBP\", \"diastolicBP\", \n  \n  # Sleep \n  \"sleep\",\n\n  # Laboratory \n  \"uric.acid\", \"protein.total\", \"bilirubin.total\", \"phosphorus\",\n  \"sodium\", \"potassium\", \"globulin\", \"calcium.total\", \n  \"high.cholesterol\",\n  \n  # Survey features\n  \"survey.weight\", \"survey.weight.mec\", \"psu\", \"strata\", \n  \n  # Survey year\n  \"year\"\n)\n\nnhanes15r.sel &lt;- nhanes15r[, vars]\n\n\n# Adults 20 years of more and not pregnant\ndim(nhanes15r.sel)\n#&gt; [1] 9971   34\nanalytic15 &lt;- subset(nhanes15r.sel, age &gt;= 20 & \n                       pregnancy != 'yes')\ndim(analytic15)\n#&gt; [1] 5719   34\n\n\n\n32.3.3 Save dataset for later use\n\ndim(analytic15)\n#&gt; [1] 5719   34\ndim(rx2015)\n#&gt; [1] 14084     3\nsave(analytic15, rx2015, file = \"data/analytic15recoded.RData\")",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Recoding cycle 9</span>"
    ]
  },
  {
    "objectID": "analytic17.html",
    "href": "analytic17.html",
    "title": "33  Recoding cycle 10",
    "section": "",
    "text": "33.1 Load downloaded dataset\nCreating analytic dataset from 2017-18 cycle\nload(file = \"data/analytic17.RData\")",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Recoding cycle 10</span>"
    ]
  },
  {
    "objectID": "analytic17.html#recoding",
    "href": "analytic17.html#recoding",
    "title": "33  Recoding cycle 10",
    "section": "33.2 Recoding",
    "text": "33.2 Recoding\n\n33.2.1 ID\n\ndat2 &lt;- nhanes17\ndat2$id &lt;- dat2$SEQN\n\n\n\n33.2.2 Demographic\n\n33.2.2.1 Age\n\ndat2$age &lt;- dat2$RIDAGEYR\ndat2$age.cat &lt;- car::recode(dat2$age, \" 0:19 = '&lt;20'; 20:49 = '20-49'; 50:64 = '50-64'; \n                            65:80 = '65+'; else = NA \")\ndat2$age.cat &lt;- factor(dat2$age.cat, levels = c(\"&lt;20\", \"20-49\", \"50-64\", \"65+\"))\ntable(dat2$age.cat, useNA = \"always\")\n#&gt; \n#&gt;   &lt;20 20-49 50-64   65+  &lt;NA&gt; \n#&gt;  3685  2500  1569  1500     0\n\n\n\n33.2.2.2 Sex\n\ndat2$sex &lt;- dat2$RIAGENDR\ntable(dat2$sex, useNA = \"always\")\n#&gt; \n#&gt;   Male Female   &lt;NA&gt; \n#&gt;   4557   4697      0\n\n\n\n33.2.2.3 Education\n\ndat2$education &lt;- dat2$DMDEDUC2\ndat2$education &lt;- as.factor(dat2$education)\ndat2$education &lt;- car::recode(dat2$education, recodes = \" c('College graduate or above') = \n'College graduate or above'; c('Some college or AA degree', 'High school graduate/GED or equi') = \n'High school'; c('Less than 9th grade', '9-11th grade (Includes 12th grad') = \n'Less than high school'; else = NA \")\ndat2$education &lt;- factor(dat2$education, \n                         levels = c(\"Less than high school\", \"High school\", \n                                    \"College graduate or above\"))\ntable(dat2$education, useNA = \"always\")\n#&gt; \n#&gt;     Less than high school               High school College graduate or above \n#&gt;                       479                      1778                      1336 \n#&gt;                      &lt;NA&gt; \n#&gt;                      5661\n\n\n\n33.2.2.4 Race/ethnicity\n\ndat2$race &lt;- dat2$RIDRETH1\ndat2$race &lt;- car::recode(dat2$race, recodes = \" 'Non-Hispanic White'='White';\n                    'Non-Hispanic Black'='Black'; c('Mexican American',\n                    'Other Hispanic')= 'Hispanic'; else='Others' \")\ndat2$race &lt;- factor(dat2$race, levels = c(\"White\", \"Black\", \"Hispanic\", \"Others\"))\ntable(dat2$race, useNA = \"always\")\n#&gt; \n#&gt;    White    Black Hispanic   Others     &lt;NA&gt; \n#&gt;     3150     2115     2187     1802        0\n\n\n\n33.2.2.5 Marital status\n\ndat2$marital &lt;- dat2$DMDMARTL\ndat2$marital &lt;- car::recode(dat2$marital, recodes = \" 'Never married'='Never married';\nc('Married', 'Living with partner') = 'Married/with partner'; \n                            c('Widowed', 'Divorced', 'Separated')='Other'; else=NA \")\ndat2$marital &lt;- factor(dat2$marital, levels = c(\"Never married\", \"Married/with partner\",\n                                                \"Other\"))\ntable(dat2$marital, useNA = \"always\")\n#&gt; \n#&gt;        Never married Married/with partner                Other \n#&gt;                 1006                 3252                 1305 \n#&gt;                 &lt;NA&gt; \n#&gt;                 3691\n\n\n\n33.2.2.6 Income\n\ndat2$income &lt;- dat2$INDHHIN2\ndat2$income  &lt;- car::recode(dat2$income, recodes = \" c('$ 0 to $ 4,999', '$ 5,000 to $ 9,999',\n'$10,000 to $14,999', '$15,000 to $19,999', 'Under $20,000')='less than $20,000';\n                       c('Over $20,000','$20,000 and Over', '$20,000 to $24,999', \n                       '$25,000 to $34,999', '$35,000 to $44,999', '$45,000 to $54,999', \n                       '$55,000 to $64,999', '$65,000 to $74,999')='$20,000 to $74,999';\n                       c('$75,000 to $99,999','$100,000 and Over')='$75,000 and Over'; \n                            else=NA \")\ndat2$income  &lt;- factor(dat2$income , levels=c(\"less than $20,000\", \"$20,000 to $74,999\", \n                                              \"$75,000 and Over\"))\ntable(dat2$income, useNA = \"always\")\n#&gt; \n#&gt;  less than $20,000 $20,000 to $74,999   $75,000 and Over               &lt;NA&gt; \n#&gt;               1589               4331               2453                881\n\n\n\n33.2.2.7 Where born / citizenship\n\ndat2$born &lt;- dat2$DMDBORN4\ndat2$born &lt;- car::recode(dat2$born, recodes = \" 'Others'='Other place';\n                       'Born in 50 US states or Washington, DC'= 'Born in US'; else=NA\")\ndat2$born &lt;- factor(dat2$born, levels = c(\"Born in US\", \"Other place\"))\ntable(dat2$born, useNA = \"always\") \n#&gt; \n#&gt;  Born in US Other place        &lt;NA&gt; \n#&gt;        7303        1948           3\n\n\n\n33.2.2.8 Pregnancy\n\ndat2$pregnancy &lt;- dat2$RIDEXPRG\ndat2$pregnancy &lt;- car::recode(dat2$pregnancy, \n                      recodes = \" 'Yes, positive lab pregnancy test' = 'Yes';\n                       'The participant was not pregnant' = 'No'; \n                       'Cannot ascertain if the particip' = 'inconclusive';\n                       else= 'outside of target population'  \")\ntable(dat2$pregnancy, useNA = \"always\") \n#&gt; \n#&gt; outside of target population                         &lt;NA&gt; \n#&gt;                         9254                            0\n\n\n\n\n33.2.3 BMI\n\n33.2.3.1 BMI and Obesity\n\ndat2$bmi &lt;- dat2$BMXBMI\nsummary(dat2$bmi)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   12.30   20.40   25.80   26.58   31.30   86.20    1249\ndat2$obese &lt;- ifelse(dat2$BMXBMI &gt;= 30, \"Yes\", \"No\")\ndat2$obese &lt;- factor(dat2$obese, levels = c(\"No\", \"Yes\"))\ntable(dat2$obese, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 5597 2408 1249\n\n\n\n\n33.2.4 Diabetes\n\ndat2$diabetes &lt;- dat2$DIQ010\ndat2$diabetes &lt;- car::recode(dat2$diabetes, \" 'Yes'='Yes'; c('No','Borderline')='No';\n                             else=NA \")\n\n# Taking insulin now or diabetic pills to lower blood sugar - they have diabetes\ndat2$diabetes[dat2$DIQ050 == \"Yes\"] &lt;- \"Yes\"\ndat2$diabetes[dat2$DIQ070 == \"Yes\"] &lt;- \"Yes\"\ntable(dat2$diabetes, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 7927  966  361\n\n\n\n33.2.5 Family history of diabetes\n\ntable(dat2$DIQ175A, useNA = \"always\")\n#&gt; \n#&gt; Family history     Don't know           &lt;NA&gt; \n#&gt;           1143              2           8109\ndat2$diabetes.family.history &lt;- dat2$DIQ175A\ndat2$diabetes.family.history &lt;- car::recode(dat2$diabetes.family.history, \" 'Family history' = 'Yes'; \n                             else = 'No' \")\ndat2$diabetes.family.history &lt;- factor(dat2$diabetes.family.history, levels = c(\"No\", \"Yes\"))\ndat2$diabetes.family.history[dat2$DIQ175A==\"Don't know\"] &lt;- NA\ntable(dat2$diabetes.family.history, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 8109 1143    2\n\n\n\n33.2.6 Smoking\n\ndat2$smoking &lt;- dat2$SMQ020\ndat2$smoking &lt;- car::recode(dat2$smoking, \" 'Yes' = 'Current smoker'; 'No' = 'Never smoker'; else=NA  \")\ndat2$smoking &lt;- factor(dat2$smoking, levels = c(\"Never smoker\", \"Previous smoker\", \"Current smoker\"))\ndat2$smoking[dat2$SMQ040 == \"Not at all\"] &lt;- \"Previous smoker\"\ntable(dat2$smoking, useNA = \"always\")\n#&gt; \n#&gt;    Never smoker Previous smoker  Current smoker            &lt;NA&gt; \n#&gt;            3497            1338            1021            3398\n\n\n\n33.2.7 Diet\n\n33.2.7.1 How healthy is the diet\n\ndat2$diet.healthy &lt;- dat2$DBQ700\ndat2$diet.healthy &lt;- car::recode(dat2$diet.healthy, recodes = \" c('Excellent', 'Very good')=\n                    'Very good or excellent'; 'Good'='Good'; c('Fair', 'Poor')=\n                    'Poor or fair'; else = NA \")\ndat2$diet.healthy &lt;- factor(dat2$diet.healthy, levels = c(\"Poor or fair\", \"Good\", \n                                                          \"Very good or excellent\"))\ntable(dat2$diet.healthy, useNA = \"always\")\n#&gt; \n#&gt;           Poor or fair                   Good Very good or excellent \n#&gt;                   2036                   2411                   1712 \n#&gt;                   &lt;NA&gt; \n#&gt;                   3095\n\n\n\n\n33.2.8 Vigorous physical activity\n\ndat2$physical.activity &lt;- dat2$PAQ605\ndat2$physical.activity &lt;- car::recode(dat2$physical.activity, recodes = \" 'No' = 'No'; \n                                      'Yes' = 'Yes'; else=NA\")\ndat2$physical.activity &lt;- factor(dat2$physical.activity, levels = c(\"No\", \"Yes\"))\ntable(dat2$physical.activity, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 4461 1389 3404\n\n\n\n33.2.9 Access to medical services\n\ndat2$medical.access &lt;- dat2$HUQ030\ndat2$medical.access &lt;- car::recode(dat2$medical.access, recodes = \" c('Yes',\n                              'There is more than one place')='Yes'; 'There is no place'=\n                              'No'; else=NA\")\ntable(dat2$medical.access, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 1398 7854    2\n\n\n\n33.2.10 Hypertension/high blood pressure\n\n33.2.10.1 Systolic BP\n\ndat2$systolic1 &lt;- dat2$BPXSY1\ndat2$systolic2 &lt;- dat2$BPXSY2\ndat2$systolic3 &lt;- dat2$BPXSY3\ndat2$systolic4 &lt;- dat2$BPXSY4\n\ndat2 &lt;- dat2 %&gt;% \n  mutate(systolicBP = rowMeans(dat2[, c(\"systolic1\", \"systolic2\", \n                                        \"systolic3\", \"systolic4\")], \n                             na.rm = TRUE))\nsummary(dat2$systolicBP)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   72.67  106.67  118.00  121.68  132.67  238.00    2537\n\n\n\n33.2.10.2 Diastolic BP\n\ndat2$diastolic1 &lt;- dat2$BPXDI1\ndat2$diastolic2 &lt;- dat2$BPXDI2\ndat2$diastolic3 &lt;- dat2$BPXDI3\ndat2$diastolic4 &lt;- dat2$BPXDI4\ndatX &lt;- dat2[, c(\"diastolic1\", \"diastolic2\", \n                 \"diastolic3\", \"diastolic4\")]\ndatX[datX ==0] &lt;- NA\ndat2$diastolicBP &lt;- rowMeans(datX[, c(\"diastolic1\", \"diastolic2\", \n                                      \"diastolic3\", \"diastolic4\")], \n                             na.rm = TRUE)\nsummary(dat2$diastolicBP)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;    8.00   61.33   70.00   69.54   77.33  135.33    2618\n\n\n\n\n33.2.11 Sleep (daily in hours)\n\ndat2$sleep &lt;- dat2$SLD012\nsummary(dat2$sleep)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   2.000   7.000   8.000   7.659   8.500  14.000    3141\n\n\n\n33.2.12 Laboratory data\n\n33.2.12.1 Uric acid (mg/dL)\n\ndat2$uric.acid &lt;- dat2$LBXSUA\nsummary(dat2$uric.acid)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   0.800   4.300   5.300   5.402   6.300  15.100    3353\n\n\n\n33.2.12.2 Total protein (g/dL)\n\ndat2$protein.total &lt;- dat2$LBXSTP\nsummary(dat2$protein.total)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   5.300   6.900   7.200   7.166   7.400  10.000    3353\n\n\n\n33.2.12.3 Total bilirubin (mg/dL)\n\ndat2$bilirubin.total &lt;- dat2$LBXSTB\nsummary(dat2$bilirubin.total)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;  0.1000  0.3000  0.4000  0.4605  0.6000  3.7000    3351\n\n\n\n33.2.12.4 Phosphorus (mg/dL)\n\ndat2$phosphorus &lt;- dat2$LBXSPH\nsummary(dat2$phosphorus)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   1.900   3.300   3.600   3.665   4.000   9.600    3353\n\n\n\n33.2.12.5 Sodium (mmol/L)\n\ndat2$sodium &lt;- dat2$LBXSNASI\nsummary(dat2$sodium)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   121.0   138.0   140.0   140.3   142.0   151.0    3350\n\n\n\n33.2.12.6 Potassium (mmol/L)\n\ndat2$potassium &lt;- dat2$LBXSKSI\nsummary(dat2$potassium)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   2.800   3.900   4.100   4.094   4.300   6.600    3355\n\n\n\n33.2.12.7 Globulin (g/dL)\n\ndat2$globulin &lt;- dat2$LBXSGB\nsummary(dat2$globulin)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   1.800   2.800   3.100   3.087   3.300   6.000    3353\n\n\n\n33.2.12.8 Total calcium (mg/dL)\n\ndat2$calcium.total &lt;- dat2$LBXSCA\nsummary(dat2$calcium.total)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;    6.40    9.10    9.30    9.32    9.60   11.70    3353\n\n\n\n33.2.12.9 High cholesterol\n\ndat2$high.cholesterol &lt;- dat2$BPQ080\ndat2$high.cholesterol &lt;- car::recode(dat2$high.cholesterol, recodes = \" 'Yes'='Yes';\n                                     'No'='No'; else = NA\")\ntable(dat2$high.cholesterol, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 4153 1968 3133\n\n\n\n\n33.2.13 Survey features\n\n33.2.13.1 Weight\n\ndat2$survey.weight &lt;- dat2$WTINT2YR\nsummary(dat2$survey.weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    2571   13074   21098   34671   36923  433085\ndat2$survey.weight.mec &lt;- dat2$WTMEC2YR\nsummary(dat2$survey.weight.mec)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;       0   12347   21060   34671   37562  419763\n\n\n\n33.2.13.2 PSU\n\ndat2$psu &lt;- as.factor(dat2$SDMVPSU)\ntable(dat2$psu)\n#&gt; \n#&gt;    1    2 \n#&gt; 4464 4790\n\n\n\n33.2.13.3 Strata\n\ndat2$strata &lt;- as.factor(dat2$SDMVSTRA)\ntable(dat2$strata)\n#&gt; \n#&gt; 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 \n#&gt; 510 638 695 554 605 653 612 693 735 551 689 609 604 596 510\n\n\n\n\n33.2.14 Survey year\n\ndat2$year &lt;- dat2$SDDSRVYR\ntable(dat2$year, useNA = \"always\") \n#&gt; \n#&gt; NHANES 2017-2018 public release                            &lt;NA&gt; \n#&gt;                            9254                               0\n\n\n\n33.2.15 ICD-10-CM codes\n\ncolnames(rxq12) &lt;- c(\"id\", \"icd10\")\ncolnames(rxq22) &lt;- c(\"id\", \"icd10\")\ncolnames(rxq32) &lt;- c(\"id\", \"icd10\")\n\nrx2017 &lt;- rbind(rxq12, rxq22, rxq32)\nrx2017 &lt;- rx2017[order(rx2017$id),]\n\nrx2017$icd10[rx2017$icd10 == \"Unknown\"] &lt;- NA\nrx2017$icd10[rx2017$icd10 == \"Refused\"] &lt;- NA\nrx2017$icd10[rx2017$icd10 == \"Don't know\"] &lt;- NA\nrx2017$icd10[rx2017$icd10 == \"\"] &lt;- NA\nrx2017$icd10.new &lt;- substr(rx2017$icd10, start = 1, stop = 3)\n\nrx2017 &lt;- na.omit(rx2017)",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Recoding cycle 10</span>"
    ]
  },
  {
    "objectID": "analytic17.html#analytic-data",
    "href": "analytic17.html#analytic-data",
    "title": "33  Recoding cycle 10",
    "section": "33.3 Analytic data",
    "text": "33.3 Analytic data\n\n33.3.1 Full dataset\n\nnhanes17r &lt;- dat2\n\n\n\n33.3.2 Analytic datset - adults 20 years of more\n\nvars &lt;- c(\n  # ID\n  \"id\",\n  \n  # Demographic\n  \"age\", \"age.cat\", \"sex\", \"education\", \"race\", \n  \"marital\", \"income\", \"born\", \"pregnancy\",\n  \n  # obesity\n  \"obese\", \n  \n  # Diabetes\n  \"diabetes\", \"diabetes.family.history\",\n  \n  # Smoking\n  \"smoking\", \n  \n  # Diet\n  \"diet.healthy\", \n\n  # Physical activity\n  \"physical.activity\", \n  \n  # Access to routine healthcare\n  \"medical.access\",\n  \n  # Blood pressure and Hypertension\n  \"systolicBP\", \"diastolicBP\", \n  \n  # Sleep \n  \"sleep\",\n\n  # Laboratory \n  \"uric.acid\", \"protein.total\", \"bilirubin.total\", \"phosphorus\",\n  \"sodium\", \"potassium\", \"globulin\", \"calcium.total\", \n  \"high.cholesterol\",\n  \n  # Survey features\n  \"survey.weight\", \"survey.weight.mec\", \"psu\", \"strata\", \n  \n  # Survey year\n  \"year\"\n)\n\nnhanes17r.sel &lt;- nhanes17r[, vars]\n\n\n# Adults 20 years of more and not pregnant\ndim(nhanes17r.sel)\n#&gt; [1] 9254   34\nanalytic17 &lt;- subset(nhanes17r.sel, age &gt;= 20 & \n                       pregnancy != 'yes')\ndim(analytic17)\n#&gt; [1] 5569   34\n\n\n\n33.3.3 Save dataset for later use\n\ndim(analytic17)\n#&gt; [1] 5569   34\ndim(rx2017)\n#&gt; [1] 15025     3\nsave(analytic17, rx2017, file = \"data/analytic17recoded.RData\")",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Recoding cycle 10</span>"
    ]
  },
  {
    "objectID": "merge13to17.html",
    "href": "merge13to17.html",
    "title": "34  Merge three cycles",
    "section": "",
    "text": "34.1 Analytic dataset",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Merge three cycles</span>"
    ]
  },
  {
    "objectID": "merge13to17.html#analytic-dataset",
    "href": "merge13to17.html#analytic-dataset",
    "title": "34  Merge three cycles",
    "section": "",
    "text": "34.1.1 Load 2013-18 datasets\n\nload(\"data/analytic13recoded.RData\")\nload(\"data/analytic15recoded.RData\")\nload(\"data/analytic17recoded.RData\")\n\n\n\n34.1.2 Merge 2013-18 datasets\n\n# adults aged 20 years or more\ndata.merged0 &lt;- rbind(analytic13, analytic15, analytic17)\ndim(data.merged0)\n#&gt; [1] 17057    34\ndata.merged &lt;- droplevels(data.merged0)\n\n\n\n34.1.3 Check missingness\n\nplot_missing(data.merged)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ℹ The deprecated feature was likely used in the DataExplorer package.\n#&gt;   Please report the issue at\n#&gt;   &lt;https://github.com/boxuancui/DataExplorer/issues&gt;.\n\n\n\n\n\n\n\n# profile_missing(data.merged)\ndim(data.merged)\n#&gt; [1] 17057    34\n\n\n\nThe data contants variables with some missing information.\n\ndata.complete &lt;- na.omit(data.merged)\ndim(data.complete)\n#&gt; [1] 6850   34\n\n\n\n\nOnly complete cases retained, and survey features/weights were ignored for simplicity.\nIn a realistic analysis, we would consider the missingness pattern before deleting or imputing such information.",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Merge three cycles</span>"
    ]
  },
  {
    "objectID": "merge13to17.html#summary-statistics",
    "href": "merge13to17.html#summary-statistics",
    "title": "34  Merge three cycles",
    "section": "34.2 Summary statistics",
    "text": "34.2 Summary statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo\n(N=4291)\nYes\n(N=2559)\nOverall\n(N=6850)\n\n\n\n\nage.cat\n\n\n\n\n\n20-49\n2208 (51.5%)\n1227 (47.9%)\n3435 (50.1%)\n\n\n50-64\n1085 (25.3%)\n767 (30.0%)\n1852 (27.0%)\n\n\n65+\n998 (23.3%)\n565 (22.1%)\n1563 (22.8%)\n\n\nsex\n\n\n\n\n\nMale\n2086 (48.6%)\n1106 (43.2%)\n3192 (46.6%)\n\n\nFemale\n2205 (51.4%)\n1453 (56.8%)\n3658 (53.4%)\n\n\neducation\n\n\n\n\n\nLess than high school\n597 (13.9%)\n419 (16.4%)\n1016 (14.8%)\n\n\nHigh school\n1809 (42.2%)\n1375 (53.7%)\n3184 (46.5%)\n\n\nCollege graduate or above\n1885 (43.9%)\n765 (29.9%)\n2650 (38.7%)\n\n\nrace\n\n\n\n\n\nWhite\n1496 (34.9%)\n932 (36.4%)\n2428 (35.4%)\n\n\nBlack\n583 (13.6%)\n581 (22.7%)\n1164 (17.0%)\n\n\nHispanic\n955 (22.3%)\n763 (29.8%)\n1718 (25.1%)\n\n\nOthers\n1257 (29.3%)\n283 (11.1%)\n1540 (22.5%)\n\n\nmarital\n\n\n\n\n\nNever married\n757 (17.6%)\n408 (15.9%)\n1165 (17.0%)\n\n\nMarried/with partner\n2756 (64.2%)\n1533 (59.9%)\n4289 (62.6%)\n\n\nOther\n778 (18.1%)\n618 (24.2%)\n1396 (20.4%)\n\n\nincome\n\n\n\n\n\nless than $20,000\n668 (15.6%)\n443 (17.3%)\n1111 (16.2%)\n\n\n$20,000 to $74,999\n1955 (45.6%)\n1353 (52.9%)\n3308 (48.3%)\n\n\n$75,000 and Over\n1668 (38.9%)\n763 (29.8%)\n2431 (35.5%)\n\n\nborn\n\n\n\n\n\nBorn in US\n2269 (52.9%)\n1745 (68.2%)\n4014 (58.6%)\n\n\nOther place\n2022 (47.1%)\n814 (31.8%)\n2836 (41.4%)\n\n\nyear\n\n\n\n\n\nNHANES 2013-2014 public release\n1976 (46.0%)\n1100 (43.0%)\n3076 (44.9%)\n\n\nNHANES 2015-2016 public release\n740 (17.2%)\n337 (13.2%)\n1077 (15.7%)\n\n\nNHANES 2017-2018 public release\n1575 (36.7%)\n1122 (43.8%)\n2697 (39.4%)\n\n\ndiabetes.family.history\n\n\n\n\n\nNo\n3656 (85.2%)\n1971 (77.0%)\n5627 (82.1%)\n\n\nYes\n635 (14.8%)\n588 (23.0%)\n1223 (17.9%)\n\n\nsmoking\n\n\n\n\n\nNever smoker\n2760 (64.3%)\n1591 (62.2%)\n4351 (63.5%)\n\n\nPrevious smoker\n917 (21.4%)\n636 (24.9%)\n1553 (22.7%)\n\n\nCurrent smoker\n614 (14.3%)\n332 (13.0%)\n946 (13.8%)\n\n\ndiet.healthy\n\n\n\n\n\nPoor or fair\n876 (20.4%)\n1006 (39.3%)\n1882 (27.5%)\n\n\nGood\n1747 (40.7%)\n1039 (40.6%)\n2786 (40.7%)\n\n\nVery good or excellent\n1668 (38.9%)\n514 (20.1%)\n2182 (31.9%)\n\n\nphysical.activity\n\n\n\n\n\nNo\n3590 (83.7%)\n2007 (78.4%)\n5597 (81.7%)\n\n\nYes\n701 (16.3%)\n552 (21.6%)\n1253 (18.3%)\n\n\nmedical.access\n\n\n\n\n\nNo\n767 (17.9%)\n319 (12.5%)\n1086 (15.9%)\n\n\nYes\n3524 (82.1%)\n2240 (87.5%)\n5764 (84.1%)\n\n\nsleep\n\n\n\n\n\nMean (SD)\n7.32 (1.42)\n7.21 (1.54)\n7.28 (1.47)\n\n\nMedian [Min, Max]\n7.00 [2.00, 14.0]\n7.00 [2.00, 14.0]\n7.00 [2.00, 14.0]\n\n\nsystolicBP\n\n\n\n\n\nMean (SD)\n122 (18.2)\n127 (17.4)\n124 (18.1)\n\n\nMedian [Min, Max]\n118 [64.7, 229]\n125 [74.0, 212]\n121 [64.7, 229]\n\n\ndiastolicBP\n\n\n\n\n\nMean (SD)\n70.2 (11.1)\n72.8 (11.5)\n71.2 (11.3)\n\n\nMedian [Min, Max]\n70.7 [12.0, 123]\n72.7 [26.0, 124]\n71.3 [12.0, 124]\n\n\nuric.acid\n\n\n\n\n\nMean (SD)\n5.19 (1.36)\n5.74 (1.48)\n5.39 (1.43)\n\n\nMedian [Min, Max]\n5.10 [1.10, 12.3]\n5.60 [2.10, 13.3]\n5.30 [1.10, 13.3]\n\n\nprotein.total\n\n\n\n\n\nMean (SD)\n7.14 (0.454)\n7.10 (0.443)\n7.12 (0.450)\n\n\nMedian [Min, Max]\n7.10 [4.70, 10.2]\n7.10 [5.40, 9.10]\n7.10 [4.70, 10.2]\n\n\nbilirubin.total\n\n\n\n\n\nMean (SD)\n0.594 (0.307)\n0.513 (0.304)\n0.564 (0.308)\n\n\nMedian [Min, Max]\n0.500 [0, 3.30]\n0.500 [0, 7.10]\n0.500 [0, 7.10]\n\n\nphosphorus\n\n\n\n\n\nMean (SD)\n3.73 (0.545)\n3.66 (0.575)\n3.70 (0.557)\n\n\nMedian [Min, Max]\n3.70 [2.00, 6.10]\n3.60 [1.80, 8.90]\n3.70 [1.80, 8.90]\n\n\nsodium\n\n\n\n\n\nMean (SD)\n140 (2.45)\n140 (2.58)\n140 (2.50)\n\n\nMedian [Min, Max]\n140 [124, 150]\n140 [121, 154]\n140 [121, 154]\n\n\npotassium\n\n\n\n\n\nMean (SD)\n4.01 (0.358)\n4.04 (0.363)\n4.02 (0.360)\n\n\nMedian [Min, Max]\n4.00 [2.80, 6.00]\n4.00 [2.80, 6.60]\n4.00 [2.80, 6.60]\n\n\nglobulin\n\n\n\n\n\nMean (SD)\n2.88 (0.438)\n3.02 (0.450)\n2.93 (0.448)\n\n\nMedian [Min, Max]\n2.80 [1.60, 6.50]\n3.00 [1.40, 5.20]\n2.90 [1.40, 6.50]\n\n\ncalcium.total\n\n\n\n\n\nMean (SD)\n9.39 (0.364)\n9.32 (0.381)\n9.36 (0.371)\n\n\nMedian [Min, Max]\n9.40 [6.40, 14.8]\n9.30 [6.60, 12.0]\n9.40 [6.40, 14.8]\n\n\nhigh.cholesterol\n\n\n\n\n\nNo\n2833 (66.0%)\n1504 (58.8%)\n4337 (63.3%)\n\n\nYes\n1458 (34.0%)\n1055 (41.2%)\n2513 (36.7%)\n\n\n\n\n\n\n\n\n\n\nInvestigator specified covariates stratified by the exposure (obesity)\nThis Table includes information about participants with and without ICD-10-CM proxy information. Therefore, the sample is is larger than the original analysis.",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Merge three cycles</span>"
    ]
  },
  {
    "objectID": "merge13to17.html#proxy-data-from-icd10-codes",
    "href": "merge13to17.html#proxy-data-from-icd10-codes",
    "title": "34  Merge three cycles",
    "section": "34.3 Proxy data from ICD10 codes",
    "text": "34.3 Proxy data from ICD10 codes\n\ndat.proxy.long &lt;- rbind(rx2013, rx2015, rx2017) \ndat.proxy.long$icd10 &lt;- NULL\n# Rename 3 digits ICD-10 codes as icd10\ncolnames(dat.proxy.long)[names(dat.proxy.long)==\"icd10.new\"] &lt;- \"icd10\"\n\n\n\nWe combine all of the ICD-10-CM information form all 3 cycles.",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Merge three cycles</span>"
    ]
  },
  {
    "objectID": "merge13to17.html#save-dataset-for-later-use",
    "href": "merge13to17.html#save-dataset-for-later-use",
    "title": "34  Merge three cycles",
    "section": "34.4 Save dataset for later use",
    "text": "34.4 Save dataset for later use\n\nsave(data.merged, \n     data.complete, \n     dat.proxy.long, \n     file = \"data/analytic3cycles.RData\")",
    "crumbs": [
      "Appendix: NHANES",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Merge three cycles</span>"
    ]
  }
]